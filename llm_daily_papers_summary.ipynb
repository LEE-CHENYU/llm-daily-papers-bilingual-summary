{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U google-generativeai\n",
    "# !pip install google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "GOOGLE_API_KEY='AIzaSyC6gfzUfG1FnyzdFzjbsfBIH3rMKDCspJo'\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>-pro\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-\u001b[1;36m1.0\u001b[0m-pro\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>-pro-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">001</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-\u001b[1;36m1.0\u001b[0m-pro-\u001b[1;36m001\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>-pro-latest\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-\u001b[1;36m1.0\u001b[0m-pro-latest\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>-pro-vision-latest\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-\u001b[1;36m1.0\u001b[0m-pro-vision-latest\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-pro\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-pro\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-pro-vision\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-pro-vision\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `response.text` quick accessor only works for simple (single-`Part`) text responses. This response is not simple text.Use the `result.parts` accessor or the full `result.candidates[index].content.parts` lookup instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the meaning of life?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python310/lib/python3.10/site-packages/google/generativeai/types/generation_types.py:328\u001b[0m, in \u001b[0;36mBaseGenerateContentResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parts) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m parts[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `response.text` quick accessor only works for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple (single-`Part`) text responses. This response is not simple text.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse the `result.parts` accessor or the full \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`result.candidates[index].content.parts` lookup \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mValueError\u001b[0m: The `response.text` quick accessor only works for simple (single-`Part`) text responses. This response is not simple text.Use the `result.parts` accessor or the full `result.candidates[index].content.parts` lookup instead."
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')\n",
    "response = model.generate_content(\"What is the meaning of life?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At the command line, only need to run once to install the package via pip:\n",
    "\n",
    "$ pip install google-generativeai\n",
    "\"\"\"\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "GOOGLE_API_KEY='AIzaSyC6gfzUfG1FnyzdFzjbsfBIH3rMKDCspJo'\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Set up the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0.9,\n",
    "  \"top_p\": 1,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 2048,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n",
    "                              generation_config=generation_config,\n",
    "                              safety_settings=safety_settings)\n",
    "\n",
    "# convo = model.start_chat(history=[\n",
    "#   {\n",
    "#     \"role\": \"user\",\n",
    "#     \"parts\": [\"For each PDF provided, generate a concise summary focusing on the key points, main arguments, and conclusions, both in English and Chinese. The summary should be clear, engaging, and accessible to a general audience with a college-level education. Each summary should include:\\t\\tIntroduction (引言): A brief introduction to the document's main topic or research question, in simple and accessible language.\\t\\tMain Findings or Arguments (主要发现或论点): Highlight the primary findings or arguments presented in the document, ensuring clarity and engagement.\\t\\tMethodology or Approach (方法论或方法): If applicable, discuss the methodology or approach used in the document, translating technical terms into layman's terms.\\t\\tConclusions and Implications (结论和影响): Conclude with the document's implications or potential impact, emphasizing its relevance and importance.The goal is to provide succinct yet comprehensive overviews that enable readers to understand the essence of each document without needing to read it in full. Ensure each summary is distinct and accurately reflects the specific content and tone of its corresponding PDF, with parallel content in both English and Chinese to cater to a bilingual audience.PDF contents:\"]\n",
    "#   },\n",
    "#   {\n",
    "#     \"role\": \"model\",\n",
    "#     \"parts\": [\"**中文摘要：**\\n\\n本文从新的角度研究了大型语言模型（LLM）在推理能力方面的发展，探索了一种在解码阶段而不是提示阶段激发 LLM 进行推理的新方法——CoT 解码。通过考虑最初解码步骤中替代顶级标记，本文发现 LLM 中本来就存在推理路径。该方法不仅绕开了提示的混杂因素，还能更准确地评估 LLM 的内在推理能力。此外，本文提出 CoT-解码算法以从顶级解码路径中提取更加可靠的解码路径，进而显著提升了各种基准上的推理能力。\\n\\n**英文摘要：**\\n\\nWe investigate the reasoning capabilities of large language models (LLMs) by exploring an alternative perspective within the decoding stage, demonstrating that even without explicit prompting, inherent CoT reasoning paths exist within these models. Rather than relying exclusively on greedy decoding, exploring alternative top-𝑘 tokens in the decoding space reveals the natural existence of reasoning paths within these models. Furthermore, our empirical observations highlight that the presence of a CoT reasoning path correlates with increased model confidence in decoding its final answer. Based on this observation, we introduce CoT-decoding to extract more reliable decoding paths from language models, thereby enhancing overall reasoning performance. Exploring alternative decoding paths incurs additional computational costs. Future work may leverage the CoT-decoding paths to fine-tune the model to enhance its reasoning capabilities.\"]\n",
    "#   },\n",
    "# ])\n",
    "\n",
    "# convo.send_message(\"YOUR_USER_INPUT\")\n",
    "# print(convo.last.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Linear Transformers with Learnable Kernel Functions are Better In-Context Models\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Linear Transformers with Learnable Kernel Functions are Better In-Context Models\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10644</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10644\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10644</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10644\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10644.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10644.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10790</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10790\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10790</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10790\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10790.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10790.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10379</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10379\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10379</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10379\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10379.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10379.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting\n",
       "</pre>\n"
      ],
      "text/plain": [
       "GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10259</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10259\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10259</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10259\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10259.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10259.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10524</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10524\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10524</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10524\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10524.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10524.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10294</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10294\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10294</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10294\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10294.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10294.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Large Language Models as Zero-shot Dialogue State Tracker through Function Calling\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10466</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10466\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10466</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10466\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10466.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10466.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10491</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10491\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10491</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10491\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10491.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10491.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10329</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10329\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10329</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10329\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10329.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10329.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SPAR: Personalized Content-Based Recommendation via Long Engagement Attention\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SPAR: Personalized Content-Based Recommendation via Long Engagement Attention\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10555</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10555\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10555</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10555\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10555.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10555.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10896</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10896\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10896</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10896\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10896.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10896.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">RLVF: Learning from Verbal Feedback without Overgeneralization\n",
       "</pre>\n"
      ],
      "text/plain": [
       "RLVF: Learning from Verbal Feedback without Overgeneralization\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10893</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10893\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10893</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10893\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10893.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10893.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://huggingface.co/papers'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "link_list = []\n",
    "code_list = []\n",
    "pdflink_list = []\n",
    "elements = soup.select('h3.mb-1 a')\n",
    "for element in elements:\n",
    "    print(element.text)\n",
    "    \n",
    "    link = 'https://huggingface.co' + element['href']\n",
    "    code = element['href'].split('/')[-1]\n",
    "    pdflink = 'https://arxiv.org/pdf/' + code + '.pdf'\n",
    "    \n",
    "    link_list.append(link)\n",
    "    code_list.append(code)\n",
    "    pdflink_list.append(pdflink)\n",
    "    \n",
    "    print(link)\n",
    "    print(code)\n",
    "    print(pdflink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"Summary\": {\\n    \"chinese_title\": \"可学习核函数的线性变换器是更好的情境模型\",\\n    \"chinese_summary\": \"在飞速发展的自然语言处理领域，推进语言模型（LM）的亚二次架构至关重要。包括状态空间模型在内的当前创新最初因其在语言建模任务上超越了 Transformer 性能而受到赞扬。然而，这些模型在重要的情境学习能力方面表现出不足——这是 Transformer 传统上表现出色的领域。基于模型作为一种混合解决方案出现，它融合了一个线性变换器和一个受指数函数泰勒展开启发的核，并由卷积网络增强。它反映了 Transformer 在情境中的适应性，并成为该领域的强有力竞争者。在我们的工作中，我们对基于核心的一个独特、优雅的改变，它增强了在多查询关联召回任务和整体语言建模过程中评估的情境学习能力，正如在 Pile 数据集上所展示的那样。\",\\n    \"english_title\": \"Linear Transformers with Learnable Kernel Functions are Better In-Context Models\",\\n    \"english_summary\": \"Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities — a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer’s in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\"\\n  }\\n}'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "# Extract text from PDFs\n",
    "def extract_text_from_pdf(pdf_link):\n",
    "    images = convert_from_path(pdf_link)\n",
    "    text = pytesseract.image_to_string(images[0])\n",
    "    return text\n",
    "\n",
    "prompt = \"\"\"For each PDF document provided, generate a concise, unified summary that captures the essence of the document's critical elements: its primary topic or question, significant findings or debates, methodology or strategy (if pertinent), and the outcomes and their significance. This summary should be crafted in clear, engaging language that is easily comprehensible to a general audience with a college-level education, succinctly conveying the document's core message and importance, thus allowing readers to grasp its essence without the need to read the document in its entirety.\n",
    "\n",
    "The summary should comprise a single, integrated paragraph that seamlessly includes the following components:\n",
    "\n",
    "Introduction: Begin with a concise introduction to the document's main focus or question, employing direct and accessible language.\n",
    "Main Findings or Arguments: Incorporate an overview of the document's crucial discoveries or arguments, ensuring the narrative is captivating and clear.\n",
    "Methodology or Approach: Where relevant, succinctly describe the document's methodology or approach, making any complex terminology understandable to the lay reader.\n",
    "Conclusions and Implications: End with a summary of the document's wider implications or its potential impact, underscoring its importance.\n",
    "\n",
    "This summary must be provided in both English and Chinese, striking a balance between clarity and depth to accommodate a bilingual audience. Avoid bullet points, aiming for a continuous, fluid paragraph that accurately reflects the specific content and tone of the corresponding PDF.\n",
    "\n",
    "The ultimate aim is to offer succinct yet comprehensive overviews, empowering readers to fully understand the essence of each document. Each summary should be distinctive, accurately mirroring the particular nuances and themes of its respective PDF.\n",
    "\n",
    "The output should conform to the following schema:\n",
    "\n",
    "{\n",
    "  \"Summary\": {\n",
    "    \"chinese_title\": \"\",\n",
    "    \"chinese_summary\": \"\",\n",
    "    \"english_title\": \"\",\n",
    "    \"english_summary\": \"\"\n",
    "  }\n",
    "}\n",
    "\n",
    "PDF contents:\"\"\"\n",
    "\n",
    "text = extract_text_from_pdf(pdflink_list[0])\n",
    "summary = model.generate_content(prompt + text).text\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\\n  \"Summary\": {\\n    \"chinese_title\": \"可学习核函数的线性变换器是更好的上下文模型\",\\n    \"chinese_summary\": \"在自然语言处理快速发展的领域内，推进语言模型 (LM) 的亚二次架构是一项至关重要的工作。包括状态空间模型在内的当前创新最初因其在语言建模任务中超越了 Transformer 性能而广受赞誉。然而，这些模型在必要的上下文学习能力方面暴露出缺陷，这是 Transformer 传统上表现出色的领域。Based 模型作为一种混合解决方案应运而生，它将线性变换器与受指数函数的泰勒展开所启发的核函数相结合，并通过卷积网络进行了扩展。它复制了 Transformer 在上下文中的熟练度，成为该领域的有力竞争者。在我们的工作中，我们对 Based 核函数提出了一种独特而优雅的修改，通过多查询关联召回任务和在 Pile 数据集上展示的整体语言建模过程评估，放大了其上下文学习能力。\",\\n    \"english_title\": \"Linear Transformers with Learnable Kernel Functions are Better In-Context Models\",\\n    \"english_summary\": \"Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities — a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer’s in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"在一千兆字节的干草堆中寻找针头：循环记忆发现了大型语言模型错失的东西\",\\n    \"chinese_summary\": \"本文讨论了使用生成式 Transformer 模型处理长文档的挑战。为了评估不同的方法，我们引入了 BABILong，这是一个新的基准，旨在评估模型在提取和处理冗长文本中的分布式事实方面的能力。我们的评估包括 GPT-4 和 RAG 的基准测试，结果表明常见的方法仅对高达 10^4 个元素的序列有效。相比之下，用循环记忆增强对 GPT-2 的微调使其能够处理涉及多达 10^7 个元素的任务。这一成就是一项重大飞跃，因为这是迄今为止任何开放神经网络模型处理的最长输入，展示了处理长序列能力的显著提高。\",\\n    \"english_title\": \"In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss\",\\n    \"english_summary\": \"This paper tackles the challenge of processing lengthy documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to 10^4 elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to 10^7 elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"DataDreamer：合成数据生成和可复制 LLM 工作流工具\",\\n    \"chinese_summary\": \"大型语言模型 (LLM) 已成为 NLP 研究人员在各种任务中的一个主要工具。如今，很多研究人员在合成数据生成、任务评估、模型微调、蒸馏和其他模型循环研究工作流中使用 LLM。但是，在使用这些模型时会产生一些挑战，这些挑战源于其规模、封闭源代码的本质以及这些新兴工作流缺乏标准化工具。这些模型的快速发展和这些独有的挑战对开放科学和使用这些模型的工作的可复制性产生了直接的不利影响。在本文中，我们介绍了 DataDreamer，这是一个开源 Python 库，允许研究人员编写简单的代码来实现强大的 LLM 工作流。DataDreamer 还帮助研究人员遵守我们提议的最佳实践，以促进开放科学和可复制性。该库和文档可以在以下位置获得：https://github.com/datadreamer-dev/DataDreamer。\",\\n    \"english_title\": \"DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows\",\\n    \"english_summary\": \"Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at: https://github.com/datadreamer-dev/DataDreamer.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"GaussianObject：只用四张图像获得带有高斯泼溅的高质量3D对象\",\\n    \"chinese_summary\": \"重建和渲染高度稀疏视图中的3D对象对推广3D视觉技术的应用和改善用户体验至关重要。然而，稀疏视图的图像只包含非常有限的3D信息，这带来了两个重大挑战：1）难以建立多视图一致性，因为匹配的图像太少；2）由于视图覆盖不足，对象信息部分省略或高度压缩。为了应对这些挑战，我们提出了GaussianObject，这是一个使用高斯泼溅来表示和渲染3D对象的框架，它仅使用4个输入图像就能实现高渲染质量。我们首先介绍了视觉包络和浮动消除技术，这些技术将结构先入为主地注入到初始优化过程中，以帮助建立多视图一致性，从而产生一个粗略的3D高斯表示。然后，我们基于扩散模型构建了一个高斯修复模型来补充被省略的对象信息，其中高斯体进一步得到细化。我们设计了一种自生成策略来获得用于训练修复模型的图像对。我们的GaussianObject在几个具有挑战性的数据集上进行了评估，包括MipNeRF360、OmniObject3D和Openll-lumination，仅使用4个视图就获得了强大的重建结果，并且明显优于以前最先进的方法。请访问我们的项目页面https://gaussianobject.github.io/。\",\\n    \"english_title\": \"GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting\",\\n    \"english_summary\": \"Reconstructing and rendering 3D objects from 2D images has been a longstanding and important topic, which plays critical roles in a vast range of real-life applications. One key factor that impedes users, especially ones without expert knowledge, from widely using these techniques is that usually dozens of multi-view images need to be captured, which is cumbersome and sometimes impractical. Efficiently reconstructing high-quality 3D objects from highly sparse captured images is of great value for expediting downstream applications such as 3D asset creation for game/movie production and AR/VR products.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"LLM Comparator：可视化分析，进行大型语言模型的并排评估\",\\n    \"chinese_summary\": \"该 PDF 文档介绍了 LLM Comparator，这是一种用于可视化分析大型语言模型 (LLM) 自动并排评估结果的交互式工具。该工具支持模型开发者和研究人员比较两个 LLM 的响应质量，识别出何时以及为何一个模型的性能优于或劣于另一个模型，并探索两个模型响应之间的差异。LLM Comparator 提供了一个交互式表格，用于检查单个提示及其响应，以及一个可视化摘要，支持帮助理解模型差异的分析工作流。\",\\n    \"english_title\": \"LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models\",\\n    \"english_summary\": \"This PDF document presents LLM Comparator, an interactive tool for visual analytics of automatic side-by-side evaluation results of large language models (LLMs). It enables model developers and researchers to compare the quality of responses from two LLMs, identify when and why one model performs better or worse than the other, and explore the differences in their responses. LLM Comparator provides an interactive table for inspecting individual prompts and their responses, and a visualization summary that supports analytical workflows for understanding model differences.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"LAVE：针对视频编辑的 LLM 助力代理与自然语言增强\",\\n    \"chinese_summary\": \"LAVE 是一款视频编辑工具，它提供了 LLM 助力代理与自然语言增强功能，可以帮助用户更轻松地编辑视频。例如，用户可以通过文字与 LAVE 的视频编辑代理互动，在整个编辑过程中获得实时帮助。此外，LAVE 还提供一系列语言增强功能，例如自动生成视频标题、显示视频摘要帮助用户快速了解视频内容、在缩略图库中通过点击视频即可将其添加到编辑时间轴等，这些功能可以极大地简化和加速视频编辑流程，降低新手入门门槛。\",\\n    \"english_title\": \"LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing\",\\n    \"english_summary\": \"The Increasing demand for video content creation has been hindered by the technical expertise and effort typically required for video editing. This paper presents an integration of large language models (LLMs) into the video editing workflow to lower these barriers, specifically focusing on assistance and language augmentation. Our proposed system, LAVE, offers an agent that interacts with users using natural language, providing assistance with various editing tasks. It also incorporates language augmentation into the editing workflow, with features such as automated video summarization, video title generation, and quick video selection from a gallery. We believe that LAVE simplifies the video editing process, making it more accessible to a wider range of users.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"基于函数调用的零样本对话状态追踪器中的大语言模型\",\\n    \"chinese_summary\": \"大语言模型（LLM）因其在一般语境下的高级理解和生成能力而在会话系统中日益普及。然而，它们在任务导向对话（TOD）中的有效性仍然不尽如人意，这不仅需要生成响应，还需要在特定任务和领域内进行有效的对话状态跟踪（DST）。在这项工作中，我们提出了一种通过函数调用使用 LLM 解决 DST 的新方法 FNCTOD。该方法改进了零样本 DST，允许适应不同的领域，而无需大量数据收集或模型调整。我们的实验结果表明，我们的方法使用中等规模的开源和专有 LLM 都取得了卓越的性能：在上下文中提示下，它使各种 7B 或 13B 参数模型能够超越 ChatGPT 实现的先前最先进（SOTA）技术，并将 ChatGPT 的性能提高了 5.6% 的平均 JGA。GPT-3.5 和 GPT-4 的单个模型结果分别提高了 4.8% 和 14%。我们还表明，通过针对小规模的不同任务导向对话进行微调，我们可以为中等规模的模型（特别是 13B 参数 LLaMA2-Chat 模型）配备函数调用能力和与 ChatGPT 相媲美的 DST 性能，同时保持其聊天能力。我们将开源实验代码和模型。\",\\n    \"english_title\": \"Large Language Models as Zero-shot Dialogue State Tracker through Function Calling\",\\n    \"english_summary\": \"Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FNCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT’s performance beating the SOTA by 5.6% Avg. JGA. Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We will open-source experimental code and model.\"\\n  }\\n}',\n",
       " '{\\n \"Summary\": {\\n  \"chinese_title\": \"构建廉价缩放：一种用于高分辨率适应的自级联扩散模型\",\\n  \"chinese_summary\": \"扩散模型在图像和视频生成中已被证明非常有效；然而，由于单尺度训练数据，它们在生成不同大小的图像时仍然面临构图挑战。针对高分辨率调整大型预训练扩散模型需要大量的计算和优化资源，但仍无法实现与低分辨率模型相当的生成能力。本文提出了一种新颖的自级联扩散模型，该模型利用从经过良好训练的低分辨率模型中获得的丰富知识，快速适应高分辨率图像和视频生成，采用无调优或廉价上采样器调优范例。自级联扩散模型集成了序列多尺度上采样器模块，能有效地适应更高的分辨率，同时保留了原始构图和生成能力。我们进一步提出了一种枢轴引导噪声重新调度策略，以加速推理过程并改善局部结构细节。与完全微调相比，我们的方法将训练速度提高了 5 倍，并且仅需要额外的 0.002M 调优参数。大量实验证明，我们的方法可以通过仅微调 10k 步，在几乎没有额外推理时间的情况下，快速适应更高分辨率的图像和视频合成。我们的代码将在 https://github.com/GuoLanging/Self-Cascade/ 发布。\",\\n  \"english_title\": \"Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation\",\\n  \"english_summary\": \"Diffusion models have proven to be highly effective in image and video generation; however, they still face composition challenges when generating images of varying sizes due to single-scale training data. Adapting large pre-trained diffusion models for higher resolution demands substantial computational and optimization resources, yet achieving a generation capability comparable to low-resolution models remains elusive. This paper proposes a novel self-cascade diffusion model that leverages the rich knowledge gained from a well-trained low-resolution model for rapid adaptation to higher-resolution image and video generation, employing either tuning-free or cheap upsampler tuning paradigms. Integrating a sequence of multi-scale upsampler modules, the self-cascade diffusion model can efficiently adapt to a higher resolution, preserving the original composition and generation capabilities. We further propose a pivot-guided noise re-schedule strategy to speed up the inference process and improve local structural details. Compared to full fine-tuning, our approach achieves a 5x training speed-up and requires only an additional 0.002M tuning parameters. Extensive experiments demonstrate that our approach can quickly adapt to higher resolution image and video synthesis by fine-tuning for just 10k steps, with virtually no additional inference time. Our code will be released at https://github.com/GuoLanging/Self-Cascade/.\"\\n }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"通用操作界面：在不使用野外机器人的情况下，在野外对机器进行教学\",\\n    \"chinese_summary\": \"我们提出了通用操作界面（UMI）——一种数据收集和策略学习框架，该框架允许直接将野外人类演示中的技能转移到可部署机器策略中。UMI 采用手持抓手结合谨慎的界面设计，可实现便携、低成本且信息丰富的数据收集，以进行极具挑战性的双手动和动态操作演示。为促进可部署策略学习，UMI 包含一个精心设计的策略界面，该界面具备推理时延迟匹配和相对轨迹动作表示。所学策略与硬件无关，且可在多种机器人平台上部署。配备这些功能，UMI 框架解锁了新的机器人操作能力，仅通过针对每个任务更改训练数据，即可实现零次概括的动态、双手动、精确定位和长时间行为。我们通过全面的真实实验展示了 UMI 的多功能性和有效性，在 UMI 上学习的策略，在针对各种人类演示进行训练后，可零次概括到新环境和物体。UMI 的硬件和软件系统可在 https://umi-gripper.github.io 获取开源。\",\\n    \"english_title\": \"Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots\",\\n    \"english_summary\": \"We present Universal Manipulation Interface (UMI)—a data collection and policy learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot manipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors, by only changing the training data for each task. We demonstrate UMI’s versatility and efficacy with comprehensive real-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when trained on diverse human demonstrations. UMI’s hardware and software system is open-sourced at https://umi-gripper.github.io.\"\\n  }\\n}',\n",
       " '{\\n \"Summary\": {\\n  \"chinese_title\": \"SPAR：通过长期参与注意力实现个性化的基于内容的推荐\",\\n  \"chinese_summary\": \"利用用户的长期参与历史对于个性化的内容推荐至关重要。预训练语言模型 (PLM) 在 NLP 中的成功使其被用于对用户历史和候选项目进行编码，并将内容推荐构建为文本语义匹配任务。然而，现有工作在处理超长的用户历史文本和不足的用户与项目的交互方面仍面临挑战。在本文中，我们引入了一个基于内容的推荐框架 SPAR，该框架有效地解决了从长期的用户参与历史中提取整体用户兴趣的挑战。它通过利用 PLM、多注意力层和注意力稀疏机制以基于会话的方式对用户历史进行编码来实现这一点。用户端和项目端特征充分融合用于参与度预测，同时保持两端的独立表示，这对于实际模型部署是有效的。此外，我们利用大型语言模型 (LLM) 从用户参与历史中提取全局兴趣来增强用户画像。在两个基准数据集上的广泛实验表明，我们的框架优于现有的最先进 (SoTA) 方法。\",\\n  \"english_title\": \"SPAR: Personalized Content-Based Recommendation via Long Engagement Attention\",\\n  \"english_summary\": \"Leveraging users’ long engagement histories is essential for personalized content recommendations. The success of pretrained language models (PLMs) in NLP has led to their use in encoding user histories and candidate items, framing content recommendations as textual semantic matching tasks. However, existing works still struggle with processing very long user historical text and insufficient user-item interaction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles the challenges of holistic user interest extraction from the long user engagement history. It achieves so by leveraging PLM, poly-attention layers and attention sparsity mechanisms to encode user’s history in a session-based manner. The user and item side features are sufficiently fused for engagement prediction while maintaining standalone representations for both sides, which is efficient for practical model deployment. Moreover, we enhance user profiling by exploiting large language model (LLM) to extract global interests from user engagement history. Extensive experiments on two benchmark datasets demonstrate that our framework outperforms existing state-of-the-art (SoTA) methods.\"\\n }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"PaLM2-VAdapter：渐进式对齐语言模型成为强大的视觉语言适配器\",\\n    \"chinese_summary\": \"本文表明，渐进式对齐的语言模型可以有效地桥接冻结的视觉编码器和大语言模型（LLM）。虽然视觉编码器和 LLM 的基本架构和预训练方法已经得到广泛研究，但视觉语言适配器的架构和训练策略在最近的研究中差异很大。我们的研究对最先进的感知器重采样器架构进行了彻底探索，并建立了一个强大的基线。然而，我们观察到，具有感知器重采样器的视觉语言对齐表现出缓慢的收敛性和有限的可扩展性，缺乏直接的监督。为了解决这个问题，我们提出了PaLM2-VAdapter，采用渐进式对齐的语言模型作为视觉语言适配器。与具有感知器重采样器的强大基线相比，我们的方法凭经验证明具有更快的收敛性、更高的性能和更强的可扩展性。跨越图像和视频的各种视觉问答（VQA）和字幕任务的广泛实验表明，我们的模型展示了最先进的视觉理解和多模态推理能力。值得注意的是，我们的方法比最先进的大型视觉语言模型的参数少了30~70%，这标志着效率的显着提高。\",\\n    \"english_title\": \"PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter\",\\n    \"english_summary\": \"This paper demonstrates that a progressively aligned language model can effectively bridge frozen vision encoders and large language models (LLMs). While the fundamental architecture and pre-training methods of vision encoders and LLMs have been extensively studied, the architecture and training strategy of vision-language adapters vary significantly across recent works. Our research undertakes a thorough exploration of the state-of-the-art perceiver resampler architecture and builds a strong baseline. However, we observe that the vision-language alignment with perceiver resampler exhibits slow convergence and limited scalability with a lack of direct supervision. To address this issue, we propose PaLM2-VAdapter, employing a progressively aligned language model as the vision-language adapter. Compared to the strong baseline with perceiver resampler, our method empirically shows faster convergence, higher performance and stronger scalability. Extensive experiments across various Visual Question Answering (VQA) and captioning tasks on both images and videos demonstrate that our model exhibits state-of-the-art visual understanding and multi-modal reasoning capabilities. Notably, our method achieves these advancements with 30~70% fewer parameters than the state-of-the-art large vision-language models, marking a significant efficiency improvement.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"RLVF：从语言反馈中学习，避免过度概括\",\\n    \"chinese_summary\": \"随着大型语言模型（LLM）在各个行业和个体中的广泛采用，针对特定用户或用例，按照高级人类反馈对其进行调整的能力变得越来越重要。虽然 LLM 用户通常希望模型始终遵循广泛的原则，比如生成流畅的文本，但个别用户和用例有更细微的偏好。例如，用户可能要求 LLM 写出更简洁的工作电子邮件，但要写出更详细的个人电子邮件，这使得反馈具有上下文相关性。根据这些偏好调整模型具有挑战性：需要大量资源才能收集所有不同上下文中的偏好，并且在一个上下文中的模型微调会对其他上下文中的模型行为产生不可预测的影响。我们研究了使用语言反馈来调整模型的问题，这种反馈对于人们来说快速且容易提供（见图 1）。合并反馈的常见方法，例如监督上下文蒸馏 (SCD) 或强化学习，利用人类反馈 (RLHF)，使用示例级监管通过监督完成或偏好标签。此类方法需要大量用户提供的（偏好）数据语料库，而获取这些数据语料库可能既昂贵又繁琐。此外，它们不会限制反馈可能适用的上下文之外的模型行为，因此 LLM 可能会以意外的方式调整其行为，例如在偏好仅适用于个人电子邮件时生成更冗长的工作电子邮件。语言反馈对于人类来说更容易、更快速地提供。为此，另一种常见的方法是将此类语言反馈纳入提示中，可能通过迭代过程来持续添加其他反馈点。然而，此方法需要在所有未来查询中重新使用该提示。随着更多反馈的积累，包含许多上下文相关反馈的长提示会使推理变得昂贵；此外，识别哪些反馈应当在给定上下文中适用可能变得很困难。我们的目标是调整 LLM，以便在提供一个指定反馈的句子时，模型能够辨别反馈适用的哪些情况，并在未来输出中适当地纳入该反馈。我们提出了情境化批判与约束性偏好优化 (C3PO)，其中我们首先为超出反馈范围和未超出反馈范围的情境合成生成假设性提示。然后，我们对这些提示进行原始完成采样，不应用反馈，以及对所收集的反馈进行修订，以便为每个提示生成一个合成偏好数据集，指定反馈应当（和不应当）如何应用。接着，我们根据合成偏好数据对模型进行微调，同时最大限度地减少超出反馈适用范围的提示的原始模型的差异。我们的实验结果表明，我们的方法有效地将语言反馈应用于相关场景，同时保留了其他上下文的现有行为。对于人类和 GPT-4 生成的语言反馈，C3PO 有效地遵守给定的反馈，与上下文中基准相当，同时将过度概括减少了 30%。\",\\n    \"english_title\": \"RLVF: Learning from Verbal Feedback without Overgeneralization\",\\n    \"english_summary\": \"The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as “Don’t use emojis when drafting emails to my boss.” However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should not) be applied. It then fine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the original model for prompts where the feedback does not apply. Our experimental results indicate that our approach effectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts. For both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably to in-context baselines while reducing overgeneralization by 30%.\"\\n  }\\n}']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = []\n",
    "\n",
    "# Specify the file path\n",
    "output_file = \"./summary_test.txt\"\n",
    "\n",
    "for pdflink in pdflink_list:\n",
    "    pdf_text = extract_text_from_pdf(pdflink)\n",
    "    \n",
    "    summary = model.generate_content(prompt + pdf_text).text\n",
    "    summaries.append(summary)\n",
    "    \n",
    "    # Write the summaries to the file\n",
    "    with open(output_file, \"a\") as file:\n",
    "        file.write(summary + \"\\n\")\n",
    "\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the file path\n",
    "output_file = \"./summaries.pkl\"\n",
    "\n",
    "# Store the summaries using pickle\n",
    "with open(output_file, \"wb\") as file:\n",
    "    pickle.dump(summaries, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"./summaries.pkl\"\n",
    "\n",
    "# Read the pickle file\n",
    "with open(file_path, \"rb\") as file:\n",
    "    summaries = pickle.load(file)\n",
    "\n",
    "summary_dict_list = []\n",
    "\n",
    "for summary in summaries:\n",
    "    summary_dict = json.loads(summary)\n",
    "    summary_dict_list.append(summary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "today = datetime.today().strftime(\"%B %d, %Y\")\n",
    "passage = f\"LLM Daily Papers【{today}】\\n总结自: https://huggingface.co/papers\\n\\n\\n\"\n",
    "index = 0\n",
    "\n",
    "for summary_dict in summary_dict_list:\n",
    "    chinese_title = summary_dict[\"Summary\"][\"chinese_title\"]\n",
    "    chinese_summary = summary_dict[\"Summary\"][\"chinese_summary\"]\n",
    "    english_title = summary_dict[\"Summary\"][\"english_title\"]\n",
    "    english_summary = summary_dict[\"Summary\"][\"english_summary\"]\n",
    "    index += 1\n",
    "    \n",
    "    passage += f\"No.{index}\\n\\n{chinese_title}\\n\\n{chinese_summary}\\n\\n{english_title}\\n\\n{english_summary}\\n\\n\"\n",
    "\n",
    "passage = passage.strip()\n",
    "with open(\"passage.txt\", \"w\") as file:\n",
    "    file.write(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"无需提示即可进行思想链推理\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"以前的语言模型（LML）推理能力提升研究，主要是关注强化特定的提示技巧，比如少量提示或零提示思想链（CoT）提示。这些方</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">法虽然有效，但通常需要大量人工提示工程。研究从一个新颖的角度出发，提出了一个问题：LML是否可以在没有提示的情况下进行</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">有效的推理？研究发现，令人惊讶的是，可以通过简单地改变解码过程来从预训练的LML中提取CoT推理路径。研究没有使用传统的</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">贪婪解码，而是研究了前k个替代标记，发现CoT路径经常存在于这些序列中。这种方法不仅绕过了提示的混杂因素，还允许研究人</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">员评估LML的内在推理能力，研究人员还观察到，解码路径中存在CoT与模型对解码答案的较高置信度相关。置信度量可以有效区分C</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">oT路径和非CoT路径。各种推理基准的大量实证研究表明，所提出的CoT解码方法明显优于标准贪婪解码方法。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Chain-of-Thought Reasoning Without Prompting\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Prior research on enhancing the reasoning capabilities of large language models </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(LLMs) primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the decoding process. Rather than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conventional greedy decoding, we investigate the top-k alternative tokens, uncovering that CoT paths are frequently</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">assess the LLMs’ intrinsic reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">path correlates with a higher confidence in the model’s decoded answer. This confidence metric effectively </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the proposed CoT-decoding substantially outperforms the standard greedy decoding.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"生成性表征式指导微调\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"基于文本的语言问题皆可归结为生成或嵌入。目前，模型只擅长于其中一类任务。我们引入了生成性表征式指导微调（GRIT），训</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">练大型语言模型通过指令来区分生成和嵌入任务，以处理这两种任务。与其他开放模型相比，我们生成的 GRITLM 7B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">在海量文本嵌入基准（MTEB）上创下新的技术水平，并且在其规模范围内，在各种生成性任务上都优于所有模型。通过进一步扩展</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">，GRITLM 8x7B 优于所有我们尝试过的开放生成语言模型，同时仍然是最佳嵌入模型之一。值得注意的是，我们发现 GRIT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">匹配仅针对生成或嵌入数据进行的训练，因此我们可以统一两者，而不会损失性能。此外，通过 GRIT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">统一能够将针对长文档的检索增强生成（RAG）的速度提高大于 60%，不再需要单独的检索和生成模型。模型、代码等均可在 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/ContextualAI/gritlm 免费获得。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Generative Representational Instruction Tuning\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"All text-based language problems can be reduced to either generation or embedding.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Current models only perform well at one or the other. We introduce generative representational instruction tuning </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between them through instructions. Compared to other open models, our resulting GRITLM 7B sets a new state of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generative tasks. By scaling up further, GRITLM 8x7B outperforms all open generative language models that we tried </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">while still being among the best embedding models. Notably, we find that GRIT matches training on only generative </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">speeds up Retrieval-Augmented Generation (RAG) by &gt; 60% for long documents, by no longer requiring separate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval and generation models. Models, code, etc. are freely available at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/ContextualAI/gritlm.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"如何训练数据高效的 LLM\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"大语言模型 (LLM) 的训练非常昂贵。在本文中，我们研究了用于预训练 LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">的数据高效方法，即旨在优化模型质量和训练资源/数据消耗的帕累托前沿的技术。我们试图了解与基于 (i) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">计算成本高的数据质量估计和 (ii) 特征空间中覆盖范围和多样性最大化措施相关的数据选择例程相关的权衡。我们的第一项技术 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ASK-LLM，利用指令调整 LLM 的零样本推理能力来直接评估训练样本的质量。为了提高覆盖率，我们提出了 DENSITY </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">采样，它对数据分布进行建模以选择多样化的样本。在我们的 19 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">个采样器的比较中，涉及数百个评估任务和预训练运行，我们发现 ASK-LLM 和 DENSITY </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">是各自类别中的最佳方法。覆盖采样可以恢复完整数据的性能，而使用 ASK-LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">数据训练的模型始终优于完整数据训练——即使我们舍弃了 90% 的原始数据集，同时收敛速度提高了 70%。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"How to Train Data-Efficient LLMs\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"The training of large language models (LLMs) is expensive. In this paper, we study</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">diversity-based measures in the feature space. Our first technique, ASK-LLM, leverages the zero-shot reasoning </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">propose DENSITY sampling, which models the data distribution to select a diverse sample. In our comparison of 19 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">samplers, involving hundreds of evaluation tasks and pre-training runs, we find that ASK-LLM and DENSITY are the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">best methods in their respective categories. Coverage sampling can recover the performance of the full data, while </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models trained on ASK-LLM data consistently outperform full-data training—even when we reject 90% of the original </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dataset, while converging up to 70% faster.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"拥有极长背景记忆摘要的类人阅读代理\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"当前大型语言模型 (LLM) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">不仅受最大背景长度限制，而且无法稳健地消耗长输入。为了解决这些限制，我们提出了 ReadAgent，这是一种 LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">代理系统，在我们的实验中将有效背景长度增加了 20 倍。受人类互动阅读长文档方式的启发，我们将 ReadAgent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">实现为一个简单的提示系统，该系统利用 LLM 的先进语言能力来：(1) 决定将哪些内容一起存储在一个记忆片段中，(2) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">将这些记忆片段压缩成称为摘要记忆的简短片段记忆，以及 (3) 采取措施在原始文本中查找段落，如果 ReadAgent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">需要提醒自己相关的细节以完成任务。我们使用检索方法、使用原始的长背景和使用摘要记忆，针对基准评估了 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ReadAgent。在三个长文档阅读理解任务：QUALITY、NarrativeQA 和 QMSum 上执行了这些评估。ReadAgent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">在所有三个任务上都优于基准，同时将有效的背景窗口扩大了 3-20 倍。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"A Human-Inspired Reading Agent with Gist\\\\nMemory of Very Long Contexts\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Current Large Language Models (LLMs) are not only limited to some maximum context </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">against baselines using retrieval methods, using the original long contexts, and using the gist memories. These </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">evaluations are performed on three long-document reading comprehension tasks: QUALITY, NarrativeQA, and QMSum. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3 — </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">20x.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"BitDelta：你的微调可能只需要 1 比特\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"大型语言模型 (LLM) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">通常分两个阶段训练：在大规模互联网数据集上进行预训练，以及针对下游任务进行微调。鉴于预训练的计算需求较高，直观地认</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">为微调会给模型添加的信息更少，因此更易于压缩。我们通过将微调模型的权重分解为其预训练部分和一个附加的 delta </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">来探索这一假设。我们引入了一种简单的方法，BitDelta，该方法成功地将这个 delta 量化为 1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">比特，而不会损害性能。这一有趣的发现不仅凸显了微调过程中添加的信息潜在冗余，而且对微调模型的多租户服务和多租户存储</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">具有重大意义。通过启用使用单个高精度基础模型以及多个 1 比特的 delta，BitDelta 将 GPU 内存需求大幅减少了 10 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">倍以上，这也可以转化为多租户设置中增强的生成延迟。我们通过跨越 Llama-2 和 Mistral 模型系列以及高达 70B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">参数的模型进行实验对 BitDelta 进行了验证，展示了在所有测试设置下最小的性能下降。代码可在 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/bitdeltal上获得。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"BitDelta: Your Fine-Tune May Only Be Worth One Bit\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Large Language Models (LLMs) are typically trained in two phases: pre-training on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training, it’s intuitive to assume that fine-tuning adds less new information to the model, and is thus more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">down to 1 bit without compromising performance. This interesting finding not only highlights the potential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance degradation over all tested settings. Code is available at https://github.com/bitdeltal.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"OpenMathInstruct-1：一个具有180万数学指令调优数据集\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"最近的研究表明，合成的生成数据集在训练大型语言模型（LLM）方面具有巨大潜力，尤其是在获取目标技能方面。当前大规模的</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">数学指令调优数据集，如MetaMathQA（Yu等人，2024）和MAmmoTH（Yue等人，2024）是使用具有商业限制性许可证的闭源LLM的输出</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">构建的。限制在这些数据生成管道中使用开源LLM的一个关键原因是最好的闭源LLM（如GPT-4）和最好的开源LLM之间的数学技能差</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">距很大。基于开源LLM的最新进展、我们提出的提示新颖性以及一些蛮力扩展，我们构建了OpenMathInstruct-1，这是一个包含180</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">万个问题-解决方案对的数学指令调优数据集。该数据集是使用最近发布的、获得许可的Mixtral模型，通过综合GSM8K和MATH（两个</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">流行的数学推理基准）的代码解释器解决方案构建的。我们最好的模型OpenMath-CodeLlama-70B，在OpenMathInstruct-1的一个子</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">集上训练，在GSM8K上获得了84.6%的分数，在MATH上获得了50.7%的分数，这与最好的gpt蒸馏模型有竞争力。我们在商业上宽松的</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">许可证下发布了我们的代码、模型和OpenMathInstruct-1数据集。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"The huge development and inference costs associated with general-purpose large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language models (LLMs) have led to the rise of smaller, task-specific LLMs. Recent work has proposed creating these</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">domain/task-specific LLMs by generating high-quality synthetic data using powerful closed-source models such as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">GPT-3.5/4 (OpenAI et al., 2023) and training smaller models on the generated distillation data (Eldan and Li, 2023;</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Gunasekar et al., 2023; Li et al., 2023). For mathematical reasoning, our task of interest, all the current </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">state-of-the-art open-source models are gpt-distilled (Wang et al., 2024; Yue et al., 2024; Gou et al., 2024; Liao </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al., 2024). However, model development recipes relying on proprietary models like GPT-4 can have serious </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations: (a) legal restraints on how the finetuned models can be used,\\\\\" (b) generating data with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">closed-source models is typically costlier than state-of-the-art open-source models, and (c) these recipes lack </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reproducibility as closed-source model behaviors can vary significantly over time (Chen et al., 2023a). For </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">developing mathematical reasoning models, why are open-source models not used in place of closed-source models? To </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answer this, we compare GPT-4 with Mixtral 8x7B model (Jiang et al., 2024), currently one of the best open-source </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLMs at mathematical reasoning, by generating code-interpreter\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"用于文本到图像生成的自博弈微调扩散模型\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"我们介绍了 SPIN-Diffusion，这是一种用于扩散模型的自博弈微调算法。该结果是从 Stable</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diffusion v1.5 对 Pick-a-Pic 数据集的获胜图片进行微调得到的。用于生成以上图像的提示是从 Pick-a-Pic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">测试集中选取的。生成的图像在整体视觉吸引力和与提示的一致性方面表现出优异的性能。SPIN-Diffusion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">的特点是它不依赖于成对的人类偏好数据，为在仅提供每个文本提示一张图片的自定义数据集上进行微调提供了有用的工具。\",\\n'</span>\n",
       ",\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"We introduce SPIN-Diffusion, a self-play fine-tuning algorithm for diffusion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models. The results are fine-tuned from Stable Diffusion v1.5 on the winner images of the Pick-a-Pic dataset. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompts used for generating the above images are chosen from the Pick-a-Pic test set. The generated images </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">demonstrate superior performance in terms of overall visual attractiveness and coherence with the prompts. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">SPIN-Diffusion is featured by its independence from paired human preference data, offering a useful tool for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning on custom datasets with only single image per text prompt provided.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"可学习核函数的线性变换器是更好的上下文模型\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"在自然语言处理快速发展的领域内，推进语言模型 (LM) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">的亚二次架构是一项至关重要的工作。包括状态空间模型在内的当前创新最初因其在语言建模任务中超越了 Transformer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">性能而广受赞誉。然而，这些模型在必要的上下文学习能力方面暴露出缺陷，这是 Transformer 传统上表现出色的领域。Based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">模型作为一种混合解决方案应运而生，它将线性变换器与受指数函数的泰勒展开所启发的核函数相结合，并通过卷积网络进行了扩</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">展。它复制了 Transformer 在上下文中的熟练度，成为该领域的有力竞争者。在我们的工作中，我们对 Based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">核函数提出了一种独特而优雅的修改，通过多查询关联召回任务和在 Pile </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">数据集上展示的整体语言建模过程评估，放大了其上下文学习能力。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Linear Transformers with Learnable Kernel Functions are Better In-Context </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Models\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Advancing the frontier of subquadratic architectures for Language Models (LMs) is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models have revealed deficiencies in essential In-Context Learning capabilities — a domain where the Transformer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Transformer’s in-context adeptness, it became a strong contender in the field. In our work, we present a singular, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dataset.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"在一千兆字节的干草堆中寻找针头：循环记忆发现了大型语言模型错失的东西\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"本文讨论了使用生成式 Transformer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">模型处理长文档的挑战。为了评估不同的方法，我们引入了 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">BABILong，这是一个新的基准，旨在评估模型在提取和处理冗长文本中的分布式事实方面的能力。我们的评估包括 GPT-4 和 RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">的基准测试，结果表明常见的方法仅对高达 10^4 个元素的序列有效。相比之下，用循环记忆增强对 GPT-2 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">的微调使其能够处理涉及多达 10^7 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">个元素的任务。这一成就是一项重大飞跃，因为这是迄今为止任何开放神经网络模型处理的最长输入，展示了处理长序列能力的显</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">著提高。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"This paper tackles the challenge of processing lengthy documents using generative </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to 10^4 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">up to 10^7 elements. This achievement marks a substantial leap, as it is by far the longest input processed by any </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">open neural network model to date, demonstrating a significant improvement in the processing capabilities for long </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequences.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"DataDreamer：合成数据生成和可复制 LLM 工作流工具\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"大型语言模型 (LLM) 已成为 NLP </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">研究人员在各种任务中的一个主要工具。如今，很多研究人员在合成数据生成、任务评估、模型微调、蒸馏和其他模型循环研究工</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">作流中使用 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM。但是，在使用这些模型时会产生一些挑战，这些挑战源于其规模、封闭源代码的本质以及这些新兴工作流缺乏标准化工具。这</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">些模型的快速发展和这些独有的挑战对开放科学和使用这些模型的工作的可复制性产生了直接的不利影响。在本文中，我们介绍了 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">DataDreamer，这是一个开源 Python 库，允许研究人员编写简单的代码来实现强大的 LLM 工作流。DataDreamer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">还帮助研究人员遵守我们提议的最佳实践，以促进开放科学和可复制性。该库和文档可以在以下位置获得：https://github.com/da</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tadreamer-dev/DataDreamer。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Workflows\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Large language models (LLMs) have become a dominant and important tool for NLP </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">when using these models that stem from their scale, their closed source nature, and the lack of standardized </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">encourage open science and reproducibility. The library and documentation are available at: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/datadreamer-dev/DataDreamer.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"GaussianObject：只用四张图像获得带有高斯泼溅的高质量3D对象\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"重建和渲染高度稀疏视图中的3D对象对推广3D视觉技术的应用和改善用户体验至关重要。然而，稀疏视图的图像只包含非常有限的</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">3D信息，这带来了两个重大挑战：1）难以建立多视图一致性，因为匹配的图像太少；2）由于视图覆盖不足，对象信息部分省略或</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">高度压缩。为了应对这些挑战，我们提出了GaussianObject，这是一个使用高斯泼溅来表示和渲染3D对象的框架，它仅使用4个输入</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">图像就能实现高渲染质量。我们首先介绍了视觉包络和浮动消除技术，这些技术将结构先入为主地注入到初始优化过程中，以帮助</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">建立多视图一致性，从而产生一个粗略的3D高斯表示。然后，我们基于扩散模型构建了一个高斯修复模型来补充被省略的对象信息</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">，其中高斯体进一步得到细化。我们设计了一种自生成策略来获得用于训练修复模型的图像对。我们的GaussianObject在几个具有</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">挑战性的数据集上进行了评估，包括MipNeRF360、OmniObject3D和Openll-lumination，仅使用4个视图就获得了强大的重建结果，</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">并且明显优于以前最先进的方法。请访问我们的项目页面https://gaussianobject.github.io/。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Gaussian Splatting\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Reconstructing and rendering 3D objects from 2D images has been a longstanding and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">important topic, which plays critical roles in a vast range of real-life applications. One key factor that impedes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">users, especially ones without expert knowledge, from widely using these techniques is that usually dozens of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multi-view images need to be captured, which is cumbersome and sometimes impractical. Efficiently reconstructing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">high-quality 3D objects from highly sparse captured images is of great value for expediting downstream applications</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">such as 3D asset creation for game/movie production and AR/VR products.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"LLM Comparator：可视化分析，进行大型语言模型的并排评估\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"该 PDF 文档介绍了 LLM Comparator，这是一种用于可视化分析大型语言模型 (LLM) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">自动并排评估结果的交互式工具。该工具支持模型开发者和研究人员比较两个 LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">的响应质量，识别出何时以及为何一个模型的性能优于或劣于另一个模型，并探索两个模型响应之间的差异。LLM Comparator </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">提供了一个交互式表格，用于检查单个提示及其响应，以及一个可视化摘要，支持帮助理解模型差异的分析工作流。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Models\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"This PDF document presents LLM Comparator, an interactive tool for visual </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">analytics of automatic side-by-side evaluation results of large language models (LLMs). It enables model developers</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and researchers to compare the quality of responses from two LLMs, identify when and why one model performs better </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or worse than the other, and explore the differences in their responses. LLM Comparator provides an interactive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">table for inspecting individual prompts and their responses, and a visualization summary that supports analytical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">workflows for understanding model differences.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"LAVE：针对视频编辑的 LLM 助力代理与自然语言增强\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"LAVE 是一款视频编辑工具，它提供了 LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">助力代理与自然语言增强功能，可以帮助用户更轻松地编辑视频。例如，用户可以通过文字与 LAVE </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">的视频编辑代理互动，在整个编辑过程中获得实时帮助。此外，LAVE </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">还提供一系列语言增强功能，例如自动生成视频标题、显示视频摘要帮助用户快速了解视频内容、在缩略图库中通过点击视频即可</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">将其添加到编辑时间轴等，这些功能可以极大地简化和加速视频编辑流程，降低新手入门门槛。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"The Increasing demand for video content creation has been hindered by the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">technical expertise and effort typically required for video editing. This paper presents an integration of large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language models (LLMs) into the video editing workflow to lower these barriers, specifically focusing on assistance</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and language augmentation. Our proposed system, LAVE, offers an agent that interacts with users using natural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language, providing assistance with various editing tasks. It also incorporates language augmentation into the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">editing workflow, with features such as automated video summarization, video title generation, and quick video </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">selection from a gallery. We believe that LAVE simplifies the video editing process, making it more accessible to a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">wider range of users.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"基于函数调用的零样本对话状态追踪器中的大语言模型\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"大语言模型（LLM）因其在一般语境下的高级理解和生成能力而在会话系统中日益普及。然而，它们在任务导向对话（TOD）中的有</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">效性仍然不尽如人意，这不仅需要生成响应，还需要在特定任务和领域内进行有效的对话状态跟踪（DST）。在这项工作中，我们提</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">出了一种通过函数调用使用 LLM 解决 DST 的新方法 FNCTOD。该方法改进了零样本 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">DST，允许适应不同的领域，而无需大量数据收集或模型调整。我们的实验结果表明，我们的方法使用中等规模的开源和专有 LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">都取得了卓越的性能：在上下文中提示下，它使各种 7B 或 13B 参数模型能够超越 ChatGPT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">实现的先前最先进（SOTA）技术，并将 ChatGPT 的性能提高了 5.6% 的平均 JGA。GPT-3.5 和 GPT-4 的单个模型结果分别提高了 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">4.8% 和 14%。我们还表明，通过针对小规模的不同任务导向对话进行微调，我们可以为中等规模的模型（特别是 13B 参数 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLaMA2-Chat 模型）配备函数调用能力和与 ChatGPT 相媲美的 DST </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">性能，同时保持其聊天能力。我们将开源实验代码和模型。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Large Language Models as Zero-shot Dialogue State Tracker through Function </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Calling\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Large language models (LLMs) are increasingly prevalent in conversational systems </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approach FNCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">adaptation to diverse domains without extensive data collection or model tuning. Our experimental results </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT’s performance beating the SOTA by 5.6% Avg. JGA. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to ChatGPT while maintaining their chat capabilities. We will open-source experimental code and model.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">' \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"chinese_title\": \"构建廉价缩放：一种用于高分辨率适应的自级联扩散模型\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"扩散模型在图像和视频生成中已被证明非常有效；然而，由于单尺度训练数据，它们在生成不同大小的图像时仍然面临构图挑战。</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">针对高分辨率调整大型预训练扩散模型需要大量的计算和优化资源，但仍无法实现与低分辨率模型相当的生成能力。本文提出了一</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">种新颖的自级联扩散模型，该模型利用从经过良好训练的低分辨率模型中获得的丰富知识，快速适应高分辨率图像和视频生成，采</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">用无调优或廉价上采样器调优范例。自级联扩散模型集成了序列多尺度上采样器模块，能有效地适应更高的分辨率，同时保留了原</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">始构图和生成能力。我们进一步提出了一种枢轴引导噪声重新调度策略，以加速推理过程并改善局部结构细节。与完全微调相比，</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">我们的方法将训练速度提高了 5 倍，并且仅需要额外的 0.002M 调优参数。大量实验证明，我们的方法可以通过仅微调 10k </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">步，在几乎没有额外推理时间的情况下，快速适应更高分辨率的图像和视频合成。我们的代码将在 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/GuoLanging/Self-Cascade/ 发布。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"english_title\": \"Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"english_summary\": \"Diffusion models have proven to be highly effective in image and video generation; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">however, they still face composition challenges when generating images of varying sizes due to single-scale </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">training data. Adapting large pre-trained diffusion models for higher resolution demands substantial computational </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and optimization resources, yet achieving a generation capability comparable to low-resolution models remains </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">elusive. This paper proposes a novel self-cascade diffusion model that leverages the rich knowledge gained from a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">well-trained low-resolution model for rapid adaptation to higher-resolution image and video generation, employing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">either tuning-free or cheap upsampler tuning paradigms. Integrating a sequence of multi-scale upsampler modules, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the self-cascade diffusion model can efficiently adapt to a higher resolution, preserving the original composition </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and generation capabilities. We further propose a pivot-guided noise re-schedule strategy to speed up the inference</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">process and improve local structural details. Compared to full fine-tuning, our approach achieves a 5x training </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">speed-up and requires only an additional 0.002M tuning parameters. Extensive experiments demonstrate that our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approach can quickly adapt to higher resolution image and video synthesis by fine-tuning for just 10k steps, with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">virtually no additional inference time. Our code will be released at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/GuoLanging/Self-Cascade/.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">' }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"通用操作界面：在不使用野外机器人的情况下，在野外对机器进行教学\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"我们提出了通用操作界面（UMI）——一种数据收集和策略学习框架，该框架允许直接将野外人类演示中的技能转移到可部署机器策</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">略中。UMI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">采用手持抓手结合谨慎的界面设计，可实现便携、低成本且信息丰富的数据收集，以进行极具挑战性的双手动和动态操作演示。为</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">促进可部署策略学习，UMI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">包含一个精心设计的策略界面，该界面具备推理时延迟匹配和相对轨迹动作表示。所学策略与硬件无关，且可在多种机器人平台上</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">部署。配备这些功能，UMI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">框架解锁了新的机器人操作能力，仅通过针对每个任务更改训练数据，即可实现零次概括的动态、双手动、精确定位和长时间行为</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">。我们通过全面的真实实验展示了 UMI 的多功能性和有效性，在 UMI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">上学习的策略，在针对各种人类演示进行训练后，可零次概括到新环境和物体。UMI 的硬件和软件系统可在 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://umi-gripper.github.io 获取开源。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Robots\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"We present Universal Manipulation Interface (UMI)—a data collection and policy </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">manipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">by only changing the training data for each task. We demonstrate UMI’s versatility and efficacy with comprehensive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">real-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">trained on diverse human demonstrations. UMI’s hardware and software system is open-sourced at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://umi-gripper.github.io.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">' \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"chinese_title\": \"SPAR：通过长期参与注意力实现个性化的基于内容的推荐\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"chinese_summary\": \"利用用户的长期参与历史对于个性化的内容推荐至关重要。预训练语言模型 (PLM) 在 NLP </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">中的成功使其被用于对用户历史和候选项目进行编码，并将内容推荐构建为文本语义匹配任务。然而，现有工作在处理超长的用户</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">历史文本和不足的用户与项目的交互方面仍面临挑战。在本文中，我们引入了一个基于内容的推荐框架 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">SPAR，该框架有效地解决了从长期的用户参与历史中提取整体用户兴趣的挑战。它通过利用 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">PLM、多注意力层和注意力稀疏机制以基于会话的方式对用户历史进行编码来实现这一点。用户端和项目端特征充分融合用于参与度</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">预测，同时保持两端的独立表示，这对于实际模型部署是有效的。此外，我们利用大型语言模型 (LLM) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">从用户参与历史中提取全局兴趣来增强用户画像。在两个基准数据集上的广泛实验表明，我们的框架优于现有的最先进 (SoTA) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">方法。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"english_title\": \"SPAR: Personalized Content-Based Recommendation via Long Engagement Attention\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"english_summary\": \"Leveraging users’ long engagement histories is essential for personalized content </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recommendations. The success of pretrained language models (PLMs) in NLP has led to their use in encoding user </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">histories and candidate items, framing content recommendations as textual semantic matching tasks. However, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">existing works still struggle with processing very long user historical text and insufficient user-item </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interaction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the challenges of holistic user interest extraction from the long user engagement history. It achieves so by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">leveraging PLM, poly-attention layers and attention sparsity mechanisms to encode user’s history in a session-based</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">manner. The user and item side features are sufficiently fused for engagement prediction while maintaining </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">standalone representations for both sides, which is efficient for practical model deployment. Moreover, we enhance </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">user profiling by exploiting large language model (LLM) to extract global interests from user engagement history. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Extensive experiments on two benchmark datasets demonstrate that our framework outperforms existing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">state-of-the-art (SoTA) methods.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">' }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"PaLM2-VAdapter：渐进式对齐语言模型成为强大的视觉语言适配器\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"本文表明，渐进式对齐的语言模型可以有效地桥接冻结的视觉编码器和大语言模型（LLM）。虽然视觉编码器和 LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">的基本架构和预训练方法已经得到广泛研究，但视觉语言适配器的架构和训练策略在最近的研究中差异很大。我们的研究对最先进</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">的感知器重采样器架构进行了彻底探索，并建立了一个强大的基线。然而，我们观察到，具有感知器重采样器的视觉语言对齐表现</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">出缓慢的收敛性和有限的可扩展性，缺乏直接的监督。为了解决这个问题，我们提出了PaLM2-VAdapter，采用渐进式对齐的语言模</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">型作为视觉语言适配器。与具有感知器重采样器的强大基线相比，我们的方法凭经验证明具有更快的收敛性、更高的性能和更强的</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">可扩展性。跨越图像和视频的各种视觉问答（VQA）和字幕任务的广泛实验表明，我们的模型展示了最先进的视觉理解和多模态推理</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">能力。值得注意的是，我们的方法比最先进的大型视觉语言模型的参数少了30~70%，这标志着效率的显着提高。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adapter\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"This paper demonstrates that a progressively aligned language model can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">effectively bridge frozen vision encoders and large language models (LLMs). While the fundamental architecture and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training methods of vision encoders and LLMs have been extensively studied, the architecture and training </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">strategy of vision-language adapters vary significantly across recent works. Our research undertakes a thorough </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">exploration of the state-of-the-art perceiver resampler architecture and builds a strong baseline. However, we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">observe that the vision-language alignment with perceiver resampler exhibits slow convergence and limited </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scalability with a lack of direct supervision. To address this issue, we propose PaLM2-VAdapter, employing a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">progressively aligned language model as the vision-language adapter. Compared to the strong baseline with perceiver</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resampler, our method empirically shows faster convergence, higher performance and stronger scalability. Extensive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments across various Visual Question Answering (VQA) and captioning tasks on both images and videos </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">demonstrate that our model exhibits state-of-the-art visual understanding and multi-modal reasoning capabilities. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Notably, our method achieves these advancements with 30~70% fewer parameters than the state-of-the-art large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vision-language models, marking a significant efficiency improvement.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"RLVF：从语言反馈中学习，避免过度概括\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"随着大型语言模型（LLM）在各个行业和个体中的广泛采用，针对特定用户或用例，按照高级人类反馈对其进行调整的能力变得越</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">来越重要。虽然 LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">用户通常希望模型始终遵循广泛的原则，比如生成流畅的文本，但个别用户和用例有更细微的偏好。例如，用户可能要求 LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">写出更简洁的工作电子邮件，但要写出更详细的个人电子邮件，这使得反馈具有上下文相关性。根据这些偏好调整模型具有挑战性</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">：需要大量资源才能收集所有不同上下文中的偏好，并且在一个上下文中的模型微调会对其他上下文中的模型行为产生不可预测的</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">影响。我们研究了使用语言反馈来调整模型的问题，这种反馈对于人们来说快速且容易提供（见图 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1）。合并反馈的常见方法，例如监督上下文蒸馏 (SCD) 或强化学习，利用人类反馈 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(RLHF)，使用示例级监管通过监督完成或偏好标签。此类方法需要大量用户提供的（偏好）数据语料库，而获取这些数据语料库可</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">能既昂贵又繁琐。此外，它们不会限制反馈可能适用的上下文之外的模型行为，因此 LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">可能会以意外的方式调整其行为，例如在偏好仅适用于个人电子邮件时生成更冗长的工作电子邮件。语言反馈对于人类来说更容易</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">、更快速地提供。为此，另一种常见的方法是将此类语言反馈纳入提示中，可能通过迭代过程来持续添加其他反馈点。然而，此方</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">法需要在所有未来查询中重新使用该提示。随着更多反馈的积累，包含许多上下文相关反馈的长提示会使推理变得昂贵；此外，识</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">别哪些反馈应当在给定上下文中适用可能变得很困难。我们的目标是调整 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM，以便在提供一个指定反馈的句子时，模型能够辨别反馈适用的哪些情况，并在未来输出中适当地纳入该反馈。我们提出了情境</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">化批判与约束性偏好优化 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(C3PO)，其中我们首先为超出反馈范围和未超出反馈范围的情境合成生成假设性提示。然后，我们对这些提示进行原始完成采样，</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">不应用反馈，以及对所收集的反馈进行修订，以便为每个提示生成一个合成偏好数据集，指定反馈应当（和不应当）如何应用。接</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">着，我们根据合成偏好数据对模型进行微调，同时最大限度地减少超出反馈适用范围的提示的原始模型的差异。我们的实验结果表</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">明，我们的方法有效地将语言反馈应用于相关场景，同时保留了其他上下文的现有行为。对于人类和 GPT-4 生成的语言反馈，C3PO</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">有效地遵守给定的反馈，与上下文中基准相当，同时将过度概括减少了 30%。\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"RLVF: Learning from Verbal Feedback without Overgeneralization\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"The diversity of contexts in which large language models (LLMs) are deployed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as “Don’t</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">use emojis when drafting emails to my boss.” However, while writing high-level feedback is far simpler than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">small synthetic preference dataset specifying how the feedback should (and should not) be applied. It then </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">original model for prompts where the feedback does not apply. Our experimental results indicate that our approach </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">effectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">For both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to in-context baselines while reducing overgeneralization by 30%.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"无需提示即可进行思想链推理\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"以前的语言模型（LML）推理能力提升研究，主要是关注强化特定的提示技巧，比如少量提示或零提示思想链（CoT）提示。这些方\u001b[0m\n",
       "\u001b[32m法虽然有效，但通常需要大量人工提示工程。研究从一个新颖的角度出发，提出了一个问题：LML是否可以在没有提示的情况下进行\u001b[0m\n",
       "\u001b[32m有效的推理？研究发现，令人惊讶的是，可以通过简单地改变解码过程来从预训练的LML中提取CoT推理路径。研究没有使用传统的\u001b[0m\n",
       "\u001b[32m贪婪解码，而是研究了前k个替代标记，发现CoT路径经常存在于这些序列中。这种方法不仅绕过了提示的混杂因素，还允许研究人\u001b[0m\n",
       "\u001b[32m员评估LML的内在推理能力，研究人员还观察到，解码路径中存在CoT与模型对解码答案的较高置信度相关。置信度量可以有效区分C\u001b[0m\n",
       "\u001b[32moT路径和非CoT路径。各种推理基准的大量实证研究表明，所提出的CoT解码方法明显优于标准贪婪解码方法。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"Chain-of-Thought Reasoning Without Prompting\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Prior research on enhancing the reasoning capabilities of large language models \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCoT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mprompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a \u001b[0m\n",
       "\u001b[32mnovel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, \u001b[0m\n",
       "\u001b[32mCoT reasoning paths can be elicited from pre-trained LLMs by simply altering the decoding process. Rather than \u001b[0m\n",
       "\u001b[32mconventional greedy decoding, we investigate the top-k alternative tokens, uncovering that CoT paths are frequently\u001b[0m\n",
       "\u001b[32minherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to \u001b[0m\n",
       "\u001b[32massess the LLMs’ intrinsic reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding \u001b[0m\n",
       "\u001b[32mpath correlates with a higher confidence in the model’s decoded answer. This confidence metric effectively \u001b[0m\n",
       "\u001b[32mdifferentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that\u001b[0m\n",
       "\u001b[32mthe proposed CoT-decoding substantially outperforms the standard greedy decoding.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"生成性表征式指导微调\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"基于文本的语言问题皆可归结为生成或嵌入。目前，模型只擅长于其中一类任务。我们引入了生成性表征式指导微调（GRIT），训\u001b[0m\n",
       "\u001b[32m练大型语言模型通过指令来区分生成和嵌入任务，以处理这两种任务。与其他开放模型相比，我们生成的 GRITLM 7B \u001b[0m\n",
       "\u001b[32m在海量文本嵌入基准（MTEB）上创下新的技术水平，并且在其规模范围内，在各种生成性任务上都优于所有模型。通过进一步扩展\u001b[0m\n",
       "\u001b[32m，GRITLM 8x7B 优于所有我们尝试过的开放生成语言模型，同时仍然是最佳嵌入模型之一。值得注意的是，我们发现 GRIT \u001b[0m\n",
       "\u001b[32m匹配仅针对生成或嵌入数据进行的训练，因此我们可以统一两者，而不会损失性能。此外，通过 GRIT \u001b[0m\n",
       "\u001b[32m统一能够将针对长文档的检索增强生成（RAG）的速度提高大于 60%，不再需要单独的检索和生成模型。模型、代码等均可在 \u001b[0m\n",
       "\u001b[32mhttps://github.com/ContextualAI/gritlm 免费获得。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"Generative Representational Instruction Tuning\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"All text-based language problems can be reduced to either generation or embedding.\u001b[0m\n",
       "\u001b[32mCurrent models only perform well at one or the other. We introduce generative representational instruction tuning \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mGRIT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m whereby a large language model is trained to handle both generative and embedding tasks by distinguishing \u001b[0m\n",
       "\u001b[32mbetween them through instructions. Compared to other open models, our resulting GRITLM 7B sets a new state of the \u001b[0m\n",
       "\u001b[32mart on the Massive Text Embedding Benchmark \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMTEB\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and outperforms all models up to its size on a range of \u001b[0m\n",
       "\u001b[32mgenerative tasks. By scaling up further, GRITLM 8x7B outperforms all open generative language models that we tried \u001b[0m\n",
       "\u001b[32mwhile still being among the best embedding models. Notably, we find that GRIT matches training on only generative \u001b[0m\n",
       "\u001b[32mor embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT \u001b[0m\n",
       "\u001b[32mspeeds up Retrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m by > 60% for long documents, by no longer requiring separate \u001b[0m\n",
       "\u001b[32mretrieval and generation models. Models, code, etc. are freely available at \u001b[0m\n",
       "\u001b[32mhttps://github.com/ContextualAI/gritlm.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"如何训练数据高效的 LLM\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"大语言模型 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m 的训练非常昂贵。在本文中，我们研究了用于预训练 LLM \u001b[0m\n",
       "\u001b[32m的数据高效方法，即旨在优化模型质量和训练资源/数据消耗的帕累托前沿的技术。我们试图了解与基于 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m计算成本高的数据质量估计和 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mii\u001b[0m\u001b[32m)\u001b[0m\u001b[32m 特征空间中覆盖范围和多样性最大化措施相关的数据选择例程相关的权衡。我们的第一项技术 \u001b[0m\n",
       "\u001b[32mASK-LLM，利用指令调整 LLM 的零样本推理能力来直接评估训练样本的质量。为了提高覆盖率，我们提出了 DENSITY \u001b[0m\n",
       "\u001b[32m采样，它对数据分布进行建模以选择多样化的样本。在我们的 19 \u001b[0m\n",
       "\u001b[32m个采样器的比较中，涉及数百个评估任务和预训练运行，我们发现 ASK-LLM 和 DENSITY \u001b[0m\n",
       "\u001b[32m是各自类别中的最佳方法。覆盖采样可以恢复完整数据的性能，而使用 ASK-LLM \u001b[0m\n",
       "\u001b[32m数据训练的模型始终优于完整数据训练——即使我们舍弃了 90% 的原始数据集，同时收敛速度提高了 70%。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"How to Train Data-Efficient LLMs\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"The training of large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is expensive. In this paper, we study\u001b[0m\n",
       "\u001b[32mdata-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model\u001b[0m\n",
       "\u001b[32mquality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection \u001b[0m\n",
       "\u001b[32mroutines based on \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m expensive-to-compute data-quality estimates, and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mii\u001b[0m\u001b[32m)\u001b[0m\u001b[32m maximization of coverage and \u001b[0m\n",
       "\u001b[32mdiversity-based measures in the feature space. Our first technique, ASK-LLM, leverages the zero-shot reasoning \u001b[0m\n",
       "\u001b[32mcapabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we\u001b[0m\n",
       "\u001b[32mpropose DENSITY sampling, which models the data distribution to select a diverse sample. In our comparison of 19 \u001b[0m\n",
       "\u001b[32msamplers, involving hundreds of evaluation tasks and pre-training runs, we find that ASK-LLM and DENSITY are the \u001b[0m\n",
       "\u001b[32mbest methods in their respective categories. Coverage sampling can recover the performance of the full data, while \u001b[0m\n",
       "\u001b[32mmodels trained on ASK-LLM data consistently outperform full-data training—even when we reject 90% of the original \u001b[0m\n",
       "\u001b[32mdataset, while converging up to 70% faster.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"拥有极长背景记忆摘要的类人阅读代理\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"当前大型语言模型 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m不仅受最大背景长度限制，而且无法稳健地消耗长输入。为了解决这些限制，我们提出了 ReadAgent，这是一种 LLM \u001b[0m\n",
       "\u001b[32m代理系统，在我们的实验中将有效背景长度增加了 20 倍。受人类互动阅读长文档方式的启发，我们将 ReadAgent \u001b[0m\n",
       "\u001b[32m实现为一个简单的提示系统，该系统利用 LLM 的先进语言能力来：\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m 决定将哪些内容一起存储在一个记忆片段中，\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m将这些记忆片段压缩成称为摘要记忆的简短片段记忆，以及 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m 采取措施在原始文本中查找段落，如果 ReadAgent \u001b[0m\n",
       "\u001b[32m需要提醒自己相关的细节以完成任务。我们使用检索方法、使用原始的长背景和使用摘要记忆，针对基准评估了 \u001b[0m\n",
       "\u001b[32mReadAgent。在三个长文档阅读理解任务：QUALITY、NarrativeQA 和 QMSum 上执行了这些评估。ReadAgent \u001b[0m\n",
       "\u001b[32m在所有三个任务上都优于基准，同时将有效的背景窗口扩大了 3-20 倍。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"A Human-Inspired Reading Agent with Gist\\\\nMemory of Very Long Contexts\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Current Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are not only limited to some maximum context \u001b[0m\n",
       "\u001b[32mlength, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, \u001b[0m\n",
       "\u001b[32man LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans \u001b[0m\n",
       "\u001b[32minteractively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced \u001b[0m\n",
       "\u001b[32mlanguage capabilities of LLMs to \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m decide what content to store together in a memory episode, \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m compress those \u001b[0m\n",
       "\u001b[32mmemory episodes into short episodic memories called gist memories, and \u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m take actions to look up passages in the \u001b[0m\n",
       "\u001b[32moriginal text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent \u001b[0m\n",
       "\u001b[32magainst baselines using retrieval methods, using the original long contexts, and using the gist memories. These \u001b[0m\n",
       "\u001b[32mevaluations are performed on three long-document reading comprehension tasks: QUALITY, NarrativeQA, and QMSum. \u001b[0m\n",
       "\u001b[32mReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3 — \u001b[0m\n",
       "\u001b[32m20x.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"BitDelta：你的微调可能只需要 1 比特\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"大型语言模型 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m通常分两个阶段训练：在大规模互联网数据集上进行预训练，以及针对下游任务进行微调。鉴于预训练的计算需求较高，直观地认\u001b[0m\n",
       "\u001b[32m为微调会给模型添加的信息更少，因此更易于压缩。我们通过将微调模型的权重分解为其预训练部分和一个附加的 delta \u001b[0m\n",
       "\u001b[32m来探索这一假设。我们引入了一种简单的方法，BitDelta，该方法成功地将这个 delta 量化为 1 \u001b[0m\n",
       "\u001b[32m比特，而不会损害性能。这一有趣的发现不仅凸显了微调过程中添加的信息潜在冗余，而且对微调模型的多租户服务和多租户存储\u001b[0m\n",
       "\u001b[32m具有重大意义。通过启用使用单个高精度基础模型以及多个 1 比特的 delta，BitDelta 将 GPU 内存需求大幅减少了 10 \u001b[0m\n",
       "\u001b[32m倍以上，这也可以转化为多租户设置中增强的生成延迟。我们通过跨越 Llama-2 和 Mistral 模型系列以及高达 70B \u001b[0m\n",
       "\u001b[32m参数的模型进行实验对 BitDelta 进行了验证，展示了在所有测试设置下最小的性能下降。代码可在 \u001b[0m\n",
       "\u001b[32mhttps://github.com/bitdeltal上获得。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"BitDelta: Your Fine-Tune May Only Be Worth One Bit\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are typically trained in two phases: pre-training on \u001b[0m\n",
       "\u001b[32mlarge internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of \u001b[0m\n",
       "\u001b[32mpre-training, it’s intuitive to assume that fine-tuning adds less new information to the model, and is thus more \u001b[0m\n",
       "\u001b[32mcompressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained \u001b[0m\n",
       "\u001b[32mcomponents and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta\u001b[0m\n",
       "\u001b[32mdown to 1 bit without compromising performance. This interesting finding not only highlights the potential \u001b[0m\n",
       "\u001b[32mredundancy of information added during fine-tuning, but also has significant implications for the multi-tenant \u001b[0m\n",
       "\u001b[32mserving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model \u001b[0m\n",
       "\u001b[32maccompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which\u001b[0m\n",
       "\u001b[32mcan also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through \u001b[0m\n",
       "\u001b[32mexperiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal \u001b[0m\n",
       "\u001b[32mperformance degradation over all tested settings. Code is available at https://github.com/bitdeltal.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"OpenMathInstruct-1：一个具有180万数学指令调优数据集\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"最近的研究表明，合成的生成数据集在训练大型语言模型（LLM）方面具有巨大潜力，尤其是在获取目标技能方面。当前大规模的\u001b[0m\n",
       "\u001b[32m数学指令调优数据集，如MetaMathQA（Yu等人，2024）和MAmmoTH（Yue等人，2024）是使用具有商业限制性许可证的闭源LLM的输出\u001b[0m\n",
       "\u001b[32m构建的。限制在这些数据生成管道中使用开源LLM的一个关键原因是最好的闭源LLM（如GPT-4）和最好的开源LLM之间的数学技能差\u001b[0m\n",
       "\u001b[32m距很大。基于开源LLM的最新进展、我们提出的提示新颖性以及一些蛮力扩展，我们构建了OpenMathInstruct-1，这是一个包含180\u001b[0m\n",
       "\u001b[32m万个问题-解决方案对的数学指令调优数据集。该数据集是使用最近发布的、获得许可的Mixtral模型，通过综合GSM8K和MATH（两个\u001b[0m\n",
       "\u001b[32m流行的数学推理基准）的代码解释器解决方案构建的。我们最好的模型OpenMath-CodeLlama-70B，在OpenMathInstruct-1的一个子\u001b[0m\n",
       "\u001b[32m集上训练，在GSM8K上获得了84.6%的分数，在MATH上获得了50.7%的分数，这与最好的gpt蒸馏模型有竞争力。我们在商业上宽松的\u001b[0m\n",
       "\u001b[32m许可证下发布了我们的代码、模型和OpenMathInstruct-1数据集。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"The huge development and inference costs associated with general-purpose large \u001b[0m\n",
       "\u001b[32mlanguage models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have led to the rise of smaller, task-specific LLMs. Recent work has proposed creating these\u001b[0m\n",
       "\u001b[32mdomain/task-specific LLMs by generating high-quality synthetic data using powerful closed-source models such as \u001b[0m\n",
       "\u001b[32mGPT-3.5/4 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOpenAI et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and training smaller models on the generated distillation data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mEldan and Li, 2023;\u001b[0m\n",
       "\u001b[32mGunasekar et al., 2023; Li et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. For mathematical reasoning, our task of interest, all the current \u001b[0m\n",
       "\u001b[32mstate-of-the-art open-source models are gpt-distilled \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWang et al., 2024; Yue et al., 2024; Gou et al., 2024; Liao \u001b[0m\n",
       "\u001b[32met al., 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. However, model development recipes relying on proprietary models like GPT-4 can have serious \u001b[0m\n",
       "\u001b[32mlimitations: \u001b[0m\u001b[32m(\u001b[0m\u001b[32ma\u001b[0m\u001b[32m)\u001b[0m\u001b[32m legal restraints on how the finetuned models can be used,\\\\\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m generating data with \u001b[0m\n",
       "\u001b[32mclosed-source models is typically costlier than state-of-the-art open-source models, and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m these recipes lack \u001b[0m\n",
       "\u001b[32mreproducibility as closed-source model behaviors can vary significantly over time \u001b[0m\u001b[32m(\u001b[0m\u001b[32mChen et al., 2023a\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. For \u001b[0m\n",
       "\u001b[32mdeveloping mathematical reasoning models, why are open-source models not used in place of closed-source models? To \u001b[0m\n",
       "\u001b[32manswer this, we compare GPT-4 with Mixtral 8x7B model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mJiang et al., 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, currently one of the best open-source \u001b[0m\n",
       "\u001b[32mLLMs at mathematical reasoning, by generating code-interpreter\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"用于文本到图像生成的自博弈微调扩散模型\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"我们介绍了 SPIN-Diffusion，这是一种用于扩散模型的自博弈微调算法。该结果是从 Stable\u001b[0m\n",
       "\u001b[32mDiffusion v1.5 对 Pick-a-Pic 数据集的获胜图片进行微调得到的。用于生成以上图像的提示是从 Pick-a-Pic \u001b[0m\n",
       "\u001b[32m测试集中选取的。生成的图像在整体视觉吸引力和与提示的一致性方面表现出优异的性能。SPIN-Diffusion \u001b[0m\n",
       "\u001b[32m的特点是它不依赖于成对的人类偏好数据，为在仅提供每个文本提示一张图片的自定义数据集上进行微调提供了有用的工具。\",\\n'\u001b[0m\n",
       ",\n",
       "        \u001b[32m'    \"english_title\": \"Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"We introduce SPIN-Diffusion, a self-play fine-tuning algorithm for diffusion \u001b[0m\n",
       "\u001b[32mmodels. The results are fine-tuned from Stable Diffusion v1.5 on the winner images of the Pick-a-Pic dataset. The \u001b[0m\n",
       "\u001b[32mprompts used for generating the above images are chosen from the Pick-a-Pic test set. The generated images \u001b[0m\n",
       "\u001b[32mdemonstrate superior performance in terms of overall visual attractiveness and coherence with the prompts. \u001b[0m\n",
       "\u001b[32mSPIN-Diffusion is featured by its independence from paired human preference data, offering a useful tool for \u001b[0m\n",
       "\u001b[32mfine-tuning on custom datasets with only single image per text prompt provided.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"可学习核函数的线性变换器是更好的上下文模型\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"在自然语言处理快速发展的领域内，推进语言模型 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m的亚二次架构是一项至关重要的工作。包括状态空间模型在内的当前创新最初因其在语言建模任务中超越了 Transformer \u001b[0m\n",
       "\u001b[32m性能而广受赞誉。然而，这些模型在必要的上下文学习能力方面暴露出缺陷，这是 Transformer 传统上表现出色的领域。Based \u001b[0m\n",
       "\u001b[32m模型作为一种混合解决方案应运而生，它将线性变换器与受指数函数的泰勒展开所启发的核函数相结合，并通过卷积网络进行了扩\u001b[0m\n",
       "\u001b[32m展。它复制了 Transformer 在上下文中的熟练度，成为该领域的有力竞争者。在我们的工作中，我们对 Based \u001b[0m\n",
       "\u001b[32m核函数提出了一种独特而优雅的修改，通过多查询关联召回任务和在 Pile \u001b[0m\n",
       "\u001b[32m数据集上展示的整体语言建模过程评估，放大了其上下文学习能力。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"Linear Transformers with Learnable Kernel Functions are Better In-Context \u001b[0m\n",
       "\u001b[32mModels\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Advancing the frontier of subquadratic architectures for Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is \u001b[0m\n",
       "\u001b[32mcrucial in the rapidly evolving field of natural language processing. Current innovations, including State Space \u001b[0m\n",
       "\u001b[32mModels, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these\u001b[0m\n",
       "\u001b[32mmodels have revealed deficiencies in essential In-Context Learning capabilities — a domain where the Transformer \u001b[0m\n",
       "\u001b[32mtraditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel \u001b[0m\n",
       "\u001b[32minspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the \u001b[0m\n",
       "\u001b[32mTransformer’s in-context adeptness, it became a strong contender in the field. In our work, we present a singular, \u001b[0m\n",
       "\u001b[32melegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the \u001b[0m\n",
       "\u001b[32mMulti-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile \u001b[0m\n",
       "\u001b[32mdataset.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"在一千兆字节的干草堆中寻找针头：循环记忆发现了大型语言模型错失的东西\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"本文讨论了使用生成式 Transformer \u001b[0m\n",
       "\u001b[32m模型处理长文档的挑战。为了评估不同的方法，我们引入了 \u001b[0m\n",
       "\u001b[32mBABILong，这是一个新的基准，旨在评估模型在提取和处理冗长文本中的分布式事实方面的能力。我们的评估包括 GPT-4 和 RAG \u001b[0m\n",
       "\u001b[32m的基准测试，结果表明常见的方法仅对高达 10^4 个元素的序列有效。相比之下，用循环记忆增强对 GPT-2 \u001b[0m\n",
       "\u001b[32m的微调使其能够处理涉及多达 10^7 \u001b[0m\n",
       "\u001b[32m个元素的任务。这一成就是一项重大飞跃，因为这是迄今为止任何开放神经网络模型处理的最长输入，展示了处理长序列能力的显\u001b[0m\n",
       "\u001b[32m著提高。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"This paper tackles the challenge of processing lengthy documents using generative \u001b[0m\n",
       "\u001b[32mtransformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess \u001b[0m\n",
       "\u001b[32mmodel capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which \u001b[0m\n",
       "\u001b[32mincludes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to 10^4 \u001b[0m\n",
       "\u001b[32melements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving \u001b[0m\n",
       "\u001b[32mup to 10^7 elements. This achievement marks a substantial leap, as it is by far the longest input processed by any \u001b[0m\n",
       "\u001b[32mopen neural network model to date, demonstrating a significant improvement in the processing capabilities for long \u001b[0m\n",
       "\u001b[32msequences.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"DataDreamer：合成数据生成和可复制 LLM 工作流工具\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"大型语言模型 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m 已成为 NLP \u001b[0m\n",
       "\u001b[32m研究人员在各种任务中的一个主要工具。如今，很多研究人员在合成数据生成、任务评估、模型微调、蒸馏和其他模型循环研究工\u001b[0m\n",
       "\u001b[32m作流中使用 \u001b[0m\n",
       "\u001b[32mLLM。但是，在使用这些模型时会产生一些挑战，这些挑战源于其规模、封闭源代码的本质以及这些新兴工作流缺乏标准化工具。这\u001b[0m\n",
       "\u001b[32m些模型的快速发展和这些独有的挑战对开放科学和使用这些模型的工作的可复制性产生了直接的不利影响。在本文中，我们介绍了 \u001b[0m\n",
       "\u001b[32mDataDreamer，这是一个开源 Python 库，允许研究人员编写简单的代码来实现强大的 LLM 工作流。DataDreamer \u001b[0m\n",
       "\u001b[32m还帮助研究人员遵守我们提议的最佳实践，以促进开放科学和可复制性。该库和文档可以在以下位置获得：https://github.com/da\u001b[0m\n",
       "\u001b[32mtadreamer-dev/DataDreamer。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM \u001b[0m\n",
       "\u001b[32mWorkflows\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have become a dominant and important tool for NLP \u001b[0m\n",
       "\u001b[32mresearchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task \u001b[0m\n",
       "\u001b[32mevaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise \u001b[0m\n",
       "\u001b[32mwhen using these models that stem from their scale, their closed source nature, and the lack of standardized \u001b[0m\n",
       "\u001b[32mtooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique \u001b[0m\n",
       "\u001b[32mchallenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In \u001b[0m\n",
       "\u001b[32mthis paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to\u001b[0m\n",
       "\u001b[32mimplement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to \u001b[0m\n",
       "\u001b[32mencourage open science and reproducibility. The library and documentation are available at: \u001b[0m\n",
       "\u001b[32mhttps://github.com/datadreamer-dev/DataDreamer.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"GaussianObject：只用四张图像获得带有高斯泼溅的高质量3D对象\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"重建和渲染高度稀疏视图中的3D对象对推广3D视觉技术的应用和改善用户体验至关重要。然而，稀疏视图的图像只包含非常有限的\u001b[0m\n",
       "\u001b[32m3D信息，这带来了两个重大挑战：1）难以建立多视图一致性，因为匹配的图像太少；2）由于视图覆盖不足，对象信息部分省略或\u001b[0m\n",
       "\u001b[32m高度压缩。为了应对这些挑战，我们提出了GaussianObject，这是一个使用高斯泼溅来表示和渲染3D对象的框架，它仅使用4个输入\u001b[0m\n",
       "\u001b[32m图像就能实现高渲染质量。我们首先介绍了视觉包络和浮动消除技术，这些技术将结构先入为主地注入到初始优化过程中，以帮助\u001b[0m\n",
       "\u001b[32m建立多视图一致性，从而产生一个粗略的3D高斯表示。然后，我们基于扩散模型构建了一个高斯修复模型来补充被省略的对象信息\u001b[0m\n",
       "\u001b[32m，其中高斯体进一步得到细化。我们设计了一种自生成策略来获得用于训练修复模型的图像对。我们的GaussianObject在几个具有\u001b[0m\n",
       "\u001b[32m挑战性的数据集上进行了评估，包括MipNeRF360、OmniObject3D和Openll-lumination，仅使用4个视图就获得了强大的重建结果，\u001b[0m\n",
       "\u001b[32m并且明显优于以前最先进的方法。请访问我们的项目页面https://gaussianobject.github.io/。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with \u001b[0m\n",
       "\u001b[32mGaussian Splatting\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Reconstructing and rendering 3D objects from 2D images has been a longstanding and\u001b[0m\n",
       "\u001b[32mimportant topic, which plays critical roles in a vast range of real-life applications. One key factor that impedes \u001b[0m\n",
       "\u001b[32musers, especially ones without expert knowledge, from widely using these techniques is that usually dozens of \u001b[0m\n",
       "\u001b[32mmulti-view images need to be captured, which is cumbersome and sometimes impractical. Efficiently reconstructing \u001b[0m\n",
       "\u001b[32mhigh-quality 3D objects from highly sparse captured images is of great value for expediting downstream applications\u001b[0m\n",
       "\u001b[32msuch as 3D asset creation for game/movie production and AR/VR products.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"LLM Comparator：可视化分析，进行大型语言模型的并排评估\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"该 PDF 文档介绍了 LLM Comparator，这是一种用于可视化分析大型语言模型 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m自动并排评估结果的交互式工具。该工具支持模型开发者和研究人员比较两个 LLM \u001b[0m\n",
       "\u001b[32m的响应质量，识别出何时以及为何一个模型的性能优于或劣于另一个模型，并探索两个模型响应之间的差异。LLM Comparator \u001b[0m\n",
       "\u001b[32m提供了一个交互式表格，用于检查单个提示及其响应，以及一个可视化摘要，支持帮助理解模型差异的分析工作流。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language \u001b[0m\n",
       "\u001b[32mModels\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"This PDF document presents LLM Comparator, an interactive tool for visual \u001b[0m\n",
       "\u001b[32manalytics of automatic side-by-side evaluation results of large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. It enables model developers\u001b[0m\n",
       "\u001b[32mand researchers to compare the quality of responses from two LLMs, identify when and why one model performs better \u001b[0m\n",
       "\u001b[32mor worse than the other, and explore the differences in their responses. LLM Comparator provides an interactive \u001b[0m\n",
       "\u001b[32mtable for inspecting individual prompts and their responses, and a visualization summary that supports analytical \u001b[0m\n",
       "\u001b[32mworkflows for understanding model differences.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"LAVE：针对视频编辑的 LLM 助力代理与自然语言增强\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"LAVE 是一款视频编辑工具，它提供了 LLM \u001b[0m\n",
       "\u001b[32m助力代理与自然语言增强功能，可以帮助用户更轻松地编辑视频。例如，用户可以通过文字与 LAVE \u001b[0m\n",
       "\u001b[32m的视频编辑代理互动，在整个编辑过程中获得实时帮助。此外，LAVE \u001b[0m\n",
       "\u001b[32m还提供一系列语言增强功能，例如自动生成视频标题、显示视频摘要帮助用户快速了解视频内容、在缩略图库中通过点击视频即可\u001b[0m\n",
       "\u001b[32m将其添加到编辑时间轴等，这些功能可以极大地简化和加速视频编辑流程，降低新手入门门槛。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"The Increasing demand for video content creation has been hindered by the \u001b[0m\n",
       "\u001b[32mtechnical expertise and effort typically required for video editing. This paper presents an integration of large \u001b[0m\n",
       "\u001b[32mlanguage models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m into the video editing workflow to lower these barriers, specifically focusing on assistance\u001b[0m\n",
       "\u001b[32mand language augmentation. Our proposed system, LAVE, offers an agent that interacts with users using natural \u001b[0m\n",
       "\u001b[32mlanguage, providing assistance with various editing tasks. It also incorporates language augmentation into the \u001b[0m\n",
       "\u001b[32mediting workflow, with features such as automated video summarization, video title generation, and quick video \u001b[0m\n",
       "\u001b[32mselection from a gallery. We believe that LAVE simplifies the video editing process, making it more accessible to a\u001b[0m\n",
       "\u001b[32mwider range of users.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"基于函数调用的零样本对话状态追踪器中的大语言模型\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"大语言模型（LLM）因其在一般语境下的高级理解和生成能力而在会话系统中日益普及。然而，它们在任务导向对话（TOD）中的有\u001b[0m\n",
       "\u001b[32m效性仍然不尽如人意，这不仅需要生成响应，还需要在特定任务和领域内进行有效的对话状态跟踪（DST）。在这项工作中，我们提\u001b[0m\n",
       "\u001b[32m出了一种通过函数调用使用 LLM 解决 DST 的新方法 FNCTOD。该方法改进了零样本 \u001b[0m\n",
       "\u001b[32mDST，允许适应不同的领域，而无需大量数据收集或模型调整。我们的实验结果表明，我们的方法使用中等规模的开源和专有 LLM \u001b[0m\n",
       "\u001b[32m都取得了卓越的性能：在上下文中提示下，它使各种 7B 或 13B 参数模型能够超越 ChatGPT \u001b[0m\n",
       "\u001b[32m实现的先前最先进（SOTA）技术，并将 ChatGPT 的性能提高了 5.6% 的平均 JGA。GPT-3.5 和 GPT-4 的单个模型结果分别提高了 \u001b[0m\n",
       "\u001b[32m4.8% 和 14%。我们还表明，通过针对小规模的不同任务导向对话进行微调，我们可以为中等规模的模型（特别是 13B 参数 \u001b[0m\n",
       "\u001b[32mLLaMA2-Chat 模型）配备函数调用能力和与 ChatGPT 相媲美的 DST \u001b[0m\n",
       "\u001b[32m性能，同时保持其聊天能力。我们将开源实验代码和模型。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"Large Language Models as Zero-shot Dialogue State Tracker through Function \u001b[0m\n",
       "\u001b[32mCalling\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are increasingly prevalent in conversational systems \u001b[0m\n",
       "\u001b[32mdue to their advanced understanding and generative capabilities in general contexts. However, their effectiveness \u001b[0m\n",
       "\u001b[32min task-oriented dialogues \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTOD\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which requires not only response generation but also effective dialogue state \u001b[0m\n",
       "\u001b[32mtracking \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDST\u001b[0m\u001b[32m)\u001b[0m\u001b[32m within specific tasks and domains, remains less satisfying. In this work, we propose a novel \u001b[0m\n",
       "\u001b[32mapproach FNCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing \u001b[0m\n",
       "\u001b[32madaptation to diverse domains without extensive data collection or model tuning. Our experimental results \u001b[0m\n",
       "\u001b[32mdemonstrate that our approach achieves exceptional performance with both modestly sized open-source and also \u001b[0m\n",
       "\u001b[32mproprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous \u001b[0m\n",
       "\u001b[32mstate-of-the-art \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSOTA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m achieved by ChatGPT, and improves ChatGPT’s performance beating the SOTA by 5.6% Avg. JGA. \u001b[0m\n",
       "\u001b[32mIndividual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by \u001b[0m\n",
       "\u001b[32mfine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, \u001b[0m\n",
       "\u001b[32mspecifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable \u001b[0m\n",
       "\u001b[32mto ChatGPT while maintaining their chat capabilities. We will open-source experimental code and model.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m' \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"chinese_title\": \"构建廉价缩放：一种用于高分辨率适应的自级联扩散模型\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"扩散模型在图像和视频生成中已被证明非常有效；然而，由于单尺度训练数据，它们在生成不同大小的图像时仍然面临构图挑战。\u001b[0m\n",
       "\u001b[32m针对高分辨率调整大型预训练扩散模型需要大量的计算和优化资源，但仍无法实现与低分辨率模型相当的生成能力。本文提出了一\u001b[0m\n",
       "\u001b[32m种新颖的自级联扩散模型，该模型利用从经过良好训练的低分辨率模型中获得的丰富知识，快速适应高分辨率图像和视频生成，采\u001b[0m\n",
       "\u001b[32m用无调优或廉价上采样器调优范例。自级联扩散模型集成了序列多尺度上采样器模块，能有效地适应更高的分辨率，同时保留了原\u001b[0m\n",
       "\u001b[32m始构图和生成能力。我们进一步提出了一种枢轴引导噪声重新调度策略，以加速推理过程并改善局部结构细节。与完全微调相比，\u001b[0m\n",
       "\u001b[32m我们的方法将训练速度提高了 5 倍，并且仅需要额外的 0.002M 调优参数。大量实验证明，我们的方法可以通过仅微调 10k \u001b[0m\n",
       "\u001b[32m步，在几乎没有额外推理时间的情况下，快速适应更高分辨率的图像和视频合成。我们的代码将在 \u001b[0m\n",
       "\u001b[32mhttps://github.com/GuoLanging/Self-Cascade/ 发布。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"english_title\": \"Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution \u001b[0m\n",
       "\u001b[32mAdaptation\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"english_summary\": \"Diffusion models have proven to be highly effective in image and video generation; \u001b[0m\n",
       "\u001b[32mhowever, they still face composition challenges when generating images of varying sizes due to single-scale \u001b[0m\n",
       "\u001b[32mtraining data. Adapting large pre-trained diffusion models for higher resolution demands substantial computational \u001b[0m\n",
       "\u001b[32mand optimization resources, yet achieving a generation capability comparable to low-resolution models remains \u001b[0m\n",
       "\u001b[32melusive. This paper proposes a novel self-cascade diffusion model that leverages the rich knowledge gained from a \u001b[0m\n",
       "\u001b[32mwell-trained low-resolution model for rapid adaptation to higher-resolution image and video generation, employing \u001b[0m\n",
       "\u001b[32meither tuning-free or cheap upsampler tuning paradigms. Integrating a sequence of multi-scale upsampler modules, \u001b[0m\n",
       "\u001b[32mthe self-cascade diffusion model can efficiently adapt to a higher resolution, preserving the original composition \u001b[0m\n",
       "\u001b[32mand generation capabilities. We further propose a pivot-guided noise re-schedule strategy to speed up the inference\u001b[0m\n",
       "\u001b[32mprocess and improve local structural details. Compared to full fine-tuning, our approach achieves a 5x training \u001b[0m\n",
       "\u001b[32mspeed-up and requires only an additional 0.002M tuning parameters. Extensive experiments demonstrate that our \u001b[0m\n",
       "\u001b[32mapproach can quickly adapt to higher resolution image and video synthesis by fine-tuning for just 10k steps, with \u001b[0m\n",
       "\u001b[32mvirtually no additional inference time. Our code will be released at \u001b[0m\n",
       "\u001b[32mhttps://github.com/GuoLanging/Self-Cascade/.\"\\n'\u001b[0m,\n",
       "        \u001b[32m' \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"通用操作界面：在不使用野外机器人的情况下，在野外对机器进行教学\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"我们提出了通用操作界面（UMI）——一种数据收集和策略学习框架，该框架允许直接将野外人类演示中的技能转移到可部署机器策\u001b[0m\n",
       "\u001b[32m略中。UMI \u001b[0m\n",
       "\u001b[32m采用手持抓手结合谨慎的界面设计，可实现便携、低成本且信息丰富的数据收集，以进行极具挑战性的双手动和动态操作演示。为\u001b[0m\n",
       "\u001b[32m促进可部署策略学习，UMI \u001b[0m\n",
       "\u001b[32m包含一个精心设计的策略界面，该界面具备推理时延迟匹配和相对轨迹动作表示。所学策略与硬件无关，且可在多种机器人平台上\u001b[0m\n",
       "\u001b[32m部署。配备这些功能，UMI \u001b[0m\n",
       "\u001b[32m框架解锁了新的机器人操作能力，仅通过针对每个任务更改训练数据，即可实现零次概括的动态、双手动、精确定位和长时间行为\u001b[0m\n",
       "\u001b[32m。我们通过全面的真实实验展示了 UMI 的多功能性和有效性，在 UMI \u001b[0m\n",
       "\u001b[32m上学习的策略，在针对各种人类演示进行训练后，可零次概括到新环境和物体。UMI 的硬件和软件系统可在 \u001b[0m\n",
       "\u001b[32mhttps://umi-gripper.github.io 获取开源。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild \u001b[0m\n",
       "\u001b[32mRobots\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"We present Universal Manipulation Interface \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUMI\u001b[0m\u001b[32m)\u001b[0m\u001b[32m—a data collection and policy \u001b[0m\n",
       "\u001b[32mlearning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot \u001b[0m\n",
       "\u001b[32mpolicies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and \u001b[0m\n",
       "\u001b[32minformation-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate \u001b[0m\n",
       "\u001b[32mdeployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency \u001b[0m\n",
       "\u001b[32mmatching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and \u001b[0m\n",
       "\u001b[32mdeployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot \u001b[0m\n",
       "\u001b[32mmanipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors,\u001b[0m\n",
       "\u001b[32mby only changing the training data for each task. We demonstrate UMI’s versatility and efficacy with comprehensive \u001b[0m\n",
       "\u001b[32mreal-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when \u001b[0m\n",
       "\u001b[32mtrained on diverse human demonstrations. UMI’s hardware and software system is open-sourced at \u001b[0m\n",
       "\u001b[32mhttps://umi-gripper.github.io.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m' \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"chinese_title\": \"SPAR：通过长期参与注意力实现个性化的基于内容的推荐\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"chinese_summary\": \"利用用户的长期参与历史对于个性化的内容推荐至关重要。预训练语言模型 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m 在 NLP \u001b[0m\n",
       "\u001b[32m中的成功使其被用于对用户历史和候选项目进行编码，并将内容推荐构建为文本语义匹配任务。然而，现有工作在处理超长的用户\u001b[0m\n",
       "\u001b[32m历史文本和不足的用户与项目的交互方面仍面临挑战。在本文中，我们引入了一个基于内容的推荐框架 \u001b[0m\n",
       "\u001b[32mSPAR，该框架有效地解决了从长期的用户参与历史中提取整体用户兴趣的挑战。它通过利用 \u001b[0m\n",
       "\u001b[32mPLM、多注意力层和注意力稀疏机制以基于会话的方式对用户历史进行编码来实现这一点。用户端和项目端特征充分融合用于参与度\u001b[0m\n",
       "\u001b[32m预测，同时保持两端的独立表示，这对于实际模型部署是有效的。此外，我们利用大型语言模型 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m从用户参与历史中提取全局兴趣来增强用户画像。在两个基准数据集上的广泛实验表明，我们的框架优于现有的最先进 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSoTA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32m方法。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"english_title\": \"SPAR: Personalized Content-Based Recommendation via Long Engagement Attention\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"english_summary\": \"Leveraging users’ long engagement histories is essential for personalized content \u001b[0m\n",
       "\u001b[32mrecommendations. The success of pretrained language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in NLP has led to their use in encoding user \u001b[0m\n",
       "\u001b[32mhistories and candidate items, framing content recommendations as textual semantic matching tasks. However, \u001b[0m\n",
       "\u001b[32mexisting works still struggle with processing very long user historical text and insufficient user-item \u001b[0m\n",
       "\u001b[32minteraction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles \u001b[0m\n",
       "\u001b[32mthe challenges of holistic user interest extraction from the long user engagement history. It achieves so by \u001b[0m\n",
       "\u001b[32mleveraging PLM, poly-attention layers and attention sparsity mechanisms to encode user’s history in a session-based\u001b[0m\n",
       "\u001b[32mmanner. The user and item side features are sufficiently fused for engagement prediction while maintaining \u001b[0m\n",
       "\u001b[32mstandalone representations for both sides, which is efficient for practical model deployment. Moreover, we enhance \u001b[0m\n",
       "\u001b[32muser profiling by exploiting large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to extract global interests from user engagement history. \u001b[0m\n",
       "\u001b[32mExtensive experiments on two benchmark datasets demonstrate that our framework outperforms existing \u001b[0m\n",
       "\u001b[32mstate-of-the-art \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSoTA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m methods.\"\\n'\u001b[0m,\n",
       "        \u001b[32m' \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"PaLM2-VAdapter：渐进式对齐语言模型成为强大的视觉语言适配器\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"本文表明，渐进式对齐的语言模型可以有效地桥接冻结的视觉编码器和大语言模型（LLM）。虽然视觉编码器和 LLM \u001b[0m\n",
       "\u001b[32m的基本架构和预训练方法已经得到广泛研究，但视觉语言适配器的架构和训练策略在最近的研究中差异很大。我们的研究对最先进\u001b[0m\n",
       "\u001b[32m的感知器重采样器架构进行了彻底探索，并建立了一个强大的基线。然而，我们观察到，具有感知器重采样器的视觉语言对齐表现\u001b[0m\n",
       "\u001b[32m出缓慢的收敛性和有限的可扩展性，缺乏直接的监督。为了解决这个问题，我们提出了PaLM2-VAdapter，采用渐进式对齐的语言模\u001b[0m\n",
       "\u001b[32m型作为视觉语言适配器。与具有感知器重采样器的强大基线相比，我们的方法凭经验证明具有更快的收敛性、更高的性能和更强的\u001b[0m\n",
       "\u001b[32m可扩展性。跨越图像和视频的各种视觉问答（VQA）和字幕任务的广泛实验表明，我们的模型展示了最先进的视觉理解和多模态推理\u001b[0m\n",
       "\u001b[32m能力。值得注意的是，我们的方法比最先进的大型视觉语言模型的参数少了30~70%，这标志着效率的显着提高。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language \u001b[0m\n",
       "\u001b[32mAdapter\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"This paper demonstrates that a progressively aligned language model can \u001b[0m\n",
       "\u001b[32meffectively bridge frozen vision encoders and large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. While the fundamental architecture and \u001b[0m\n",
       "\u001b[32mpre-training methods of vision encoders and LLMs have been extensively studied, the architecture and training \u001b[0m\n",
       "\u001b[32mstrategy of vision-language adapters vary significantly across recent works. Our research undertakes a thorough \u001b[0m\n",
       "\u001b[32mexploration of the state-of-the-art perceiver resampler architecture and builds a strong baseline. However, we \u001b[0m\n",
       "\u001b[32mobserve that the vision-language alignment with perceiver resampler exhibits slow convergence and limited \u001b[0m\n",
       "\u001b[32mscalability with a lack of direct supervision. To address this issue, we propose PaLM2-VAdapter, employing a \u001b[0m\n",
       "\u001b[32mprogressively aligned language model as the vision-language adapter. Compared to the strong baseline with perceiver\u001b[0m\n",
       "\u001b[32mresampler, our method empirically shows faster convergence, higher performance and stronger scalability. Extensive \u001b[0m\n",
       "\u001b[32mexperiments across various Visual Question Answering \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and captioning tasks on both images and videos \u001b[0m\n",
       "\u001b[32mdemonstrate that our model exhibits state-of-the-art visual understanding and multi-modal reasoning capabilities. \u001b[0m\n",
       "\u001b[32mNotably, our method achieves these advancements with 30~70% fewer parameters than the state-of-the-art large \u001b[0m\n",
       "\u001b[32mvision-language models, marking a significant efficiency improvement.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"RLVF：从语言反馈中学习，避免过度概括\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"随着大型语言模型（LLM）在各个行业和个体中的广泛采用，针对特定用户或用例，按照高级人类反馈对其进行调整的能力变得越\u001b[0m\n",
       "\u001b[32m来越重要。虽然 LLM \u001b[0m\n",
       "\u001b[32m用户通常希望模型始终遵循广泛的原则，比如生成流畅的文本，但个别用户和用例有更细微的偏好。例如，用户可能要求 LLM \u001b[0m\n",
       "\u001b[32m写出更简洁的工作电子邮件，但要写出更详细的个人电子邮件，这使得反馈具有上下文相关性。根据这些偏好调整模型具有挑战性\u001b[0m\n",
       "\u001b[32m：需要大量资源才能收集所有不同上下文中的偏好，并且在一个上下文中的模型微调会对其他上下文中的模型行为产生不可预测的\u001b[0m\n",
       "\u001b[32m影响。我们研究了使用语言反馈来调整模型的问题，这种反馈对于人们来说快速且容易提供（见图 \u001b[0m\n",
       "\u001b[32m1）。合并反馈的常见方法，例如监督上下文蒸馏 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSCD\u001b[0m\u001b[32m)\u001b[0m\u001b[32m 或强化学习，利用人类反馈 \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mRLHF\u001b[0m\u001b[32m)\u001b[0m\u001b[32m，使用示例级监管通过监督完成或偏好标签。此类方法需要大量用户提供的（偏好）数据语料库，而获取这些数据语料库可\u001b[0m\n",
       "\u001b[32m能既昂贵又繁琐。此外，它们不会限制反馈可能适用的上下文之外的模型行为，因此 LLM \u001b[0m\n",
       "\u001b[32m可能会以意外的方式调整其行为，例如在偏好仅适用于个人电子邮件时生成更冗长的工作电子邮件。语言反馈对于人类来说更容易\u001b[0m\n",
       "\u001b[32m、更快速地提供。为此，另一种常见的方法是将此类语言反馈纳入提示中，可能通过迭代过程来持续添加其他反馈点。然而，此方\u001b[0m\n",
       "\u001b[32m法需要在所有未来查询中重新使用该提示。随着更多反馈的积累，包含许多上下文相关反馈的长提示会使推理变得昂贵；此外，识\u001b[0m\n",
       "\u001b[32m别哪些反馈应当在给定上下文中适用可能变得很困难。我们的目标是调整 \u001b[0m\n",
       "\u001b[32mLLM，以便在提供一个指定反馈的句子时，模型能够辨别反馈适用的哪些情况，并在未来输出中适当地纳入该反馈。我们提出了情境\u001b[0m\n",
       "\u001b[32m化批判与约束性偏好优化 \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mC3PO\u001b[0m\u001b[32m)\u001b[0m\u001b[32m，其中我们首先为超出反馈范围和未超出反馈范围的情境合成生成假设性提示。然后，我们对这些提示进行原始完成采样，\u001b[0m\n",
       "\u001b[32m不应用反馈，以及对所收集的反馈进行修订，以便为每个提示生成一个合成偏好数据集，指定反馈应当（和不应当）如何应用。接\u001b[0m\n",
       "\u001b[32m着，我们根据合成偏好数据对模型进行微调，同时最大限度地减少超出反馈适用范围的提示的原始模型的差异。我们的实验结果表\u001b[0m\n",
       "\u001b[32m明，我们的方法有效地将语言反馈应用于相关场景，同时保留了其他上下文的现有行为。对于人类和 GPT-4 生成的语言反馈，C3PO\u001b[0m\n",
       "\u001b[32m有效地遵守给定的反馈，与上下文中基准相当，同时将过度概括减少了 30%。\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"RLVF: Learning from Verbal Feedback without Overgeneralization\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"The diversity of contexts in which large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are deployed \u001b[0m\n",
       "\u001b[32mrequires the ability to modify or customize default model behaviors to incorporate nuanced requirements and \u001b[0m\n",
       "\u001b[32mpreferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as “Don’t\u001b[0m\n",
       "\u001b[32muse emojis when drafting emails to my boss.” However, while writing high-level feedback is far simpler than \u001b[0m\n",
       "\u001b[32mcollecting annotations for reinforcement learning from human feedback \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRLHF\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, we find that simply prompting a model\u001b[0m\n",
       "\u001b[32mwith such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the \u001b[0m\n",
       "\u001b[32mproblem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized \u001b[0m\n",
       "\u001b[32mCritiques with Constrained Preference Optimization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mC3PO\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. C3PO uses a piece of high-level feedback to generate a \u001b[0m\n",
       "\u001b[32msmall synthetic preference dataset specifying how the feedback should \u001b[0m\u001b[32m(\u001b[0m\u001b[32mand should not\u001b[0m\u001b[32m)\u001b[0m\u001b[32m be applied. It then \u001b[0m\n",
       "\u001b[32mfine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the \u001b[0m\n",
       "\u001b[32moriginal model for prompts where the feedback does not apply. Our experimental results indicate that our approach \u001b[0m\n",
       "\u001b[32meffectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts. \u001b[0m\n",
       "\u001b[32mFor both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably \u001b[0m\n",
       "\u001b[32mto in-context baselines while reducing overgeneralization by 30%.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "summary_list = []\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"./summary_test.txt\"\n",
    "\n",
    "# Read the file and store each 8 lines as a list element\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for i in range(0, len(lines), 8):\n",
    "        summary = lines[i:i+8]\n",
    "        summary_list.append(summary)\n",
    "\n",
    "# Print the list of summaries\n",
    "print(summary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere (Unsued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cohere guardrails-ai pydantic -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import guardrails as gd\n",
    "from guardrails.validators import ValidRange, ValidChoices\n",
    "from pydantic import BaseModel, Field\n",
    "from rich import print\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summary(BaseModel):\n",
    "    chinese_title: str = Field(..., description=\"在摘要开头呈现文档的中文标题。这将作为摘要的首要内容，立刻向读者明确文档的核心主题。\", min_length=1)\n",
    "    chinese_summary: str = Field(..., description=\"\"\"针对每个提供的PDF文件，创建一个精确且简洁的摘要，捕捉文件的精髓，专注于主要话题或问题、关键发现或论点、方法论（如果适用）以及结论和影响。摘要应该是引人入胜的，用清晰简单的语言书写，确保普通大学教育水平的读者能够轻松理解。\n",
    "\n",
    "将以下元素整合进一个连贯的段落中，不使用项目符号：\n",
    "\n",
    "引言: 以文件的中心主题或探询的简短介绍开始。\n",
    "主要发现或论点: 突出文件的主要发现或论点，保持清晰和吸引力。\n",
    "方法论或方法: 如果相关，简要描述方法论或方法，简化任何技术术语。\n",
    "结论和影响: 以文件的更广泛影响或其潜在影响的总结结束，强调其重要性。\n",
    "你的目标是提供一个简洁而全面的概览，使读者能够在不需要阅读全文的情况下把握文件的本质。每个摘要必须是独特的，准确反映PDF的特定内容和语调。\"\"\", min_length=1)\n",
    "    english_title: str = Field(..., description=\"Present the document's in English. This should be the first element in the summary to immediately inform the reader of the document's subject.\", min_length=1)\n",
    "    english_summary: str = Field(..., description=\"\"\"For each PDF document provided, create an articulate and concise summary that captures the essence of the document, focusing on the main topic or question, key findings or arguments, methodology (if applicable), and the conclusions and implications. The summary should be engaging, written in clear, simple language to ensure it's accessible to a general audience with a college-level education.\n",
    "\n",
    "Incorporate the following elements into a single, coherent paragraph without using bullet points:\n",
    "\n",
    "Introduction: Begin with a brief introduction to the document's central theme or inquiry.\n",
    "Main Findings or Arguments: Highlight the document's primary discoveries or arguments, maintaining clarity and engagement.\n",
    "Methodology or Approach: Briefly describe the methodology or approach, if relevant, simplifying any technical terms.\n",
    "Conclusions and Implications: Finish with a summary of the document's broader implications or its potential impact, underlining its significance.\n",
    "Your goal is to provide a succinct yet thorough overview that allows readers to grasp the document's essence without needing to delve into the full text. Each summary must be unique, reflecting the specific content and tone of the PDF accurately.\"\"\", min_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = pdflink_list[0]\n",
    "\n",
    "PROMPT = f\"\"\"For each PDF provided, produce a unified, concise summary that encapsulates the document's essential aspects, including its main topic or research question, key findings or arguments, methodology or approach (if relevant), and conclusions and implications. Craft this summary to be clear, engaging, and easily understandable to a general audience with a college-level education. Aim to convey the core message and significance of the document succinctly, yet thoroughly, allowing readers to grasp its essence without reading the document in its entirety.\n",
    "Each summary should seamlessly integrate the following elements into a single, cohesive paragraph:\n",
    "* Introduction: Start with a succinct introduction to the document's primary focus or inquiry, using straightforward and accessible language.\n",
    "* Main Findings or Arguments: Weave in an overview of the document's key discoveries or arguments, ensuring the narrative remains engaging and lucid.\n",
    "* Methodology or Approach: Where applicable, briefly elucidate on the methodology or approach employed in the document, simplifying any complex terminologies for the lay reader.\n",
    "* Conclusions and Implications: Conclude by summarizing the document's broader implications or its potential impact, highlighting why it matters.\n",
    "This summary needs to be crafted in both English and Chinese, maintaining a balance between clarity and depth to cater to a bilingual audience. Please refrain from using bullet points, ensuring the summary remains in one continuous, flowing paragraph that accurately mirrors the specific content and tone of its corresponding PDF.\n",
    "The ultimate goal is to deliver succinct yet comprehensive overviews, enabling readers to understand the essence of each document fully. Each summary must be unique, faithfully reflecting the specific nuances and themes of its respective PDF.\n",
    "\n",
    "{{Summary}}\n",
    "\n",
    "PDF contents:{pdf_text}\n",
    "\n",
    "@complete_json_suffix_v2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For each PDF provided, produce a unified, concise summary that encapsulates the document's essential aspects, \n",
       "including its main topic or research question, key findings or arguments, methodology or approach <span style=\"font-weight: bold\">(</span>if relevant<span style=\"font-weight: bold\">)</span>, \n",
       "and conclusions and implications. Craft this summary to be clear, engaging, and easily understandable to a general \n",
       "audience with a college-level education. Aim to convey the core message and significance of the document \n",
       "succinctly, yet thoroughly, allowing readers to grasp its essence without reading the document in its entirety.\n",
       "Each summary should seamlessly integrate the following elements into a single, cohesive paragraph:\n",
       "* Introduction: Start with a succinct introduction to the document's primary focus or inquiry, using \n",
       "straightforward and accessible language.\n",
       "* Main Findings or Arguments: Weave in an overview of the document's key discoveries or arguments, ensuring the \n",
       "narrative remains engaging and lucid.\n",
       "* Methodology or Approach: Where applicable, briefly elucidate on the methodology or approach employed in the \n",
       "document, simplifying any complex terminologies for the lay reader.\n",
       "* Conclusions and Implications: Conclude by summarizing the document's broader implications or its potential \n",
       "impact, highlighting why it matters.\n",
       "This summary needs to be crafted in both English and Chinese, maintaining a balance between clarity and depth to \n",
       "cater to a bilingual audience. Please refrain from using bullet points, ensuring the summary remains in one \n",
       "continuous, flowing paragraph that accurately mirrors the specific content and tone of its corresponding PDF.\n",
       "The ultimate goal is to deliver succinct yet comprehensive overviews, enabling readers to understand the essence of\n",
       "each document fully. Each summary must be unique, faithfully reflecting the specific nuances and themes of its \n",
       "respective PDF.\n",
       "\n",
       "<span style=\"font-weight: bold\">{</span>Summary<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "PDF contents:<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10644.pdf</span>\n",
       "\n",
       "@complete_json_suffix_v2\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For each PDF provided, produce a unified, concise summary that encapsulates the document's essential aspects, \n",
       "including its main topic or research question, key findings or arguments, methodology or approach \u001b[1m(\u001b[0mif relevant\u001b[1m)\u001b[0m, \n",
       "and conclusions and implications. Craft this summary to be clear, engaging, and easily understandable to a general \n",
       "audience with a college-level education. Aim to convey the core message and significance of the document \n",
       "succinctly, yet thoroughly, allowing readers to grasp its essence without reading the document in its entirety.\n",
       "Each summary should seamlessly integrate the following elements into a single, cohesive paragraph:\n",
       "* Introduction: Start with a succinct introduction to the document's primary focus or inquiry, using \n",
       "straightforward and accessible language.\n",
       "* Main Findings or Arguments: Weave in an overview of the document's key discoveries or arguments, ensuring the \n",
       "narrative remains engaging and lucid.\n",
       "* Methodology or Approach: Where applicable, briefly elucidate on the methodology or approach employed in the \n",
       "document, simplifying any complex terminologies for the lay reader.\n",
       "* Conclusions and Implications: Conclude by summarizing the document's broader implications or its potential \n",
       "impact, highlighting why it matters.\n",
       "This summary needs to be crafted in both English and Chinese, maintaining a balance between clarity and depth to \n",
       "cater to a bilingual audience. Please refrain from using bullet points, ensuring the summary remains in one \n",
       "continuous, flowing paragraph that accurately mirrors the specific content and tone of its corresponding PDF.\n",
       "The ultimate goal is to deliver succinct yet comprehensive overviews, enabling readers to understand the essence of\n",
       "each document fully. Each summary must be unique, faithfully reflecting the specific nuances and themes of its \n",
       "respective PDF.\n",
       "\n",
       "\u001b[1m{\u001b[0mSummary\u001b[1m}\u001b[0m\n",
       "\n",
       "PDF contents:\u001b[4;94mhttps://arxiv.org/pdf/2402.10644.pdf\u001b[0m\n",
       "\n",
       "@complete_json_suffix_v2\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "guard = gd.Guard.from_pydantic(Summary, prompt=PROMPT)\n",
    "print(guard.base_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6ziwZlCAQi45cixuyDoGKm6A17VsJYXR8Ipe1Elq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'user_id' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m co \u001b[38;5;241m=\u001b[39m cohere\u001b[38;5;241m.\u001b[39mClient(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6ziwZlCAQi45cixuyDoGKm6A17VsJYXR8Ipe1Elq\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m raw_llm_output, validated_output \u001b[38;5;241m=\u001b[39m guard(\n\u001b[1;32m      3\u001b[0m     co\u001b[38;5;241m.\u001b[39mgenerate,\n\u001b[1;32m      4\u001b[0m     prompt_params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: Summary},\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommand\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      7\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "co = cohere.Client(api_key='6ziwZlCAQi45cixuyDoGKm6A17VsJYXR8Ipe1Elq')\n",
    "raw_llm_output, validated_output = guard(\n",
    "    co.generate,\n",
    "    prompt_params={\"Summary\": Summary},\n",
    "    model='command',\n",
    "    max_tokens=1024,\n",
    "    temperature=0.6\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
