{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U google-generativeai\n",
    "# !pip install google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "GOOGLE_API_KEY='AIzaSyC6gfzUfG1FnyzdFzjbsfBIH3rMKDCspJo'\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>-pro\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-\u001b[1;36m1.0\u001b[0m-pro\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>-pro-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">001</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-\u001b[1;36m1.0\u001b[0m-pro-\u001b[1;36m001\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>-pro-latest\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-\u001b[1;36m1.0\u001b[0m-pro-latest\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>-pro-vision-latest\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-\u001b[1;36m1.0\u001b[0m-pro-vision-latest\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-pro\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-pro\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">models/gemini-pro-vision\n",
       "</pre>\n"
      ],
      "text/plain": [
       "models/gemini-pro-vision\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `response.text` quick accessor only works for simple (single-`Part`) text responses. This response is not simple text.Use the `result.parts` accessor or the full `result.candidates[index].content.parts` lookup instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the meaning of life?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python310/lib/python3.10/site-packages/google/generativeai/types/generation_types.py:328\u001b[0m, in \u001b[0;36mBaseGenerateContentResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parts) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m parts[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `response.text` quick accessor only works for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple (single-`Part`) text responses. This response is not simple text.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse the `result.parts` accessor or the full \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`result.candidates[index].content.parts` lookup \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mValueError\u001b[0m: The `response.text` quick accessor only works for simple (single-`Part`) text responses. This response is not simple text.Use the `result.parts` accessor or the full `result.candidates[index].content.parts` lookup instead."
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')\n",
    "response = model.generate_content(\"What is the meaning of life?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At the command line, only need to run once to install the package via pip:\n",
    "\n",
    "$ pip install google-generativeai\n",
    "\"\"\"\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "GOOGLE_API_KEY='AIzaSyC6gfzUfG1FnyzdFzjbsfBIH3rMKDCspJo'\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Set up the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0.9,\n",
    "  \"top_p\": 1,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 2048,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n",
    "                              generation_config=generation_config,\n",
    "                              safety_settings=safety_settings)\n",
    "\n",
    "# convo = model.start_chat(history=[\n",
    "#   {\n",
    "#     \"role\": \"user\",\n",
    "#     \"parts\": [\"For each PDF provided, generate a concise summary focusing on the key points, main arguments, and conclusions, both in English and Chinese. The summary should be clear, engaging, and accessible to a general audience with a college-level education. Each summary should include:\\t\\tIntroduction (å¼•è¨€): A brief introduction to the document's main topic or research question, in simple and accessible language.\\t\\tMain Findings or Arguments (ä¸»è¦å‘ç°æˆ–è®ºç‚¹): Highlight the primary findings or arguments presented in the document, ensuring clarity and engagement.\\t\\tMethodology or Approach (æ–¹æ³•è®ºæˆ–æ–¹æ³•): If applicable, discuss the methodology or approach used in the document, translating technical terms into layman's terms.\\t\\tConclusions and Implications (ç»“è®ºå’Œå½±å“): Conclude with the document's implications or potential impact, emphasizing its relevance and importance.The goal is to provide succinct yet comprehensive overviews that enable readers to understand the essence of each document without needing to read it in full. Ensure each summary is distinct and accurately reflects the specific content and tone of its corresponding PDF, with parallel content in both English and Chinese to cater to a bilingual audience.PDF contents:\"]\n",
    "#   },\n",
    "#   {\n",
    "#     \"role\": \"model\",\n",
    "#     \"parts\": [\"**ä¸­æ–‡æ‘˜è¦ï¼š**\\n\\næœ¬æ–‡ä»æ–°çš„è§’åº¦ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„å‘å±•ï¼Œæ¢ç´¢äº†ä¸€ç§åœ¨è§£ç é˜¶æ®µè€Œä¸æ˜¯æç¤ºé˜¶æ®µæ¿€å‘ LLM è¿›è¡Œæ¨ç†çš„æ–°æ–¹æ³•â€”â€”CoT è§£ç ã€‚é€šè¿‡è€ƒè™‘æœ€åˆè§£ç æ­¥éª¤ä¸­æ›¿ä»£é¡¶çº§æ ‡è®°ï¼Œæœ¬æ–‡å‘ç° LLM ä¸­æœ¬æ¥å°±å­˜åœ¨æ¨ç†è·¯å¾„ã€‚è¯¥æ–¹æ³•ä¸ä»…ç»•å¼€äº†æç¤ºçš„æ··æ‚å› ç´ ï¼Œè¿˜èƒ½æ›´å‡†ç¡®åœ°è¯„ä¼° LLM çš„å†…åœ¨æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡º CoT-è§£ç ç®—æ³•ä»¥ä»é¡¶çº§è§£ç è·¯å¾„ä¸­æå–æ›´åŠ å¯é çš„è§£ç è·¯å¾„ï¼Œè¿›è€Œæ˜¾è‘—æå‡äº†å„ç§åŸºå‡†ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚\\n\\n**è‹±æ–‡æ‘˜è¦ï¼š**\\n\\nWe investigate the reasoning capabilities of large language models (LLMs) by exploring an alternative perspective within the decoding stage, demonstrating that even without explicit prompting, inherent CoT reasoning paths exist within these models. Rather than relying exclusively on greedy decoding, exploring alternative top-ğ‘˜ tokens in the decoding space reveals the natural existence of reasoning paths within these models. Furthermore, our empirical observations highlight that the presence of a CoT reasoning path correlates with increased model confidence in decoding its final answer. Based on this observation, we introduce CoT-decoding to extract more reliable decoding paths from language models, thereby enhancing overall reasoning performance. Exploring alternative decoding paths incurs additional computational costs. Future work may leverage the CoT-decoding paths to fine-tune the model to enhance its reasoning capabilities.\"]\n",
    "#   },\n",
    "# ])\n",
    "\n",
    "# convo.send_message(\"YOUR_USER_INPUT\")\n",
    "# print(convo.last.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Linear Transformers with Learnable Kernel Functions are Better In-Context Models\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Linear Transformers with Learnable Kernel Functions are Better In-Context Models\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10644</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10644\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10644</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10644\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10644.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10644.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss\n",
       "</pre>\n"
      ],
      "text/plain": [
       "In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10790</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10790\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10790</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10790\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10790.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10790.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10379</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10379\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10379</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10379\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10379.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10379.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting\n",
       "</pre>\n"
      ],
      "text/plain": [
       "GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10259</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10259\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10259</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10259\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10259.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10259.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10524</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10524\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10524</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10524\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10524.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10524.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing\n",
       "</pre>\n"
      ],
      "text/plain": [
       "LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10294</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10294\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10294</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10294\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10294.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10294.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Large Language Models as Zero-shot Dialogue State Tracker through Function Calling\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10466</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10466\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10466</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10466\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10466.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10466.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10491</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10491\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10491</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10491\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10491.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10491.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10329</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10329\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10329</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10329\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10329.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10329.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SPAR: Personalized Content-Based Recommendation via Long Engagement Attention\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SPAR: Personalized Content-Based Recommendation via Long Engagement Attention\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10555</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10555\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10555</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10555\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10555.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10555.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10896</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10896\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10896</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10896\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10896.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10896.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">RLVF: Learning from Verbal Feedback without Overgeneralization\n",
       "</pre>\n"
      ],
      "text/plain": [
       "RLVF: Learning from Verbal Feedback without Overgeneralization\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/papers/2402.10893</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://huggingface.co/papers/2402.10893\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2402.10893</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2402.10893\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10893.pdf</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[4;94mhttps://arxiv.org/pdf/2402.10893.pdf\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://huggingface.co/papers'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "link_list = []\n",
    "code_list = []\n",
    "pdflink_list = []\n",
    "elements = soup.select('h3.mb-1 a')\n",
    "for element in elements:\n",
    "    print(element.text)\n",
    "    \n",
    "    link = 'https://huggingface.co' + element['href']\n",
    "    code = element['href'].split('/')[-1]\n",
    "    pdflink = 'https://arxiv.org/pdf/' + code + '.pdf'\n",
    "    \n",
    "    link_list.append(link)\n",
    "    code_list.append(code)\n",
    "    pdflink_list.append(pdflink)\n",
    "    \n",
    "    print(link)\n",
    "    print(code)\n",
    "    print(pdflink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"Summary\": {\\n    \"chinese_title\": \"å¯å­¦ä¹ æ ¸å‡½æ•°çš„çº¿æ€§å˜æ¢å™¨æ˜¯æ›´å¥½çš„æƒ…å¢ƒæ¨¡å‹\",\\n    \"chinese_summary\": \"åœ¨é£é€Ÿå‘å±•çš„è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œæ¨è¿›è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„äºšäºŒæ¬¡æ¶æ„è‡³å…³é‡è¦ã€‚åŒ…æ‹¬çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨å†…çš„å½“å‰åˆ›æ–°æœ€åˆå› å…¶åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šè¶…è¶Šäº† Transformer æ€§èƒ½è€Œå—åˆ°èµæ‰¬ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨é‡è¦çš„æƒ…å¢ƒå­¦ä¹ èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºä¸è¶³â€”â€”è¿™æ˜¯ Transformer ä¼ ç»Ÿä¸Šè¡¨ç°å‡ºè‰²çš„é¢†åŸŸã€‚åŸºäºæ¨¡å‹ä½œä¸ºä¸€ç§æ··åˆè§£å†³æ–¹æ¡ˆå‡ºç°ï¼Œå®ƒèåˆäº†ä¸€ä¸ªçº¿æ€§å˜æ¢å™¨å’Œä¸€ä¸ªå—æŒ‡æ•°å‡½æ•°æ³°å‹’å±•å¼€å¯å‘çš„æ ¸ï¼Œå¹¶ç”±å·ç§¯ç½‘ç»œå¢å¼ºã€‚å®ƒåæ˜ äº† Transformer åœ¨æƒ…å¢ƒä¸­çš„é€‚åº”æ€§ï¼Œå¹¶æˆä¸ºè¯¥é¢†åŸŸçš„å¼ºæœ‰åŠ›ç«äº‰è€…ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºæ ¸å¿ƒçš„ä¸€ä¸ªç‹¬ç‰¹ã€ä¼˜é›…çš„æ”¹å˜ï¼Œå®ƒå¢å¼ºäº†åœ¨å¤šæŸ¥è¯¢å…³è”å¬å›ä»»åŠ¡å’Œæ•´ä½“è¯­è¨€å»ºæ¨¡è¿‡ç¨‹ä¸­è¯„ä¼°çš„æƒ…å¢ƒå­¦ä¹ èƒ½åŠ›ï¼Œæ­£å¦‚åœ¨ Pile æ•°æ®é›†ä¸Šæ‰€å±•ç¤ºçš„é‚£æ ·ã€‚\",\\n    \"english_title\": \"Linear Transformers with Learnable Kernel Functions are Better In-Context Models\",\\n    \"english_summary\": \"Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities â€” a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformerâ€™s in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\"\\n  }\\n}'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "# Extract text from PDFs\n",
    "def extract_text_from_pdf(pdf_link):\n",
    "    images = convert_from_path(pdf_link)\n",
    "    text = pytesseract.image_to_string(images[0])\n",
    "    return text\n",
    "\n",
    "prompt = \"\"\"For each PDF document provided, generate a concise, unified summary that captures the essence of the document's critical elements: its primary topic or question, significant findings or debates, methodology or strategy (if pertinent), and the outcomes and their significance. This summary should be crafted in clear, engaging language that is easily comprehensible to a general audience with a college-level education, succinctly conveying the document's core message and importance, thus allowing readers to grasp its essence without the need to read the document in its entirety.\n",
    "\n",
    "The summary should comprise a single, integrated paragraph that seamlessly includes the following components:\n",
    "\n",
    "Introduction: Begin with a concise introduction to the document's main focus or question, employing direct and accessible language.\n",
    "Main Findings or Arguments: Incorporate an overview of the document's crucial discoveries or arguments, ensuring the narrative is captivating and clear.\n",
    "Methodology or Approach: Where relevant, succinctly describe the document's methodology or approach, making any complex terminology understandable to the lay reader.\n",
    "Conclusions and Implications: End with a summary of the document's wider implications or its potential impact, underscoring its importance.\n",
    "\n",
    "This summary must be provided in both English and Chinese, striking a balance between clarity and depth to accommodate a bilingual audience. Avoid bullet points, aiming for a continuous, fluid paragraph that accurately reflects the specific content and tone of the corresponding PDF.\n",
    "\n",
    "The ultimate aim is to offer succinct yet comprehensive overviews, empowering readers to fully understand the essence of each document. Each summary should be distinctive, accurately mirroring the particular nuances and themes of its respective PDF.\n",
    "\n",
    "The output should conform to the following schema:\n",
    "\n",
    "{\n",
    "  \"Summary\": {\n",
    "    \"chinese_title\": \"\",\n",
    "    \"chinese_summary\": \"\",\n",
    "    \"english_title\": \"\",\n",
    "    \"english_summary\": \"\"\n",
    "  }\n",
    "}\n",
    "\n",
    "PDF contents:\"\"\"\n",
    "\n",
    "text = extract_text_from_pdf(pdflink_list[0])\n",
    "summary = model.generate_content(prompt + text).text\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\\n  \"Summary\": {\\n    \"chinese_title\": \"å¯å­¦ä¹ æ ¸å‡½æ•°çš„çº¿æ€§å˜æ¢å™¨æ˜¯æ›´å¥½çš„ä¸Šä¸‹æ–‡æ¨¡å‹\",\\n    \"chinese_summary\": \"åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å¿«é€Ÿå‘å±•çš„é¢†åŸŸå†…ï¼Œæ¨è¿›è¯­è¨€æ¨¡å‹ (LM) çš„äºšäºŒæ¬¡æ¶æ„æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„å·¥ä½œã€‚åŒ…æ‹¬çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨å†…çš„å½“å‰åˆ›æ–°æœ€åˆå› å…¶åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­è¶…è¶Šäº† Transformer æ€§èƒ½è€Œå¹¿å—èµèª‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¿…è¦çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ–¹é¢æš´éœ²å‡ºç¼ºé™·ï¼Œè¿™æ˜¯ Transformer ä¼ ç»Ÿä¸Šè¡¨ç°å‡ºè‰²çš„é¢†åŸŸã€‚Based æ¨¡å‹ä½œä¸ºä¸€ç§æ··åˆè§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œå®ƒå°†çº¿æ€§å˜æ¢å™¨ä¸å—æŒ‡æ•°å‡½æ•°çš„æ³°å‹’å±•å¼€æ‰€å¯å‘çš„æ ¸å‡½æ•°ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡å·ç§¯ç½‘ç»œè¿›è¡Œäº†æ‰©å±•ã€‚å®ƒå¤åˆ¶äº† Transformer åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç†Ÿç»ƒåº¦ï¼Œæˆä¸ºè¯¥é¢†åŸŸçš„æœ‰åŠ›ç«äº‰è€…ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹ Based æ ¸å‡½æ•°æå‡ºäº†ä¸€ç§ç‹¬ç‰¹è€Œä¼˜é›…çš„ä¿®æ”¹ï¼Œé€šè¿‡å¤šæŸ¥è¯¢å…³è”å¬å›ä»»åŠ¡å’Œåœ¨ Pile æ•°æ®é›†ä¸Šå±•ç¤ºçš„æ•´ä½“è¯­è¨€å»ºæ¨¡è¿‡ç¨‹è¯„ä¼°ï¼Œæ”¾å¤§äº†å…¶ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚\",\\n    \"english_title\": \"Linear Transformers with Learnable Kernel Functions are Better In-Context Models\",\\n    \"english_summary\": \"Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities â€” a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformerâ€™s in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"åœ¨ä¸€åƒå…†å­—èŠ‚çš„å¹²è‰å †ä¸­å¯»æ‰¾é’ˆå¤´ï¼šå¾ªç¯è®°å¿†å‘ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹é”™å¤±çš„ä¸œè¥¿\",\\n    \"chinese_summary\": \"æœ¬æ–‡è®¨è®ºäº†ä½¿ç”¨ç”Ÿæˆå¼ Transformer æ¨¡å‹å¤„ç†é•¿æ–‡æ¡£çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è¯„ä¼°ä¸åŒçš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº† BABILongï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨æå–å’Œå¤„ç†å†—é•¿æ–‡æœ¬ä¸­çš„åˆ†å¸ƒå¼äº‹å®æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬ GPT-4 å’Œ RAG çš„åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜å¸¸è§çš„æ–¹æ³•ä»…å¯¹é«˜è¾¾ 10^4 ä¸ªå…ƒç´ çš„åºåˆ—æœ‰æ•ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”¨å¾ªç¯è®°å¿†å¢å¼ºå¯¹ GPT-2 çš„å¾®è°ƒä½¿å…¶èƒ½å¤Ÿå¤„ç†æ¶‰åŠå¤šè¾¾ 10^7 ä¸ªå…ƒç´ çš„ä»»åŠ¡ã€‚è¿™ä¸€æˆå°±æ˜¯ä¸€é¡¹é‡å¤§é£è·ƒï¼Œå› ä¸ºè¿™æ˜¯è¿„ä»Šä¸ºæ­¢ä»»ä½•å¼€æ”¾ç¥ç»ç½‘ç»œæ¨¡å‹å¤„ç†çš„æœ€é•¿è¾“å…¥ï¼Œå±•ç¤ºäº†å¤„ç†é•¿åºåˆ—èƒ½åŠ›çš„æ˜¾è‘—æé«˜ã€‚\",\\n    \"english_title\": \"In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss\",\\n    \"english_summary\": \"This paper tackles the challenge of processing lengthy documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to 10^4 elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to 10^7 elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"DataDreamerï¼šåˆæˆæ•°æ®ç”Ÿæˆå’Œå¯å¤åˆ¶ LLM å·¥ä½œæµå·¥å…·\",\\n    \"chinese_summary\": \"å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) å·²æˆä¸º NLP ç ”ç©¶äººå‘˜åœ¨å„ç§ä»»åŠ¡ä¸­çš„ä¸€ä¸ªä¸»è¦å·¥å…·ã€‚å¦‚ä»Šï¼Œå¾ˆå¤šç ”ç©¶äººå‘˜åœ¨åˆæˆæ•°æ®ç”Ÿæˆã€ä»»åŠ¡è¯„ä¼°ã€æ¨¡å‹å¾®è°ƒã€è’¸é¦å’Œå…¶ä»–æ¨¡å‹å¾ªç¯ç ”ç©¶å·¥ä½œæµä¸­ä½¿ç”¨ LLMã€‚ä½†æ˜¯ï¼Œåœ¨ä½¿ç”¨è¿™äº›æ¨¡å‹æ—¶ä¼šäº§ç”Ÿä¸€äº›æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜æºäºå…¶è§„æ¨¡ã€å°é—­æºä»£ç çš„æœ¬è´¨ä»¥åŠè¿™äº›æ–°å…´å·¥ä½œæµç¼ºä¹æ ‡å‡†åŒ–å·¥å…·ã€‚è¿™äº›æ¨¡å‹çš„å¿«é€Ÿå‘å±•å’Œè¿™äº›ç‹¬æœ‰çš„æŒ‘æˆ˜å¯¹å¼€æ”¾ç§‘å­¦å’Œä½¿ç”¨è¿™äº›æ¨¡å‹çš„å·¥ä½œçš„å¯å¤åˆ¶æ€§äº§ç”Ÿäº†ç›´æ¥çš„ä¸åˆ©å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† DataDreamerï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æº Python åº“ï¼Œå…è®¸ç ”ç©¶äººå‘˜ç¼–å†™ç®€å•çš„ä»£ç æ¥å®ç°å¼ºå¤§çš„ LLM å·¥ä½œæµã€‚DataDreamer è¿˜å¸®åŠ©ç ”ç©¶äººå‘˜éµå®ˆæˆ‘ä»¬æè®®çš„æœ€ä½³å®è·µï¼Œä»¥ä¿ƒè¿›å¼€æ”¾ç§‘å­¦å’Œå¯å¤åˆ¶æ€§ã€‚è¯¥åº“å’Œæ–‡æ¡£å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å¾—ï¼šhttps://github.com/datadreamer-dev/DataDreamerã€‚\",\\n    \"english_title\": \"DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows\",\\n    \"english_summary\": \"Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at: https://github.com/datadreamer-dev/DataDreamer.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"GaussianObjectï¼šåªç”¨å››å¼ å›¾åƒè·å¾—å¸¦æœ‰é«˜æ–¯æ³¼æº…çš„é«˜è´¨é‡3Då¯¹è±¡\",\\n    \"chinese_summary\": \"é‡å»ºå’Œæ¸²æŸ“é«˜åº¦ç¨€ç–è§†å›¾ä¸­çš„3Då¯¹è±¡å¯¹æ¨å¹¿3Dè§†è§‰æŠ€æœ¯çš„åº”ç”¨å’Œæ”¹å–„ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç¨€ç–è§†å›¾çš„å›¾åƒåªåŒ…å«éå¸¸æœ‰é™çš„3Dä¿¡æ¯ï¼Œè¿™å¸¦æ¥äº†ä¸¤ä¸ªé‡å¤§æŒ‘æˆ˜ï¼š1ï¼‰éš¾ä»¥å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå› ä¸ºåŒ¹é…çš„å›¾åƒå¤ªå°‘ï¼›2ï¼‰ç”±äºè§†å›¾è¦†ç›–ä¸è¶³ï¼Œå¯¹è±¡ä¿¡æ¯éƒ¨åˆ†çœç•¥æˆ–é«˜åº¦å‹ç¼©ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GaussianObjectï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨é«˜æ–¯æ³¼æº…æ¥è¡¨ç¤ºå’Œæ¸²æŸ“3Då¯¹è±¡çš„æ¡†æ¶ï¼Œå®ƒä»…ä½¿ç”¨4ä¸ªè¾“å…¥å›¾åƒå°±èƒ½å®ç°é«˜æ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»äº†è§†è§‰åŒ…ç»œå’Œæµ®åŠ¨æ¶ˆé™¤æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å°†ç»“æ„å…ˆå…¥ä¸ºä¸»åœ°æ³¨å…¥åˆ°åˆå§‹ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œä»¥å¸®åŠ©å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªç²—ç•¥çš„3Dé«˜æ–¯è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬åŸºäºæ‰©æ•£æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªé«˜æ–¯ä¿®å¤æ¨¡å‹æ¥è¡¥å……è¢«çœç•¥çš„å¯¹è±¡ä¿¡æ¯ï¼Œå…¶ä¸­é«˜æ–¯ä½“è¿›ä¸€æ­¥å¾—åˆ°ç»†åŒ–ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªç”Ÿæˆç­–ç•¥æ¥è·å¾—ç”¨äºè®­ç»ƒä¿®å¤æ¨¡å‹çš„å›¾åƒå¯¹ã€‚æˆ‘ä»¬çš„GaussianObjectåœ¨å‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬MipNeRF360ã€OmniObject3Då’ŒOpenll-luminationï¼Œä»…ä½¿ç”¨4ä¸ªè§†å›¾å°±è·å¾—äº†å¼ºå¤§çš„é‡å»ºç»“æœï¼Œå¹¶ä¸”æ˜æ˜¾ä¼˜äºä»¥å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢https://gaussianobject.github.io/ã€‚\",\\n    \"english_title\": \"GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting\",\\n    \"english_summary\": \"Reconstructing and rendering 3D objects from 2D images has been a longstanding and important topic, which plays critical roles in a vast range of real-life applications. One key factor that impedes users, especially ones without expert knowledge, from widely using these techniques is that usually dozens of multi-view images need to be captured, which is cumbersome and sometimes impractical. Efficiently reconstructing high-quality 3D objects from highly sparse captured images is of great value for expediting downstream applications such as 3D asset creation for game/movie production and AR/VR products.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"LLM Comparatorï¼šå¯è§†åŒ–åˆ†æï¼Œè¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¶æ’è¯„ä¼°\",\\n    \"chinese_summary\": \"è¯¥ PDF æ–‡æ¡£ä»‹ç»äº† LLM Comparatorï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¯è§†åŒ–åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) è‡ªåŠ¨å¹¶æ’è¯„ä¼°ç»“æœçš„äº¤äº’å¼å·¥å…·ã€‚è¯¥å·¥å…·æ”¯æŒæ¨¡å‹å¼€å‘è€…å’Œç ”ç©¶äººå‘˜æ¯”è¾ƒä¸¤ä¸ª LLM çš„å“åº”è´¨é‡ï¼Œè¯†åˆ«å‡ºä½•æ—¶ä»¥åŠä¸ºä½•ä¸€ä¸ªæ¨¡å‹çš„æ€§èƒ½ä¼˜äºæˆ–åŠ£äºå¦ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶æ¢ç´¢ä¸¤ä¸ªæ¨¡å‹å“åº”ä¹‹é—´çš„å·®å¼‚ã€‚LLM Comparator æä¾›äº†ä¸€ä¸ªäº¤äº’å¼è¡¨æ ¼ï¼Œç”¨äºæ£€æŸ¥å•ä¸ªæç¤ºåŠå…¶å“åº”ï¼Œä»¥åŠä¸€ä¸ªå¯è§†åŒ–æ‘˜è¦ï¼Œæ”¯æŒå¸®åŠ©ç†è§£æ¨¡å‹å·®å¼‚çš„åˆ†æå·¥ä½œæµã€‚\",\\n    \"english_title\": \"LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models\",\\n    \"english_summary\": \"This PDF document presents LLM Comparator, an interactive tool for visual analytics of automatic side-by-side evaluation results of large language models (LLMs). It enables model developers and researchers to compare the quality of responses from two LLMs, identify when and why one model performs better or worse than the other, and explore the differences in their responses. LLM Comparator provides an interactive table for inspecting individual prompts and their responses, and a visualization summary that supports analytical workflows for understanding model differences.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"LAVEï¼šé’ˆå¯¹è§†é¢‘ç¼–è¾‘çš„ LLM åŠ©åŠ›ä»£ç†ä¸è‡ªç„¶è¯­è¨€å¢å¼º\",\\n    \"chinese_summary\": \"LAVE æ˜¯ä¸€æ¬¾è§†é¢‘ç¼–è¾‘å·¥å…·ï¼Œå®ƒæä¾›äº† LLM åŠ©åŠ›ä»£ç†ä¸è‡ªç„¶è¯­è¨€å¢å¼ºåŠŸèƒ½ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æ›´è½»æ¾åœ°ç¼–è¾‘è§†é¢‘ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ–‡å­—ä¸ LAVE çš„è§†é¢‘ç¼–è¾‘ä»£ç†äº’åŠ¨ï¼Œåœ¨æ•´ä¸ªç¼–è¾‘è¿‡ç¨‹ä¸­è·å¾—å®æ—¶å¸®åŠ©ã€‚æ­¤å¤–ï¼ŒLAVE è¿˜æä¾›ä¸€ç³»åˆ—è¯­è¨€å¢å¼ºåŠŸèƒ½ï¼Œä¾‹å¦‚è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ ‡é¢˜ã€æ˜¾ç¤ºè§†é¢‘æ‘˜è¦å¸®åŠ©ç”¨æˆ·å¿«é€Ÿäº†è§£è§†é¢‘å†…å®¹ã€åœ¨ç¼©ç•¥å›¾åº“ä¸­é€šè¿‡ç‚¹å‡»è§†é¢‘å³å¯å°†å…¶æ·»åŠ åˆ°ç¼–è¾‘æ—¶é—´è½´ç­‰ï¼Œè¿™äº›åŠŸèƒ½å¯ä»¥æå¤§åœ°ç®€åŒ–å’ŒåŠ é€Ÿè§†é¢‘ç¼–è¾‘æµç¨‹ï¼Œé™ä½æ–°æ‰‹å…¥é—¨é—¨æ§›ã€‚\",\\n    \"english_title\": \"LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing\",\\n    \"english_summary\": \"The Increasing demand for video content creation has been hindered by the technical expertise and effort typically required for video editing. This paper presents an integration of large language models (LLMs) into the video editing workflow to lower these barriers, specifically focusing on assistance and language augmentation. Our proposed system, LAVE, offers an agent that interacts with users using natural language, providing assistance with various editing tasks. It also incorporates language augmentation into the editing workflow, with features such as automated video summarization, video title generation, and quick video selection from a gallery. We believe that LAVE simplifies the video editing process, making it more accessible to a wider range of users.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"åŸºäºå‡½æ•°è°ƒç”¨çš„é›¶æ ·æœ¬å¯¹è¯çŠ¶æ€è¿½è¸ªå™¨ä¸­çš„å¤§è¯­è¨€æ¨¡å‹\",\\n    \"chinese_summary\": \"å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶åœ¨ä¸€èˆ¬è¯­å¢ƒä¸‹çš„é«˜çº§ç†è§£å’Œç”Ÿæˆèƒ½åŠ›è€Œåœ¨ä¼šè¯ç³»ç»Ÿä¸­æ—¥ç›Šæ™®åŠã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä»»åŠ¡å¯¼å‘å¯¹è¯ï¼ˆTODï¼‰ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶ä¸å°½å¦‚äººæ„ï¼Œè¿™ä¸ä»…éœ€è¦ç”Ÿæˆå“åº”ï¼Œè¿˜éœ€è¦åœ¨ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸå†…è¿›è¡Œæœ‰æ•ˆçš„å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡å‡½æ•°è°ƒç”¨ä½¿ç”¨ LLM è§£å†³ DST çš„æ–°æ–¹æ³• FNCTODã€‚è¯¥æ–¹æ³•æ”¹è¿›äº†é›¶æ ·æœ¬ DSTï¼Œå…è®¸é€‚åº”ä¸åŒçš„é¢†åŸŸï¼Œè€Œæ— éœ€å¤§é‡æ•°æ®æ”¶é›†æˆ–æ¨¡å‹è°ƒæ•´ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä¸­ç­‰è§„æ¨¡çš„å¼€æºå’Œä¸“æœ‰ LLM éƒ½å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼šåœ¨ä¸Šä¸‹æ–‡ä¸­æç¤ºä¸‹ï¼Œå®ƒä½¿å„ç§ 7B æˆ– 13B å‚æ•°æ¨¡å‹èƒ½å¤Ÿè¶…è¶Š ChatGPT å®ç°çš„å…ˆå‰æœ€å…ˆè¿›ï¼ˆSOTAï¼‰æŠ€æœ¯ï¼Œå¹¶å°† ChatGPT çš„æ€§èƒ½æé«˜äº† 5.6% çš„å¹³å‡ JGAã€‚GPT-3.5 å’Œ GPT-4 çš„å•ä¸ªæ¨¡å‹ç»“æœåˆ†åˆ«æé«˜äº† 4.8% å’Œ 14%ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œé€šè¿‡é’ˆå¯¹å°è§„æ¨¡çš„ä¸åŒä»»åŠ¡å¯¼å‘å¯¹è¯è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºä¸­ç­‰è§„æ¨¡çš„æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ 13B å‚æ•° LLaMA2-Chat æ¨¡å‹ï¼‰é…å¤‡å‡½æ•°è°ƒç”¨èƒ½åŠ›å’Œä¸ ChatGPT ç›¸åª²ç¾çš„ DST æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå…¶èŠå¤©èƒ½åŠ›ã€‚æˆ‘ä»¬å°†å¼€æºå®éªŒä»£ç å’Œæ¨¡å‹ã€‚\",\\n    \"english_title\": \"Large Language Models as Zero-shot Dialogue State Tracker through Function Calling\",\\n    \"english_summary\": \"Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FNCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPTâ€™s performance beating the SOTA by 5.6% Avg. JGA. Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We will open-source experimental code and model.\"\\n  }\\n}',\n",
       " '{\\n \"Summary\": {\\n  \"chinese_title\": \"æ„å»ºå»‰ä»·ç¼©æ”¾ï¼šä¸€ç§ç”¨äºé«˜åˆ†è¾¨ç‡é€‚åº”çš„è‡ªçº§è”æ‰©æ•£æ¨¡å‹\",\\n  \"chinese_summary\": \"æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­å·²è¢«è¯æ˜éå¸¸æœ‰æ•ˆï¼›ç„¶è€Œï¼Œç”±äºå•å°ºåº¦è®­ç»ƒæ•°æ®ï¼Œå®ƒä»¬åœ¨ç”Ÿæˆä¸åŒå¤§å°çš„å›¾åƒæ—¶ä»ç„¶é¢ä¸´æ„å›¾æŒ‘æˆ˜ã€‚é’ˆå¯¹é«˜åˆ†è¾¨ç‡è°ƒæ•´å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—å’Œä¼˜åŒ–èµ„æºï¼Œä½†ä»æ— æ³•å®ç°ä¸ä½åˆ†è¾¨ç‡æ¨¡å‹ç›¸å½“çš„ç”Ÿæˆèƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªçº§è”æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ä»ç»è¿‡è‰¯å¥½è®­ç»ƒçš„ä½åˆ†è¾¨ç‡æ¨¡å‹ä¸­è·å¾—çš„ä¸°å¯ŒçŸ¥è¯†ï¼Œå¿«é€Ÿé€‚åº”é«˜åˆ†è¾¨ç‡å›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œé‡‡ç”¨æ— è°ƒä¼˜æˆ–å»‰ä»·ä¸Šé‡‡æ ·å™¨è°ƒä¼˜èŒƒä¾‹ã€‚è‡ªçº§è”æ‰©æ•£æ¨¡å‹é›†æˆäº†åºåˆ—å¤šå°ºåº¦ä¸Šé‡‡æ ·å™¨æ¨¡å—ï¼Œèƒ½æœ‰æ•ˆåœ°é€‚åº”æ›´é«˜çš„åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¿ç•™äº†åŸå§‹æ„å›¾å’Œç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ¢è½´å¼•å¯¼å™ªå£°é‡æ–°è°ƒåº¦ç­–ç•¥ï¼Œä»¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹å¹¶æ”¹å–„å±€éƒ¨ç»“æ„ç»†èŠ‚ã€‚ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†è®­ç»ƒé€Ÿåº¦æé«˜äº† 5 å€ï¼Œå¹¶ä¸”ä»…éœ€è¦é¢å¤–çš„ 0.002M è°ƒä¼˜å‚æ•°ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é€šè¿‡ä»…å¾®è°ƒ 10k æ­¥ï¼Œåœ¨å‡ ä¹æ²¡æœ‰é¢å¤–æ¨ç†æ—¶é—´çš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿé€‚åº”æ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒå’Œè§†é¢‘åˆæˆã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨ https://github.com/GuoLanging/Self-Cascade/ å‘å¸ƒã€‚\",\\n  \"english_title\": \"Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution Adaptation\",\\n  \"english_summary\": \"Diffusion models have proven to be highly effective in image and video generation; however, they still face composition challenges when generating images of varying sizes due to single-scale training data. Adapting large pre-trained diffusion models for higher resolution demands substantial computational and optimization resources, yet achieving a generation capability comparable to low-resolution models remains elusive. This paper proposes a novel self-cascade diffusion model that leverages the rich knowledge gained from a well-trained low-resolution model for rapid adaptation to higher-resolution image and video generation, employing either tuning-free or cheap upsampler tuning paradigms. Integrating a sequence of multi-scale upsampler modules, the self-cascade diffusion model can efficiently adapt to a higher resolution, preserving the original composition and generation capabilities. We further propose a pivot-guided noise re-schedule strategy to speed up the inference process and improve local structural details. Compared to full fine-tuning, our approach achieves a 5x training speed-up and requires only an additional 0.002M tuning parameters. Extensive experiments demonstrate that our approach can quickly adapt to higher resolution image and video synthesis by fine-tuning for just 10k steps, with virtually no additional inference time. Our code will be released at https://github.com/GuoLanging/Self-Cascade/.\"\\n }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"é€šç”¨æ“ä½œç•Œé¢ï¼šåœ¨ä¸ä½¿ç”¨é‡å¤–æœºå™¨äººçš„æƒ…å†µä¸‹ï¼Œåœ¨é‡å¤–å¯¹æœºå™¨è¿›è¡Œæ•™å­¦\",\\n    \"chinese_summary\": \"æˆ‘ä»¬æå‡ºäº†é€šç”¨æ“ä½œç•Œé¢ï¼ˆUMIï¼‰â€”â€”ä¸€ç§æ•°æ®æ”¶é›†å’Œç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸ç›´æ¥å°†é‡å¤–äººç±»æ¼”ç¤ºä¸­çš„æŠ€èƒ½è½¬ç§»åˆ°å¯éƒ¨ç½²æœºå™¨ç­–ç•¥ä¸­ã€‚UMI é‡‡ç”¨æ‰‹æŒæŠ“æ‰‹ç»“åˆè°¨æ…çš„ç•Œé¢è®¾è®¡ï¼Œå¯å®ç°ä¾¿æºã€ä½æˆæœ¬ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ•°æ®æ”¶é›†ï¼Œä»¥è¿›è¡Œæå…·æŒ‘æˆ˜æ€§çš„åŒæ‰‹åŠ¨å’ŒåŠ¨æ€æ“ä½œæ¼”ç¤ºã€‚ä¸ºä¿ƒè¿›å¯éƒ¨ç½²ç­–ç•¥å­¦ä¹ ï¼ŒUMI åŒ…å«ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„ç­–ç•¥ç•Œé¢ï¼Œè¯¥ç•Œé¢å…·å¤‡æ¨ç†æ—¶å»¶è¿ŸåŒ¹é…å’Œç›¸å¯¹è½¨è¿¹åŠ¨ä½œè¡¨ç¤ºã€‚æ‰€å­¦ç­–ç•¥ä¸ç¡¬ä»¶æ— å…³ï¼Œä¸”å¯åœ¨å¤šç§æœºå™¨äººå¹³å°ä¸Šéƒ¨ç½²ã€‚é…å¤‡è¿™äº›åŠŸèƒ½ï¼ŒUMI æ¡†æ¶è§£é”äº†æ–°çš„æœºå™¨äººæ“ä½œèƒ½åŠ›ï¼Œä»…é€šè¿‡é’ˆå¯¹æ¯ä¸ªä»»åŠ¡æ›´æ”¹è®­ç»ƒæ•°æ®ï¼Œå³å¯å®ç°é›¶æ¬¡æ¦‚æ‹¬çš„åŠ¨æ€ã€åŒæ‰‹åŠ¨ã€ç²¾ç¡®å®šä½å’Œé•¿æ—¶é—´è¡Œä¸ºã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„çœŸå®å®éªŒå±•ç¤ºäº† UMI çš„å¤šåŠŸèƒ½æ€§å’Œæœ‰æ•ˆæ€§ï¼Œåœ¨ UMI ä¸Šå­¦ä¹ çš„ç­–ç•¥ï¼Œåœ¨é’ˆå¯¹å„ç§äººç±»æ¼”ç¤ºè¿›è¡Œè®­ç»ƒåï¼Œå¯é›¶æ¬¡æ¦‚æ‹¬åˆ°æ–°ç¯å¢ƒå’Œç‰©ä½“ã€‚UMI çš„ç¡¬ä»¶å’Œè½¯ä»¶ç³»ç»Ÿå¯åœ¨ https://umi-gripper.github.io è·å–å¼€æºã€‚\",\\n    \"english_title\": \"Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots\",\\n    \"english_summary\": \"We present Universal Manipulation Interface (UMI)â€”a data collection and policy learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot manipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors, by only changing the training data for each task. We demonstrate UMIâ€™s versatility and efficacy with comprehensive real-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when trained on diverse human demonstrations. UMIâ€™s hardware and software system is open-sourced at https://umi-gripper.github.io.\"\\n  }\\n}',\n",
       " '{\\n \"Summary\": {\\n  \"chinese_title\": \"SPARï¼šé€šè¿‡é•¿æœŸå‚ä¸æ³¨æ„åŠ›å®ç°ä¸ªæ€§åŒ–çš„åŸºäºå†…å®¹çš„æ¨è\",\\n  \"chinese_summary\": \"åˆ©ç”¨ç”¨æˆ·çš„é•¿æœŸå‚ä¸å†å²å¯¹äºä¸ªæ€§åŒ–çš„å†…å®¹æ¨èè‡³å…³é‡è¦ã€‚é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (PLM) åœ¨ NLP ä¸­çš„æˆåŠŸä½¿å…¶è¢«ç”¨äºå¯¹ç”¨æˆ·å†å²å’Œå€™é€‰é¡¹ç›®è¿›è¡Œç¼–ç ï¼Œå¹¶å°†å†…å®¹æ¨èæ„å»ºä¸ºæ–‡æœ¬è¯­ä¹‰åŒ¹é…ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œåœ¨å¤„ç†è¶…é•¿çš„ç”¨æˆ·å†å²æ–‡æœ¬å’Œä¸è¶³çš„ç”¨æˆ·ä¸é¡¹ç›®çš„äº¤äº’æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå†…å®¹çš„æ¨èæ¡†æ¶ SPARï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°è§£å†³äº†ä»é•¿æœŸçš„ç”¨æˆ·å‚ä¸å†å²ä¸­æå–æ•´ä½“ç”¨æˆ·å…´è¶£çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡åˆ©ç”¨ PLMã€å¤šæ³¨æ„åŠ›å±‚å’Œæ³¨æ„åŠ›ç¨€ç–æœºåˆ¶ä»¥åŸºäºä¼šè¯çš„æ–¹å¼å¯¹ç”¨æˆ·å†å²è¿›è¡Œç¼–ç æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç”¨æˆ·ç«¯å’Œé¡¹ç›®ç«¯ç‰¹å¾å……åˆ†èåˆç”¨äºå‚ä¸åº¦é¢„æµ‹ï¼ŒåŒæ—¶ä¿æŒä¸¤ç«¯çš„ç‹¬ç«‹è¡¨ç¤ºï¼Œè¿™å¯¹äºå®é™…æ¨¡å‹éƒ¨ç½²æ˜¯æœ‰æ•ˆçš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä»ç”¨æˆ·å‚ä¸å†å²ä¸­æå–å…¨å±€å…´è¶£æ¥å¢å¼ºç”¨æˆ·ç”»åƒã€‚åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿› (SoTA) æ–¹æ³•ã€‚\",\\n  \"english_title\": \"SPAR: Personalized Content-Based Recommendation via Long Engagement Attention\",\\n  \"english_summary\": \"Leveraging usersâ€™ long engagement histories is essential for personalized content recommendations. The success of pretrained language models (PLMs) in NLP has led to their use in encoding user histories and candidate items, framing content recommendations as textual semantic matching tasks. However, existing works still struggle with processing very long user historical text and insufficient user-item interaction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles the challenges of holistic user interest extraction from the long user engagement history. It achieves so by leveraging PLM, poly-attention layers and attention sparsity mechanisms to encode userâ€™s history in a session-based manner. The user and item side features are sufficiently fused for engagement prediction while maintaining standalone representations for both sides, which is efficient for practical model deployment. Moreover, we enhance user profiling by exploiting large language model (LLM) to extract global interests from user engagement history. Extensive experiments on two benchmark datasets demonstrate that our framework outperforms existing state-of-the-art (SoTA) methods.\"\\n }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"PaLM2-VAdapterï¼šæ¸è¿›å¼å¯¹é½è¯­è¨€æ¨¡å‹æˆä¸ºå¼ºå¤§çš„è§†è§‰è¯­è¨€é€‚é…å™¨\",\\n    \"chinese_summary\": \"æœ¬æ–‡è¡¨æ˜ï¼Œæ¸è¿›å¼å¯¹é½çš„è¯­è¨€æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ¡¥æ¥å†»ç»“çš„è§†è§‰ç¼–ç å™¨å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è™½ç„¶è§†è§‰ç¼–ç å™¨å’Œ LLM çš„åŸºæœ¬æ¶æ„å’Œé¢„è®­ç»ƒæ–¹æ³•å·²ç»å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†è§†è§‰è¯­è¨€é€‚é…å™¨çš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥åœ¨æœ€è¿‘çš„ç ”ç©¶ä¸­å·®å¼‚å¾ˆå¤§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹æœ€å…ˆè¿›çš„æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨æ¶æ„è¿›è¡Œäº†å½»åº•æ¢ç´¢ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå…·æœ‰æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨çš„è§†è§‰è¯­è¨€å¯¹é½è¡¨ç°å‡ºç¼“æ…¢çš„æ”¶æ•›æ€§å’Œæœ‰é™çš„å¯æ‰©å±•æ€§ï¼Œç¼ºä¹ç›´æ¥çš„ç›‘ç£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PaLM2-VAdapterï¼Œé‡‡ç”¨æ¸è¿›å¼å¯¹é½çš„è¯­è¨€æ¨¡å‹ä½œä¸ºè§†è§‰è¯­è¨€é€‚é…å™¨ã€‚ä¸å…·æœ‰æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨çš„å¼ºå¤§åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡­ç»éªŒè¯æ˜å…·æœ‰æ›´å¿«çš„æ”¶æ•›æ€§ã€æ›´é«˜çš„æ€§èƒ½å’Œæ›´å¼ºçš„å¯æ‰©å±•æ€§ã€‚è·¨è¶Šå›¾åƒå’Œè§†é¢‘çš„å„ç§è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œå­—å¹•ä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å±•ç¤ºäº†æœ€å…ˆè¿›çš„è§†è§‰ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å‚æ•°å°‘äº†30~70%ï¼Œè¿™æ ‡å¿—ç€æ•ˆç‡çš„æ˜¾ç€æé«˜ã€‚\",\\n    \"english_title\": \"PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter\",\\n    \"english_summary\": \"This paper demonstrates that a progressively aligned language model can effectively bridge frozen vision encoders and large language models (LLMs). While the fundamental architecture and pre-training methods of vision encoders and LLMs have been extensively studied, the architecture and training strategy of vision-language adapters vary significantly across recent works. Our research undertakes a thorough exploration of the state-of-the-art perceiver resampler architecture and builds a strong baseline. However, we observe that the vision-language alignment with perceiver resampler exhibits slow convergence and limited scalability with a lack of direct supervision. To address this issue, we propose PaLM2-VAdapter, employing a progressively aligned language model as the vision-language adapter. Compared to the strong baseline with perceiver resampler, our method empirically shows faster convergence, higher performance and stronger scalability. Extensive experiments across various Visual Question Answering (VQA) and captioning tasks on both images and videos demonstrate that our model exhibits state-of-the-art visual understanding and multi-modal reasoning capabilities. Notably, our method achieves these advancements with 30~70% fewer parameters than the state-of-the-art large vision-language models, marking a significant efficiency improvement.\"\\n  }\\n}',\n",
       " '{\\n  \"Summary\": {\\n    \"chinese_title\": \"RLVFï¼šä»è¯­è¨€åé¦ˆä¸­å­¦ä¹ ï¼Œé¿å…è¿‡åº¦æ¦‚æ‹¬\",\\n    \"chinese_summary\": \"éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªè¡Œä¸šå’Œä¸ªä½“ä¸­çš„å¹¿æ³›é‡‡ç”¨ï¼Œé’ˆå¯¹ç‰¹å®šç”¨æˆ·æˆ–ç”¨ä¾‹ï¼ŒæŒ‰ç…§é«˜çº§äººç±»åé¦ˆå¯¹å…¶è¿›è¡Œè°ƒæ•´çš„èƒ½åŠ›å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚è™½ç„¶ LLM ç”¨æˆ·é€šå¸¸å¸Œæœ›æ¨¡å‹å§‹ç»ˆéµå¾ªå¹¿æ³›çš„åŸåˆ™ï¼Œæ¯”å¦‚ç”Ÿæˆæµç•…çš„æ–‡æœ¬ï¼Œä½†ä¸ªåˆ«ç”¨æˆ·å’Œç”¨ä¾‹æœ‰æ›´ç»†å¾®çš„åå¥½ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯èƒ½è¦æ±‚ LLM å†™å‡ºæ›´ç®€æ´çš„å·¥ä½œç”µå­é‚®ä»¶ï¼Œä½†è¦å†™å‡ºæ›´è¯¦ç»†çš„ä¸ªäººç”µå­é‚®ä»¶ï¼Œè¿™ä½¿å¾—åé¦ˆå…·æœ‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚æ ¹æ®è¿™äº›åå¥½è°ƒæ•´æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼šéœ€è¦å¤§é‡èµ„æºæ‰èƒ½æ”¶é›†æ‰€æœ‰ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„åå¥½ï¼Œå¹¶ä¸”åœ¨ä¸€ä¸ªä¸Šä¸‹æ–‡ä¸­çš„æ¨¡å‹å¾®è°ƒä¼šå¯¹å…¶ä»–ä¸Šä¸‹æ–‡ä¸­çš„æ¨¡å‹è¡Œä¸ºäº§ç”Ÿä¸å¯é¢„æµ‹çš„å½±å“ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨è¯­è¨€åé¦ˆæ¥è°ƒæ•´æ¨¡å‹çš„é—®é¢˜ï¼Œè¿™ç§åé¦ˆå¯¹äºäººä»¬æ¥è¯´å¿«é€Ÿä¸”å®¹æ˜“æä¾›ï¼ˆè§å›¾ 1ï¼‰ã€‚åˆå¹¶åé¦ˆçš„å¸¸è§æ–¹æ³•ï¼Œä¾‹å¦‚ç›‘ç£ä¸Šä¸‹æ–‡è’¸é¦ (SCD) æˆ–å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨äººç±»åé¦ˆ (RLHF)ï¼Œä½¿ç”¨ç¤ºä¾‹çº§ç›‘ç®¡é€šè¿‡ç›‘ç£å®Œæˆæˆ–åå¥½æ ‡ç­¾ã€‚æ­¤ç±»æ–¹æ³•éœ€è¦å¤§é‡ç”¨æˆ·æä¾›çš„ï¼ˆåå¥½ï¼‰æ•°æ®è¯­æ–™åº“ï¼Œè€Œè·å–è¿™äº›æ•°æ®è¯­æ–™åº“å¯èƒ½æ—¢æ˜‚è´µåˆç¹çã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¸ä¼šé™åˆ¶åé¦ˆå¯èƒ½é€‚ç”¨çš„ä¸Šä¸‹æ–‡ä¹‹å¤–çš„æ¨¡å‹è¡Œä¸ºï¼Œå› æ­¤ LLM å¯èƒ½ä¼šä»¥æ„å¤–çš„æ–¹å¼è°ƒæ•´å…¶è¡Œä¸ºï¼Œä¾‹å¦‚åœ¨åå¥½ä»…é€‚ç”¨äºä¸ªäººç”µå­é‚®ä»¶æ—¶ç”Ÿæˆæ›´å†—é•¿çš„å·¥ä½œç”µå­é‚®ä»¶ã€‚è¯­è¨€åé¦ˆå¯¹äºäººç±»æ¥è¯´æ›´å®¹æ˜“ã€æ›´å¿«é€Ÿåœ°æä¾›ã€‚ä¸ºæ­¤ï¼Œå¦ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯å°†æ­¤ç±»è¯­è¨€åé¦ˆçº³å…¥æç¤ºä¸­ï¼Œå¯èƒ½é€šè¿‡è¿­ä»£è¿‡ç¨‹æ¥æŒç»­æ·»åŠ å…¶ä»–åé¦ˆç‚¹ã€‚ç„¶è€Œï¼Œæ­¤æ–¹æ³•éœ€è¦åœ¨æ‰€æœ‰æœªæ¥æŸ¥è¯¢ä¸­é‡æ–°ä½¿ç”¨è¯¥æç¤ºã€‚éšç€æ›´å¤šåé¦ˆçš„ç§¯ç´¯ï¼ŒåŒ…å«è®¸å¤šä¸Šä¸‹æ–‡ç›¸å…³åé¦ˆçš„é•¿æç¤ºä¼šä½¿æ¨ç†å˜å¾—æ˜‚è´µï¼›æ­¤å¤–ï¼Œè¯†åˆ«å“ªäº›åé¦ˆåº”å½“åœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­é€‚ç”¨å¯èƒ½å˜å¾—å¾ˆå›°éš¾ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è°ƒæ•´ LLMï¼Œä»¥ä¾¿åœ¨æä¾›ä¸€ä¸ªæŒ‡å®šåé¦ˆçš„å¥å­æ—¶ï¼Œæ¨¡å‹èƒ½å¤Ÿè¾¨åˆ«åé¦ˆé€‚ç”¨çš„å“ªäº›æƒ…å†µï¼Œå¹¶åœ¨æœªæ¥è¾“å‡ºä¸­é€‚å½“åœ°çº³å…¥è¯¥åé¦ˆã€‚æˆ‘ä»¬æå‡ºäº†æƒ…å¢ƒåŒ–æ‰¹åˆ¤ä¸çº¦æŸæ€§åå¥½ä¼˜åŒ– (C3PO)ï¼Œå…¶ä¸­æˆ‘ä»¬é¦–å…ˆä¸ºè¶…å‡ºåé¦ˆèŒƒå›´å’Œæœªè¶…å‡ºåé¦ˆèŒƒå›´çš„æƒ…å¢ƒåˆæˆç”Ÿæˆå‡è®¾æ€§æç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹è¿™äº›æç¤ºè¿›è¡ŒåŸå§‹å®Œæˆé‡‡æ ·ï¼Œä¸åº”ç”¨åé¦ˆï¼Œä»¥åŠå¯¹æ‰€æ”¶é›†çš„åé¦ˆè¿›è¡Œä¿®è®¢ï¼Œä»¥ä¾¿ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆä¸€ä¸ªåˆæˆåå¥½æ•°æ®é›†ï¼ŒæŒ‡å®šåé¦ˆåº”å½“ï¼ˆå’Œä¸åº”å½“ï¼‰å¦‚ä½•åº”ç”¨ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æ ¹æ®åˆæˆåå¥½æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘è¶…å‡ºåé¦ˆé€‚ç”¨èŒƒå›´çš„æç¤ºçš„åŸå§‹æ¨¡å‹çš„å·®å¼‚ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å°†è¯­è¨€åé¦ˆåº”ç”¨äºç›¸å…³åœºæ™¯ï¼ŒåŒæ—¶ä¿ç•™äº†å…¶ä»–ä¸Šä¸‹æ–‡çš„ç°æœ‰è¡Œä¸ºã€‚å¯¹äºäººç±»å’Œ GPT-4 ç”Ÿæˆçš„è¯­è¨€åé¦ˆï¼ŒC3PO æœ‰æ•ˆåœ°éµå®ˆç»™å®šçš„åé¦ˆï¼Œä¸ä¸Šä¸‹æ–‡ä¸­åŸºå‡†ç›¸å½“ï¼ŒåŒæ—¶å°†è¿‡åº¦æ¦‚æ‹¬å‡å°‘äº† 30%ã€‚\",\\n    \"english_title\": \"RLVF: Learning from Verbal Feedback without Overgeneralization\",\\n    \"english_summary\": \"The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as â€œDonâ€™t use emojis when drafting emails to my boss.â€ However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should not) be applied. It then fine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the original model for prompts where the feedback does not apply. Our experimental results indicate that our approach effectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts. For both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably to in-context baselines while reducing overgeneralization by 30%.\"\\n  }\\n}']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = []\n",
    "\n",
    "# Specify the file path\n",
    "output_file = \"./summary_test.txt\"\n",
    "\n",
    "for pdflink in pdflink_list:\n",
    "    pdf_text = extract_text_from_pdf(pdflink)\n",
    "    \n",
    "    summary = model.generate_content(prompt + pdf_text).text\n",
    "    summaries.append(summary)\n",
    "    \n",
    "    # Write the summaries to the file\n",
    "    with open(output_file, \"a\") as file:\n",
    "        file.write(summary + \"\\n\")\n",
    "\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the file path\n",
    "output_file = \"./summaries.pkl\"\n",
    "\n",
    "# Store the summaries using pickle\n",
    "with open(output_file, \"wb\") as file:\n",
    "    pickle.dump(summaries, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"./summaries.pkl\"\n",
    "\n",
    "# Read the pickle file\n",
    "with open(file_path, \"rb\") as file:\n",
    "    summaries = pickle.load(file)\n",
    "\n",
    "summary_dict_list = []\n",
    "\n",
    "for summary in summaries:\n",
    "    summary_dict = json.loads(summary)\n",
    "    summary_dict_list.append(summary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "today = datetime.today().strftime(\"%B %d, %Y\")\n",
    "passage = f\"LLM Daily Papersã€{today}ã€‘\\næ€»ç»“è‡ª: https://huggingface.co/papers\\n\\n\\n\"\n",
    "index = 0\n",
    "\n",
    "for summary_dict in summary_dict_list:\n",
    "    chinese_title = summary_dict[\"Summary\"][\"chinese_title\"]\n",
    "    chinese_summary = summary_dict[\"Summary\"][\"chinese_summary\"]\n",
    "    english_title = summary_dict[\"Summary\"][\"english_title\"]\n",
    "    english_summary = summary_dict[\"Summary\"][\"english_summary\"]\n",
    "    index += 1\n",
    "    \n",
    "    passage += f\"No.{index}\\n\\n{chinese_title}\\n\\n{chinese_summary}\\n\\n{english_title}\\n\\n{english_summary}\\n\\n\"\n",
    "\n",
    "passage = passage.strip()\n",
    "with open(\"passage.txt\", \"w\") as file:\n",
    "    file.write(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"æ— éœ€æç¤ºå³å¯è¿›è¡Œæ€æƒ³é“¾æ¨ç†\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"ä»¥å‰çš„è¯­è¨€æ¨¡å‹ï¼ˆLMLï¼‰æ¨ç†èƒ½åŠ›æå‡ç ”ç©¶ï¼Œä¸»è¦æ˜¯å…³æ³¨å¼ºåŒ–ç‰¹å®šçš„æç¤ºæŠ€å·§ï¼Œæ¯”å¦‚å°‘é‡æç¤ºæˆ–é›¶æç¤ºæ€æƒ³é“¾ï¼ˆCoTï¼‰æç¤ºã€‚è¿™äº›æ–¹</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡äººå·¥æç¤ºå·¥ç¨‹ã€‚ç ”ç©¶ä»ä¸€ä¸ªæ–°é¢–çš„è§’åº¦å‡ºå‘ï¼Œæå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šLMLæ˜¯å¦å¯ä»¥åœ¨æ²¡æœ‰æç¤ºçš„æƒ…å†µä¸‹è¿›è¡Œ</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æœ‰æ•ˆçš„æ¨ç†ï¼Ÿç ”ç©¶å‘ç°ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå¯ä»¥é€šè¿‡ç®€å•åœ°æ”¹å˜è§£ç è¿‡ç¨‹æ¥ä»é¢„è®­ç»ƒçš„LMLä¸­æå–CoTæ¨ç†è·¯å¾„ã€‚ç ”ç©¶æ²¡æœ‰ä½¿ç”¨ä¼ ç»Ÿçš„</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">è´ªå©ªè§£ç ï¼Œè€Œæ˜¯ç ”ç©¶äº†å‰kä¸ªæ›¿ä»£æ ‡è®°ï¼Œå‘ç°CoTè·¯å¾„ç»å¸¸å­˜åœ¨äºè¿™äº›åºåˆ—ä¸­ã€‚è¿™ç§æ–¹æ³•ä¸ä»…ç»•è¿‡äº†æç¤ºçš„æ··æ‚å› ç´ ï¼Œè¿˜å…è®¸ç ”ç©¶äºº</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å‘˜è¯„ä¼°LMLçš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶äººå‘˜è¿˜è§‚å¯Ÿåˆ°ï¼Œè§£ç è·¯å¾„ä¸­å­˜åœ¨CoTä¸æ¨¡å‹å¯¹è§£ç ç­”æ¡ˆçš„è¾ƒé«˜ç½®ä¿¡åº¦ç›¸å…³ã€‚ç½®ä¿¡åº¦é‡å¯ä»¥æœ‰æ•ˆåŒºåˆ†C</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">oTè·¯å¾„å’ŒéCoTè·¯å¾„ã€‚å„ç§æ¨ç†åŸºå‡†çš„å¤§é‡å®è¯ç ”ç©¶è¡¨æ˜ï¼Œæ‰€æå‡ºçš„CoTè§£ç æ–¹æ³•æ˜æ˜¾ä¼˜äºæ ‡å‡†è´ªå©ªè§£ç æ–¹æ³•ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Chain-of-Thought Reasoning Without Prompting\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Prior research on enhancing the reasoning capabilities of large language models </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(LLMs) primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought (CoT) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the decoding process. Rather than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conventional greedy decoding, we investigate the top-k alternative tokens, uncovering that CoT paths are frequently</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">assess the LLMsâ€™ intrinsic reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">path correlates with a higher confidence in the modelâ€™s decoded answer. This confidence metric effectively </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">differentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the proposed CoT-decoding substantially outperforms the standard greedy decoding.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"ç”Ÿæˆæ€§è¡¨å¾å¼æŒ‡å¯¼å¾®è°ƒ\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"åŸºäºæ–‡æœ¬çš„è¯­è¨€é—®é¢˜çš†å¯å½’ç»“ä¸ºç”Ÿæˆæˆ–åµŒå…¥ã€‚ç›®å‰ï¼Œæ¨¡å‹åªæ“…é•¿äºå…¶ä¸­ä¸€ç±»ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ç”Ÿæˆæ€§è¡¨å¾å¼æŒ‡å¯¼å¾®è°ƒï¼ˆGRITï¼‰ï¼Œè®­</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æŒ‡ä»¤æ¥åŒºåˆ†ç”Ÿæˆå’ŒåµŒå…¥ä»»åŠ¡ï¼Œä»¥å¤„ç†è¿™ä¸¤ç§ä»»åŠ¡ã€‚ä¸å…¶ä»–å¼€æ”¾æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬ç”Ÿæˆçš„ GRITLM 7B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">åœ¨æµ·é‡æ–‡æœ¬åµŒå…¥åŸºå‡†ï¼ˆMTEBï¼‰ä¸Šåˆ›ä¸‹æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å…¶è§„æ¨¡èŒƒå›´å†…ï¼Œåœ¨å„ç§ç”Ÿæˆæ€§ä»»åŠ¡ä¸Šéƒ½ä¼˜äºæ‰€æœ‰æ¨¡å‹ã€‚é€šè¿‡è¿›ä¸€æ­¥æ‰©å±•</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ï¼ŒGRITLM 8x7B ä¼˜äºæ‰€æœ‰æˆ‘ä»¬å°è¯•è¿‡çš„å¼€æ”¾ç”Ÿæˆè¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶ä»ç„¶æ˜¯æœ€ä½³åµŒå…¥æ¨¡å‹ä¹‹ä¸€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç° GRIT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">åŒ¹é…ä»…é’ˆå¯¹ç”Ÿæˆæˆ–åµŒå…¥æ•°æ®è¿›è¡Œçš„è®­ç»ƒï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç»Ÿä¸€ä¸¤è€…ï¼Œè€Œä¸ä¼šæŸå¤±æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡ GRIT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ç»Ÿä¸€èƒ½å¤Ÿå°†é’ˆå¯¹é•¿æ–‡æ¡£çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„é€Ÿåº¦æé«˜å¤§äº 60%ï¼Œä¸å†éœ€è¦å•ç‹¬çš„æ£€ç´¢å’Œç”Ÿæˆæ¨¡å‹ã€‚æ¨¡å‹ã€ä»£ç ç­‰å‡å¯åœ¨ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/ContextualAI/gritlm å…è´¹è·å¾—ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Generative Representational Instruction Tuning\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"All text-based language problems can be reduced to either generation or embedding.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Current models only perform well at one or the other. We introduce generative representational instruction tuning </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between them through instructions. Compared to other open models, our resulting GRITLM 7B sets a new state of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generative tasks. By scaling up further, GRITLM 8x7B outperforms all open generative language models that we tried </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">while still being among the best embedding models. Notably, we find that GRIT matches training on only generative </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">speeds up Retrieval-Augmented Generation (RAG) by &gt; 60% for long documents, by no longer requiring separate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval and generation models. Models, code, etc. are freely available at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/ContextualAI/gritlm.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"å¦‚ä½•è®­ç»ƒæ•°æ®é«˜æ•ˆçš„ LLM\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"å¤§è¯­è¨€æ¨¡å‹ (LLM) çš„è®­ç»ƒéå¸¸æ˜‚è´µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”¨äºé¢„è®­ç»ƒ LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">çš„æ•°æ®é«˜æ•ˆæ–¹æ³•ï¼Œå³æ—¨åœ¨ä¼˜åŒ–æ¨¡å‹è´¨é‡å’Œè®­ç»ƒèµ„æº/æ•°æ®æ¶ˆè€—çš„å¸•ç´¯æ‰˜å‰æ²¿çš„æŠ€æœ¯ã€‚æˆ‘ä»¬è¯•å›¾äº†è§£ä¸åŸºäº (i) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">è®¡ç®—æˆæœ¬é«˜çš„æ•°æ®è´¨é‡ä¼°è®¡å’Œ (ii) ç‰¹å¾ç©ºé—´ä¸­è¦†ç›–èŒƒå›´å’Œå¤šæ ·æ€§æœ€å¤§åŒ–æªæ–½ç›¸å…³çš„æ•°æ®é€‰æ‹©ä¾‹ç¨‹ç›¸å…³çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„ç¬¬ä¸€é¡¹æŠ€æœ¯ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ASK-LLMï¼Œåˆ©ç”¨æŒ‡ä»¤è°ƒæ•´ LLM çš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›æ¥ç›´æ¥è¯„ä¼°è®­ç»ƒæ ·æœ¬çš„è´¨é‡ã€‚ä¸ºäº†æé«˜è¦†ç›–ç‡ï¼Œæˆ‘ä»¬æå‡ºäº† DENSITY </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">é‡‡æ ·ï¼Œå®ƒå¯¹æ•°æ®åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ä»¥é€‰æ‹©å¤šæ ·åŒ–çš„æ ·æœ¬ã€‚åœ¨æˆ‘ä»¬çš„ 19 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä¸ªé‡‡æ ·å™¨çš„æ¯”è¾ƒä¸­ï¼Œæ¶‰åŠæ•°ç™¾ä¸ªè¯„ä¼°ä»»åŠ¡å’Œé¢„è®­ç»ƒè¿è¡Œï¼Œæˆ‘ä»¬å‘ç° ASK-LLM å’Œ DENSITY </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ˜¯å„è‡ªç±»åˆ«ä¸­çš„æœ€ä½³æ–¹æ³•ã€‚è¦†ç›–é‡‡æ ·å¯ä»¥æ¢å¤å®Œæ•´æ•°æ®çš„æ€§èƒ½ï¼Œè€Œä½¿ç”¨ ASK-LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ•°æ®è®­ç»ƒçš„æ¨¡å‹å§‹ç»ˆä¼˜äºå®Œæ•´æ•°æ®è®­ç»ƒâ€”â€”å³ä½¿æˆ‘ä»¬èˆå¼ƒäº† 90% çš„åŸå§‹æ•°æ®é›†ï¼ŒåŒæ—¶æ”¶æ•›é€Ÿåº¦æé«˜äº† 70%ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"How to Train Data-Efficient LLMs\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"The training of large language models (LLMs) is expensive. In this paper, we study</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">diversity-based measures in the feature space. Our first technique, ASK-LLM, leverages the zero-shot reasoning </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">propose DENSITY sampling, which models the data distribution to select a diverse sample. In our comparison of 19 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">samplers, involving hundreds of evaluation tasks and pre-training runs, we find that ASK-LLM and DENSITY are the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">best methods in their respective categories. Coverage sampling can recover the performance of the full data, while </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models trained on ASK-LLM data consistently outperform full-data trainingâ€”even when we reject 90% of the original </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dataset, while converging up to 70% faster.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"æ‹¥æœ‰æé•¿èƒŒæ™¯è®°å¿†æ‘˜è¦çš„ç±»äººé˜…è¯»ä»£ç†\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä¸ä»…å—æœ€å¤§èƒŒæ™¯é•¿åº¦é™åˆ¶ï¼Œè€Œä¸”æ— æ³•ç¨³å¥åœ°æ¶ˆè€—é•¿è¾“å…¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† ReadAgentï¼Œè¿™æ˜¯ä¸€ç§ LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä»£ç†ç³»ç»Ÿï¼Œåœ¨æˆ‘ä»¬çš„å®éªŒä¸­å°†æœ‰æ•ˆèƒŒæ™¯é•¿åº¦å¢åŠ äº† 20 å€ã€‚å—äººç±»äº’åŠ¨é˜…è¯»é•¿æ–‡æ¡£æ–¹å¼çš„å¯å‘ï¼Œæˆ‘ä»¬å°† ReadAgent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å®ç°ä¸ºä¸€ä¸ªç®€å•çš„æç¤ºç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨ LLM çš„å…ˆè¿›è¯­è¨€èƒ½åŠ›æ¥ï¼š(1) å†³å®šå°†å“ªäº›å†…å®¹ä¸€èµ·å­˜å‚¨åœ¨ä¸€ä¸ªè®°å¿†ç‰‡æ®µä¸­ï¼Œ(2) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å°†è¿™äº›è®°å¿†ç‰‡æ®µå‹ç¼©æˆç§°ä¸ºæ‘˜è¦è®°å¿†çš„ç®€çŸ­ç‰‡æ®µè®°å¿†ï¼Œä»¥åŠ (3) é‡‡å–æªæ–½åœ¨åŸå§‹æ–‡æœ¬ä¸­æŸ¥æ‰¾æ®µè½ï¼Œå¦‚æœ ReadAgent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">éœ€è¦æé†’è‡ªå·±ç›¸å…³çš„ç»†èŠ‚ä»¥å®Œæˆä»»åŠ¡ã€‚æˆ‘ä»¬ä½¿ç”¨æ£€ç´¢æ–¹æ³•ã€ä½¿ç”¨åŸå§‹çš„é•¿èƒŒæ™¯å’Œä½¿ç”¨æ‘˜è¦è®°å¿†ï¼Œé’ˆå¯¹åŸºå‡†è¯„ä¼°äº† </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ReadAgentã€‚åœ¨ä¸‰ä¸ªé•¿æ–‡æ¡£é˜…è¯»ç†è§£ä»»åŠ¡ï¼šQUALITYã€NarrativeQA å’Œ QMSum ä¸Šæ‰§è¡Œäº†è¿™äº›è¯„ä¼°ã€‚ReadAgent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">åœ¨æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ä¸Šéƒ½ä¼˜äºåŸºå‡†ï¼ŒåŒæ—¶å°†æœ‰æ•ˆçš„èƒŒæ™¯çª—å£æ‰©å¤§äº† 3-20 å€ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"A Human-Inspired Reading Agent with Gist\\\\nMemory of Very Long Contexts\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Current Large Language Models (LLMs) are not only limited to some maximum context </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">against baselines using retrieval methods, using the original long contexts, and using the gist memories. These </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">evaluations are performed on three long-document reading comprehension tasks: QUALITY, NarrativeQA, and QMSum. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3 â€” </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">20x.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"BitDeltaï¼šä½ çš„å¾®è°ƒå¯èƒ½åªéœ€è¦ 1 æ¯”ç‰¹\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">é€šå¸¸åˆ†ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒï¼šåœ¨å¤§è§„æ¨¡äº’è”ç½‘æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥åŠé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚é‰´äºé¢„è®­ç»ƒçš„è®¡ç®—éœ€æ±‚è¾ƒé«˜ï¼Œç›´è§‚åœ°è®¤</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä¸ºå¾®è°ƒä¼šç»™æ¨¡å‹æ·»åŠ çš„ä¿¡æ¯æ›´å°‘ï¼Œå› æ­¤æ›´æ˜“äºå‹ç¼©ã€‚æˆ‘ä»¬é€šè¿‡å°†å¾®è°ƒæ¨¡å‹çš„æƒé‡åˆ†è§£ä¸ºå…¶é¢„è®­ç»ƒéƒ¨åˆ†å’Œä¸€ä¸ªé™„åŠ çš„ delta </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ¥æ¢ç´¢è¿™ä¸€å‡è®¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼ŒBitDeltaï¼Œè¯¥æ–¹æ³•æˆåŠŸåœ°å°†è¿™ä¸ª delta é‡åŒ–ä¸º 1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ¯”ç‰¹ï¼Œè€Œä¸ä¼šæŸå®³æ€§èƒ½ã€‚è¿™ä¸€æœ‰è¶£çš„å‘ç°ä¸ä»…å‡¸æ˜¾äº†å¾®è°ƒè¿‡ç¨‹ä¸­æ·»åŠ çš„ä¿¡æ¯æ½œåœ¨å†—ä½™ï¼Œè€Œä¸”å¯¹å¾®è°ƒæ¨¡å‹çš„å¤šç§Ÿæˆ·æœåŠ¡å’Œå¤šç§Ÿæˆ·å­˜å‚¨</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å…·æœ‰é‡å¤§æ„ä¹‰ã€‚é€šè¿‡å¯ç”¨ä½¿ç”¨å•ä¸ªé«˜ç²¾åº¦åŸºç¡€æ¨¡å‹ä»¥åŠå¤šä¸ª 1 æ¯”ç‰¹çš„ deltaï¼ŒBitDelta å°† GPU å†…å­˜éœ€æ±‚å¤§å¹…å‡å°‘äº† 10 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å€ä»¥ä¸Šï¼Œè¿™ä¹Ÿå¯ä»¥è½¬åŒ–ä¸ºå¤šç§Ÿæˆ·è®¾ç½®ä¸­å¢å¼ºçš„ç”Ÿæˆå»¶è¿Ÿã€‚æˆ‘ä»¬é€šè¿‡è·¨è¶Š Llama-2 å’Œ Mistral æ¨¡å‹ç³»åˆ—ä»¥åŠé«˜è¾¾ 70B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å‚æ•°çš„æ¨¡å‹è¿›è¡Œå®éªŒå¯¹ BitDelta è¿›è¡Œäº†éªŒè¯ï¼Œå±•ç¤ºäº†åœ¨æ‰€æœ‰æµ‹è¯•è®¾ç½®ä¸‹æœ€å°çš„æ€§èƒ½ä¸‹é™ã€‚ä»£ç å¯åœ¨ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/bitdeltalä¸Šè·å¾—ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"BitDelta: Your Fine-Tune May Only Be Worth One Bit\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Large Language Models (LLMs) are typically trained in two phases: pre-training on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training, itâ€™s intuitive to assume that fine-tuning adds less new information to the model, and is thus more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">down to 1 bit without compromising performance. This interesting finding not only highlights the potential </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance degradation over all tested settings. Code is available at https://github.com/bitdeltal.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"OpenMathInstruct-1ï¼šä¸€ä¸ªå…·æœ‰180ä¸‡æ•°å­¦æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆæˆçš„ç”Ÿæˆæ•°æ®é›†åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è·å–ç›®æ ‡æŠ€èƒ½æ–¹é¢ã€‚å½“å‰å¤§è§„æ¨¡çš„</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ•°å­¦æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œå¦‚MetaMathQAï¼ˆYuç­‰äººï¼Œ2024ï¼‰å’ŒMAmmoTHï¼ˆYueç­‰äººï¼Œ2024ï¼‰æ˜¯ä½¿ç”¨å…·æœ‰å•†ä¸šé™åˆ¶æ€§è®¸å¯è¯çš„é—­æºLLMçš„è¾“å‡º</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ„å»ºçš„ã€‚é™åˆ¶åœ¨è¿™äº›æ•°æ®ç”Ÿæˆç®¡é“ä¸­ä½¿ç”¨å¼€æºLLMçš„ä¸€ä¸ªå…³é”®åŸå› æ˜¯æœ€å¥½çš„é—­æºLLMï¼ˆå¦‚GPT-4ï¼‰å’Œæœ€å¥½çš„å¼€æºLLMä¹‹é—´çš„æ•°å­¦æŠ€èƒ½å·®</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">è·å¾ˆå¤§ã€‚åŸºäºå¼€æºLLMçš„æœ€æ–°è¿›å±•ã€æˆ‘ä»¬æå‡ºçš„æç¤ºæ–°é¢–æ€§ä»¥åŠä¸€äº›è›®åŠ›æ‰©å±•ï¼Œæˆ‘ä»¬æ„å»ºäº†OpenMathInstruct-1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«180</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä¸‡ä¸ªé—®é¢˜-è§£å†³æ–¹æ¡ˆå¯¹çš„æ•°å­¦æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ˜¯ä½¿ç”¨æœ€è¿‘å‘å¸ƒçš„ã€è·å¾—è®¸å¯çš„Mixtralæ¨¡å‹ï¼Œé€šè¿‡ç»¼åˆGSM8Kå’ŒMATHï¼ˆä¸¤ä¸ª</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æµè¡Œçš„æ•°å­¦æ¨ç†åŸºå‡†ï¼‰çš„ä»£ç è§£é‡Šå™¨è§£å†³æ–¹æ¡ˆæ„å»ºçš„ã€‚æˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹OpenMath-CodeLlama-70Bï¼Œåœ¨OpenMathInstruct-1çš„ä¸€ä¸ªå­</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">é›†ä¸Šè®­ç»ƒï¼Œåœ¨GSM8Kä¸Šè·å¾—äº†84.6%çš„åˆ†æ•°ï¼Œåœ¨MATHä¸Šè·å¾—äº†50.7%çš„åˆ†æ•°ï¼Œè¿™ä¸æœ€å¥½çš„gptè’¸é¦æ¨¡å‹æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬åœ¨å•†ä¸šä¸Šå®½æ¾çš„</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">è®¸å¯è¯ä¸‹å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’ŒOpenMathInstruct-1æ•°æ®é›†ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"The huge development and inference costs associated with general-purpose large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language models (LLMs) have led to the rise of smaller, task-specific LLMs. Recent work has proposed creating these</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">domain/task-specific LLMs by generating high-quality synthetic data using powerful closed-source models such as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">GPT-3.5/4 (OpenAI et al., 2023) and training smaller models on the generated distillation data (Eldan and Li, 2023;</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Gunasekar et al., 2023; Li et al., 2023). For mathematical reasoning, our task of interest, all the current </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">state-of-the-art open-source models are gpt-distilled (Wang et al., 2024; Yue et al., 2024; Gou et al., 2024; Liao </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al., 2024). However, model development recipes relying on proprietary models like GPT-4 can have serious </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations: (a) legal restraints on how the finetuned models can be used,\\\\\" (b) generating data with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">closed-source models is typically costlier than state-of-the-art open-source models, and (c) these recipes lack </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reproducibility as closed-source model behaviors can vary significantly over time (Chen et al., 2023a). For </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">developing mathematical reasoning models, why are open-source models not used in place of closed-source models? To </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">answer this, we compare GPT-4 with Mixtral 8x7B model (Jiang et al., 2024), currently one of the best open-source </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLMs at mathematical reasoning, by generating code-interpreter\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è‡ªåšå¼ˆå¾®è°ƒæ‰©æ•£æ¨¡å‹\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"æˆ‘ä»¬ä»‹ç»äº† SPIN-Diffusionï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„è‡ªåšå¼ˆå¾®è°ƒç®—æ³•ã€‚è¯¥ç»“æœæ˜¯ä» Stable</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diffusion v1.5 å¯¹ Pick-a-Pic æ•°æ®é›†çš„è·èƒœå›¾ç‰‡è¿›è¡Œå¾®è°ƒå¾—åˆ°çš„ã€‚ç”¨äºç”Ÿæˆä»¥ä¸Šå›¾åƒçš„æç¤ºæ˜¯ä» Pick-a-Pic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æµ‹è¯•é›†ä¸­é€‰å–çš„ã€‚ç”Ÿæˆçš„å›¾åƒåœ¨æ•´ä½“è§†è§‰å¸å¼•åŠ›å’Œä¸æç¤ºçš„ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚SPIN-Diffusion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">çš„ç‰¹ç‚¹æ˜¯å®ƒä¸ä¾èµ–äºæˆå¯¹çš„äººç±»åå¥½æ•°æ®ï¼Œä¸ºåœ¨ä»…æä¾›æ¯ä¸ªæ–‡æœ¬æç¤ºä¸€å¼ å›¾ç‰‡çš„è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæä¾›äº†æœ‰ç”¨çš„å·¥å…·ã€‚\",\\n'</span>\n",
       ",\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"We introduce SPIN-Diffusion, a self-play fine-tuning algorithm for diffusion </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models. The results are fine-tuned from Stable Diffusion v1.5 on the winner images of the Pick-a-Pic dataset. The </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompts used for generating the above images are chosen from the Pick-a-Pic test set. The generated images </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">demonstrate superior performance in terms of overall visual attractiveness and coherence with the prompts. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">SPIN-Diffusion is featured by its independence from paired human preference data, offering a useful tool for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning on custom datasets with only single image per text prompt provided.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"å¯å­¦ä¹ æ ¸å‡½æ•°çš„çº¿æ€§å˜æ¢å™¨æ˜¯æ›´å¥½çš„ä¸Šä¸‹æ–‡æ¨¡å‹\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å¿«é€Ÿå‘å±•çš„é¢†åŸŸå†…ï¼Œæ¨è¿›è¯­è¨€æ¨¡å‹ (LM) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">çš„äºšäºŒæ¬¡æ¶æ„æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„å·¥ä½œã€‚åŒ…æ‹¬çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨å†…çš„å½“å‰åˆ›æ–°æœ€åˆå› å…¶åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­è¶…è¶Šäº† Transformer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ€§èƒ½è€Œå¹¿å—èµèª‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¿…è¦çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ–¹é¢æš´éœ²å‡ºç¼ºé™·ï¼Œè¿™æ˜¯ Transformer ä¼ ç»Ÿä¸Šè¡¨ç°å‡ºè‰²çš„é¢†åŸŸã€‚Based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ¨¡å‹ä½œä¸ºä¸€ç§æ··åˆè§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œå®ƒå°†çº¿æ€§å˜æ¢å™¨ä¸å—æŒ‡æ•°å‡½æ•°çš„æ³°å‹’å±•å¼€æ‰€å¯å‘çš„æ ¸å‡½æ•°ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡å·ç§¯ç½‘ç»œè¿›è¡Œäº†æ‰©</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å±•ã€‚å®ƒå¤åˆ¶äº† Transformer åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç†Ÿç»ƒåº¦ï¼Œæˆä¸ºè¯¥é¢†åŸŸçš„æœ‰åŠ›ç«äº‰è€…ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹ Based </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ ¸å‡½æ•°æå‡ºäº†ä¸€ç§ç‹¬ç‰¹è€Œä¼˜é›…çš„ä¿®æ”¹ï¼Œé€šè¿‡å¤šæŸ¥è¯¢å…³è”å¬å›ä»»åŠ¡å’Œåœ¨ Pile </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ•°æ®é›†ä¸Šå±•ç¤ºçš„æ•´ä½“è¯­è¨€å»ºæ¨¡è¿‡ç¨‹è¯„ä¼°ï¼Œæ”¾å¤§äº†å…¶ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Linear Transformers with Learnable Kernel Functions are Better In-Context </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Models\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Advancing the frontier of subquadratic architectures for Language Models (LMs) is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models have revealed deficiencies in essential In-Context Learning capabilities â€” a domain where the Transformer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Transformerâ€™s in-context adeptness, it became a strong contender in the field. In our work, we present a singular, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dataset.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"åœ¨ä¸€åƒå…†å­—èŠ‚çš„å¹²è‰å †ä¸­å¯»æ‰¾é’ˆå¤´ï¼šå¾ªç¯è®°å¿†å‘ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹é”™å¤±çš„ä¸œè¥¿\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"æœ¬æ–‡è®¨è®ºäº†ä½¿ç”¨ç”Ÿæˆå¼ Transformer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ¨¡å‹å¤„ç†é•¿æ–‡æ¡£çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è¯„ä¼°ä¸åŒçš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº† </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">BABILongï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨æå–å’Œå¤„ç†å†—é•¿æ–‡æœ¬ä¸­çš„åˆ†å¸ƒå¼äº‹å®æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬ GPT-4 å’Œ RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">çš„åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜å¸¸è§çš„æ–¹æ³•ä»…å¯¹é«˜è¾¾ 10^4 ä¸ªå…ƒç´ çš„åºåˆ—æœ‰æ•ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”¨å¾ªç¯è®°å¿†å¢å¼ºå¯¹ GPT-2 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">çš„å¾®è°ƒä½¿å…¶èƒ½å¤Ÿå¤„ç†æ¶‰åŠå¤šè¾¾ 10^7 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä¸ªå…ƒç´ çš„ä»»åŠ¡ã€‚è¿™ä¸€æˆå°±æ˜¯ä¸€é¡¹é‡å¤§é£è·ƒï¼Œå› ä¸ºè¿™æ˜¯è¿„ä»Šä¸ºæ­¢ä»»ä½•å¼€æ”¾ç¥ç»ç½‘ç»œæ¨¡å‹å¤„ç†çš„æœ€é•¿è¾“å…¥ï¼Œå±•ç¤ºäº†å¤„ç†é•¿åºåˆ—èƒ½åŠ›çš„æ˜¾</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">è‘—æé«˜ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"This paper tackles the challenge of processing lengthy documents using generative </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to 10^4 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">up to 10^7 elements. This achievement marks a substantial leap, as it is by far the longest input processed by any </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">open neural network model to date, demonstrating a significant improvement in the processing capabilities for long </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequences.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"DataDreamerï¼šåˆæˆæ•°æ®ç”Ÿæˆå’Œå¯å¤åˆ¶ LLM å·¥ä½œæµå·¥å…·\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) å·²æˆä¸º NLP </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ç ”ç©¶äººå‘˜åœ¨å„ç§ä»»åŠ¡ä¸­çš„ä¸€ä¸ªä¸»è¦å·¥å…·ã€‚å¦‚ä»Šï¼Œå¾ˆå¤šç ”ç©¶äººå‘˜åœ¨åˆæˆæ•°æ®ç”Ÿæˆã€ä»»åŠ¡è¯„ä¼°ã€æ¨¡å‹å¾®è°ƒã€è’¸é¦å’Œå…¶ä»–æ¨¡å‹å¾ªç¯ç ”ç©¶å·¥</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä½œæµä¸­ä½¿ç”¨ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLMã€‚ä½†æ˜¯ï¼Œåœ¨ä½¿ç”¨è¿™äº›æ¨¡å‹æ—¶ä¼šäº§ç”Ÿä¸€äº›æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜æºäºå…¶è§„æ¨¡ã€å°é—­æºä»£ç çš„æœ¬è´¨ä»¥åŠè¿™äº›æ–°å…´å·¥ä½œæµç¼ºä¹æ ‡å‡†åŒ–å·¥å…·ã€‚è¿™</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">äº›æ¨¡å‹çš„å¿«é€Ÿå‘å±•å’Œè¿™äº›ç‹¬æœ‰çš„æŒ‘æˆ˜å¯¹å¼€æ”¾ç§‘å­¦å’Œä½¿ç”¨è¿™äº›æ¨¡å‹çš„å·¥ä½œçš„å¯å¤åˆ¶æ€§äº§ç”Ÿäº†ç›´æ¥çš„ä¸åˆ©å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">DataDreamerï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æº Python åº“ï¼Œå…è®¸ç ”ç©¶äººå‘˜ç¼–å†™ç®€å•çš„ä»£ç æ¥å®ç°å¼ºå¤§çš„ LLM å·¥ä½œæµã€‚DataDreamer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">è¿˜å¸®åŠ©ç ”ç©¶äººå‘˜éµå®ˆæˆ‘ä»¬æè®®çš„æœ€ä½³å®è·µï¼Œä»¥ä¿ƒè¿›å¼€æ”¾ç§‘å­¦å’Œå¯å¤åˆ¶æ€§ã€‚è¯¥åº“å’Œæ–‡æ¡£å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å¾—ï¼šhttps://github.com/da</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tadreamer-dev/DataDreamerã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Workflows\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Large language models (LLMs) have become a dominant and important tool for NLP </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">when using these models that stem from their scale, their closed source nature, and the lack of standardized </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">encourage open science and reproducibility. The library and documentation are available at: </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/datadreamer-dev/DataDreamer.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"GaussianObjectï¼šåªç”¨å››å¼ å›¾åƒè·å¾—å¸¦æœ‰é«˜æ–¯æ³¼æº…çš„é«˜è´¨é‡3Då¯¹è±¡\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"é‡å»ºå’Œæ¸²æŸ“é«˜åº¦ç¨€ç–è§†å›¾ä¸­çš„3Då¯¹è±¡å¯¹æ¨å¹¿3Dè§†è§‰æŠ€æœ¯çš„åº”ç”¨å’Œæ”¹å–„ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç¨€ç–è§†å›¾çš„å›¾åƒåªåŒ…å«éå¸¸æœ‰é™çš„</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">3Dä¿¡æ¯ï¼Œè¿™å¸¦æ¥äº†ä¸¤ä¸ªé‡å¤§æŒ‘æˆ˜ï¼š1ï¼‰éš¾ä»¥å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå› ä¸ºåŒ¹é…çš„å›¾åƒå¤ªå°‘ï¼›2ï¼‰ç”±äºè§†å›¾è¦†ç›–ä¸è¶³ï¼Œå¯¹è±¡ä¿¡æ¯éƒ¨åˆ†çœç•¥æˆ–</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">é«˜åº¦å‹ç¼©ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GaussianObjectï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨é«˜æ–¯æ³¼æº…æ¥è¡¨ç¤ºå’Œæ¸²æŸ“3Då¯¹è±¡çš„æ¡†æ¶ï¼Œå®ƒä»…ä½¿ç”¨4ä¸ªè¾“å…¥</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å›¾åƒå°±èƒ½å®ç°é«˜æ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»äº†è§†è§‰åŒ…ç»œå’Œæµ®åŠ¨æ¶ˆé™¤æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å°†ç»“æ„å…ˆå…¥ä¸ºä¸»åœ°æ³¨å…¥åˆ°åˆå§‹ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œä»¥å¸®åŠ©</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªç²—ç•¥çš„3Dé«˜æ–¯è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬åŸºäºæ‰©æ•£æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªé«˜æ–¯ä¿®å¤æ¨¡å‹æ¥è¡¥å……è¢«çœç•¥çš„å¯¹è±¡ä¿¡æ¯</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ï¼Œå…¶ä¸­é«˜æ–¯ä½“è¿›ä¸€æ­¥å¾—åˆ°ç»†åŒ–ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªç”Ÿæˆç­–ç•¥æ¥è·å¾—ç”¨äºè®­ç»ƒä¿®å¤æ¨¡å‹çš„å›¾åƒå¯¹ã€‚æˆ‘ä»¬çš„GaussianObjectåœ¨å‡ ä¸ªå…·æœ‰</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬MipNeRF360ã€OmniObject3Då’ŒOpenll-luminationï¼Œä»…ä½¿ç”¨4ä¸ªè§†å›¾å°±è·å¾—äº†å¼ºå¤§çš„é‡å»ºç»“æœï¼Œ</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å¹¶ä¸”æ˜æ˜¾ä¼˜äºä»¥å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢https://gaussianobject.github.io/ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Gaussian Splatting\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Reconstructing and rendering 3D objects from 2D images has been a longstanding and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">important topic, which plays critical roles in a vast range of real-life applications. One key factor that impedes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">users, especially ones without expert knowledge, from widely using these techniques is that usually dozens of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multi-view images need to be captured, which is cumbersome and sometimes impractical. Efficiently reconstructing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">high-quality 3D objects from highly sparse captured images is of great value for expediting downstream applications</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">such as 3D asset creation for game/movie production and AR/VR products.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"LLM Comparatorï¼šå¯è§†åŒ–åˆ†æï¼Œè¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¶æ’è¯„ä¼°\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"è¯¥ PDF æ–‡æ¡£ä»‹ç»äº† LLM Comparatorï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¯è§†åŒ–åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">è‡ªåŠ¨å¹¶æ’è¯„ä¼°ç»“æœçš„äº¤äº’å¼å·¥å…·ã€‚è¯¥å·¥å…·æ”¯æŒæ¨¡å‹å¼€å‘è€…å’Œç ”ç©¶äººå‘˜æ¯”è¾ƒä¸¤ä¸ª LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">çš„å“åº”è´¨é‡ï¼Œè¯†åˆ«å‡ºä½•æ—¶ä»¥åŠä¸ºä½•ä¸€ä¸ªæ¨¡å‹çš„æ€§èƒ½ä¼˜äºæˆ–åŠ£äºå¦ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶æ¢ç´¢ä¸¤ä¸ªæ¨¡å‹å“åº”ä¹‹é—´çš„å·®å¼‚ã€‚LLM Comparator </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æä¾›äº†ä¸€ä¸ªäº¤äº’å¼è¡¨æ ¼ï¼Œç”¨äºæ£€æŸ¥å•ä¸ªæç¤ºåŠå…¶å“åº”ï¼Œä»¥åŠä¸€ä¸ªå¯è§†åŒ–æ‘˜è¦ï¼Œæ”¯æŒå¸®åŠ©ç†è§£æ¨¡å‹å·®å¼‚çš„åˆ†æå·¥ä½œæµã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Models\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"This PDF document presents LLM Comparator, an interactive tool for visual </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">analytics of automatic side-by-side evaluation results of large language models (LLMs). It enables model developers</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and researchers to compare the quality of responses from two LLMs, identify when and why one model performs better </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or worse than the other, and explore the differences in their responses. LLM Comparator provides an interactive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">table for inspecting individual prompts and their responses, and a visualization summary that supports analytical </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">workflows for understanding model differences.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"LAVEï¼šé’ˆå¯¹è§†é¢‘ç¼–è¾‘çš„ LLM åŠ©åŠ›ä»£ç†ä¸è‡ªç„¶è¯­è¨€å¢å¼º\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": \"LAVE æ˜¯ä¸€æ¬¾è§†é¢‘ç¼–è¾‘å·¥å…·ï¼Œå®ƒæä¾›äº† LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">åŠ©åŠ›ä»£ç†ä¸è‡ªç„¶è¯­è¨€å¢å¼ºåŠŸèƒ½ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æ›´è½»æ¾åœ°ç¼–è¾‘è§†é¢‘ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ–‡å­—ä¸ LAVE </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">çš„è§†é¢‘ç¼–è¾‘ä»£ç†äº’åŠ¨ï¼Œåœ¨æ•´ä¸ªç¼–è¾‘è¿‡ç¨‹ä¸­è·å¾—å®æ—¶å¸®åŠ©ã€‚æ­¤å¤–ï¼ŒLAVE </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">è¿˜æä¾›ä¸€ç³»åˆ—è¯­è¨€å¢å¼ºåŠŸèƒ½ï¼Œä¾‹å¦‚è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ ‡é¢˜ã€æ˜¾ç¤ºè§†é¢‘æ‘˜è¦å¸®åŠ©ç”¨æˆ·å¿«é€Ÿäº†è§£è§†é¢‘å†…å®¹ã€åœ¨ç¼©ç•¥å›¾åº“ä¸­é€šè¿‡ç‚¹å‡»è§†é¢‘å³å¯</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å°†å…¶æ·»åŠ åˆ°ç¼–è¾‘æ—¶é—´è½´ç­‰ï¼Œè¿™äº›åŠŸèƒ½å¯ä»¥æå¤§åœ°ç®€åŒ–å’ŒåŠ é€Ÿè§†é¢‘ç¼–è¾‘æµç¨‹ï¼Œé™ä½æ–°æ‰‹å…¥é—¨é—¨æ§›ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"The Increasing demand for video content creation has been hindered by the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">technical expertise and effort typically required for video editing. This paper presents an integration of large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language models (LLMs) into the video editing workflow to lower these barriers, specifically focusing on assistance</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and language augmentation. Our proposed system, LAVE, offers an agent that interacts with users using natural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language, providing assistance with various editing tasks. It also incorporates language augmentation into the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">editing workflow, with features such as automated video summarization, video title generation, and quick video </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">selection from a gallery. We believe that LAVE simplifies the video editing process, making it more accessible to a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">wider range of users.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"åŸºäºå‡½æ•°è°ƒç”¨çš„é›¶æ ·æœ¬å¯¹è¯çŠ¶æ€è¿½è¸ªå™¨ä¸­çš„å¤§è¯­è¨€æ¨¡å‹\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶åœ¨ä¸€èˆ¬è¯­å¢ƒä¸‹çš„é«˜çº§ç†è§£å’Œç”Ÿæˆèƒ½åŠ›è€Œåœ¨ä¼šè¯ç³»ç»Ÿä¸­æ—¥ç›Šæ™®åŠã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä»»åŠ¡å¯¼å‘å¯¹è¯ï¼ˆTODï¼‰ä¸­çš„æœ‰</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ•ˆæ€§ä»ç„¶ä¸å°½å¦‚äººæ„ï¼Œè¿™ä¸ä»…éœ€è¦ç”Ÿæˆå“åº”ï¼Œè¿˜éœ€è¦åœ¨ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸå†…è¿›è¡Œæœ‰æ•ˆçš„å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å‡ºäº†ä¸€ç§é€šè¿‡å‡½æ•°è°ƒç”¨ä½¿ç”¨ LLM è§£å†³ DST çš„æ–°æ–¹æ³• FNCTODã€‚è¯¥æ–¹æ³•æ”¹è¿›äº†é›¶æ ·æœ¬ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">DSTï¼Œå…è®¸é€‚åº”ä¸åŒçš„é¢†åŸŸï¼Œè€Œæ— éœ€å¤§é‡æ•°æ®æ”¶é›†æˆ–æ¨¡å‹è°ƒæ•´ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä¸­ç­‰è§„æ¨¡çš„å¼€æºå’Œä¸“æœ‰ LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">éƒ½å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼šåœ¨ä¸Šä¸‹æ–‡ä¸­æç¤ºä¸‹ï¼Œå®ƒä½¿å„ç§ 7B æˆ– 13B å‚æ•°æ¨¡å‹èƒ½å¤Ÿè¶…è¶Š ChatGPT </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å®ç°çš„å…ˆå‰æœ€å…ˆè¿›ï¼ˆSOTAï¼‰æŠ€æœ¯ï¼Œå¹¶å°† ChatGPT çš„æ€§èƒ½æé«˜äº† 5.6% çš„å¹³å‡ JGAã€‚GPT-3.5 å’Œ GPT-4 çš„å•ä¸ªæ¨¡å‹ç»“æœåˆ†åˆ«æé«˜äº† </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">4.8% å’Œ 14%ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œé€šè¿‡é’ˆå¯¹å°è§„æ¨¡çš„ä¸åŒä»»åŠ¡å¯¼å‘å¯¹è¯è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºä¸­ç­‰è§„æ¨¡çš„æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ 13B å‚æ•° </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLaMA2-Chat æ¨¡å‹ï¼‰é…å¤‡å‡½æ•°è°ƒç”¨èƒ½åŠ›å’Œä¸ ChatGPT ç›¸åª²ç¾çš„ DST </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå…¶èŠå¤©èƒ½åŠ›ã€‚æˆ‘ä»¬å°†å¼€æºå®éªŒä»£ç å’Œæ¨¡å‹ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Large Language Models as Zero-shot Dialogue State Tracker through Function </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Calling\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"Large language models (LLMs) are increasingly prevalent in conversational systems </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approach FNCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">adaptation to diverse domains without extensive data collection or model tuning. Our experimental results </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPTâ€™s performance beating the SOTA by 5.6% Avg. JGA. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to ChatGPT while maintaining their chat capabilities. We will open-source experimental code and model.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">' \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"chinese_title\": \"æ„å»ºå»‰ä»·ç¼©æ”¾ï¼šä¸€ç§ç”¨äºé«˜åˆ†è¾¨ç‡é€‚åº”çš„è‡ªçº§è”æ‰©æ•£æ¨¡å‹\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­å·²è¢«è¯æ˜éå¸¸æœ‰æ•ˆï¼›ç„¶è€Œï¼Œç”±äºå•å°ºåº¦è®­ç»ƒæ•°æ®ï¼Œå®ƒä»¬åœ¨ç”Ÿæˆä¸åŒå¤§å°çš„å›¾åƒæ—¶ä»ç„¶é¢ä¸´æ„å›¾æŒ‘æˆ˜ã€‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">é’ˆå¯¹é«˜åˆ†è¾¨ç‡è°ƒæ•´å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—å’Œä¼˜åŒ–èµ„æºï¼Œä½†ä»æ— æ³•å®ç°ä¸ä½åˆ†è¾¨ç‡æ¨¡å‹ç›¸å½“çš„ç”Ÿæˆèƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ç§æ–°é¢–çš„è‡ªçº§è”æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ä»ç»è¿‡è‰¯å¥½è®­ç»ƒçš„ä½åˆ†è¾¨ç‡æ¨¡å‹ä¸­è·å¾—çš„ä¸°å¯ŒçŸ¥è¯†ï¼Œå¿«é€Ÿé€‚åº”é«˜åˆ†è¾¨ç‡å›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œé‡‡</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ç”¨æ— è°ƒä¼˜æˆ–å»‰ä»·ä¸Šé‡‡æ ·å™¨è°ƒä¼˜èŒƒä¾‹ã€‚è‡ªçº§è”æ‰©æ•£æ¨¡å‹é›†æˆäº†åºåˆ—å¤šå°ºåº¦ä¸Šé‡‡æ ·å™¨æ¨¡å—ï¼Œèƒ½æœ‰æ•ˆåœ°é€‚åº”æ›´é«˜çš„åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¿ç•™äº†åŸ</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å§‹æ„å›¾å’Œç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ¢è½´å¼•å¯¼å™ªå£°é‡æ–°è°ƒåº¦ç­–ç•¥ï¼Œä»¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹å¹¶æ”¹å–„å±€éƒ¨ç»“æ„ç»†èŠ‚ã€‚ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼Œ</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æˆ‘ä»¬çš„æ–¹æ³•å°†è®­ç»ƒé€Ÿåº¦æé«˜äº† 5 å€ï¼Œå¹¶ä¸”ä»…éœ€è¦é¢å¤–çš„ 0.002M è°ƒä¼˜å‚æ•°ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é€šè¿‡ä»…å¾®è°ƒ 10k </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ­¥ï¼Œåœ¨å‡ ä¹æ²¡æœ‰é¢å¤–æ¨ç†æ—¶é—´çš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿé€‚åº”æ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒå’Œè§†é¢‘åˆæˆã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/GuoLanging/Self-Cascade/ å‘å¸ƒã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"english_title\": \"Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adaptation\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"english_summary\": \"Diffusion models have proven to be highly effective in image and video generation; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">however, they still face composition challenges when generating images of varying sizes due to single-scale </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">training data. Adapting large pre-trained diffusion models for higher resolution demands substantial computational </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and optimization resources, yet achieving a generation capability comparable to low-resolution models remains </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">elusive. This paper proposes a novel self-cascade diffusion model that leverages the rich knowledge gained from a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">well-trained low-resolution model for rapid adaptation to higher-resolution image and video generation, employing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">either tuning-free or cheap upsampler tuning paradigms. Integrating a sequence of multi-scale upsampler modules, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the self-cascade diffusion model can efficiently adapt to a higher resolution, preserving the original composition </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and generation capabilities. We further propose a pivot-guided noise re-schedule strategy to speed up the inference</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">process and improve local structural details. Compared to full fine-tuning, our approach achieves a 5x training </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">speed-up and requires only an additional 0.002M tuning parameters. Extensive experiments demonstrate that our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">approach can quickly adapt to higher resolution image and video synthesis by fine-tuning for just 10k steps, with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">virtually no additional inference time. Our code will be released at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://github.com/GuoLanging/Self-Cascade/.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">' }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"é€šç”¨æ“ä½œç•Œé¢ï¼šåœ¨ä¸ä½¿ç”¨é‡å¤–æœºå™¨äººçš„æƒ…å†µä¸‹ï¼Œåœ¨é‡å¤–å¯¹æœºå™¨è¿›è¡Œæ•™å­¦\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"æˆ‘ä»¬æå‡ºäº†é€šç”¨æ“ä½œç•Œé¢ï¼ˆUMIï¼‰â€”â€”ä¸€ç§æ•°æ®æ”¶é›†å’Œç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸ç›´æ¥å°†é‡å¤–äººç±»æ¼”ç¤ºä¸­çš„æŠ€èƒ½è½¬ç§»åˆ°å¯éƒ¨ç½²æœºå™¨ç­–</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ç•¥ä¸­ã€‚UMI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">é‡‡ç”¨æ‰‹æŒæŠ“æ‰‹ç»“åˆè°¨æ…çš„ç•Œé¢è®¾è®¡ï¼Œå¯å®ç°ä¾¿æºã€ä½æˆæœ¬ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ•°æ®æ”¶é›†ï¼Œä»¥è¿›è¡Œæå…·æŒ‘æˆ˜æ€§çš„åŒæ‰‹åŠ¨å’ŒåŠ¨æ€æ“ä½œæ¼”ç¤ºã€‚ä¸º</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä¿ƒè¿›å¯éƒ¨ç½²ç­–ç•¥å­¦ä¹ ï¼ŒUMI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">åŒ…å«ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„ç­–ç•¥ç•Œé¢ï¼Œè¯¥ç•Œé¢å…·å¤‡æ¨ç†æ—¶å»¶è¿ŸåŒ¹é…å’Œç›¸å¯¹è½¨è¿¹åŠ¨ä½œè¡¨ç¤ºã€‚æ‰€å­¦ç­–ç•¥ä¸ç¡¬ä»¶æ— å…³ï¼Œä¸”å¯åœ¨å¤šç§æœºå™¨äººå¹³å°ä¸Š</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">éƒ¨ç½²ã€‚é…å¤‡è¿™äº›åŠŸèƒ½ï¼ŒUMI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ¡†æ¶è§£é”äº†æ–°çš„æœºå™¨äººæ“ä½œèƒ½åŠ›ï¼Œä»…é€šè¿‡é’ˆå¯¹æ¯ä¸ªä»»åŠ¡æ›´æ”¹è®­ç»ƒæ•°æ®ï¼Œå³å¯å®ç°é›¶æ¬¡æ¦‚æ‹¬çš„åŠ¨æ€ã€åŒæ‰‹åŠ¨ã€ç²¾ç¡®å®šä½å’Œé•¿æ—¶é—´è¡Œä¸º</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„çœŸå®å®éªŒå±•ç¤ºäº† UMI çš„å¤šåŠŸèƒ½æ€§å’Œæœ‰æ•ˆæ€§ï¼Œåœ¨ UMI </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä¸Šå­¦ä¹ çš„ç­–ç•¥ï¼Œåœ¨é’ˆå¯¹å„ç§äººç±»æ¼”ç¤ºè¿›è¡Œè®­ç»ƒåï¼Œå¯é›¶æ¬¡æ¦‚æ‹¬åˆ°æ–°ç¯å¢ƒå’Œç‰©ä½“ã€‚UMI çš„ç¡¬ä»¶å’Œè½¯ä»¶ç³»ç»Ÿå¯åœ¨ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://umi-gripper.github.io è·å–å¼€æºã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Robots\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"We present Universal Manipulation Interface (UMI)â€”a data collection and policy </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">learning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">policies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">deployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">matching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">deployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">manipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">by only changing the training data for each task. We demonstrate UMIâ€™s versatility and efficacy with comprehensive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">real-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">trained on diverse human demonstrations. UMIâ€™s hardware and software system is open-sourced at </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://umi-gripper.github.io.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">' \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"chinese_title\": \"SPARï¼šé€šè¿‡é•¿æœŸå‚ä¸æ³¨æ„åŠ›å®ç°ä¸ªæ€§åŒ–çš„åŸºäºå†…å®¹çš„æ¨è\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"chinese_summary\": \"åˆ©ç”¨ç”¨æˆ·çš„é•¿æœŸå‚ä¸å†å²å¯¹äºä¸ªæ€§åŒ–çš„å†…å®¹æ¨èè‡³å…³é‡è¦ã€‚é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (PLM) åœ¨ NLP </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä¸­çš„æˆåŠŸä½¿å…¶è¢«ç”¨äºå¯¹ç”¨æˆ·å†å²å’Œå€™é€‰é¡¹ç›®è¿›è¡Œç¼–ç ï¼Œå¹¶å°†å†…å®¹æ¨èæ„å»ºä¸ºæ–‡æœ¬è¯­ä¹‰åŒ¹é…ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œåœ¨å¤„ç†è¶…é•¿çš„ç”¨æˆ·</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å†å²æ–‡æœ¬å’Œä¸è¶³çš„ç”¨æˆ·ä¸é¡¹ç›®çš„äº¤äº’æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå†…å®¹çš„æ¨èæ¡†æ¶ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">SPARï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°è§£å†³äº†ä»é•¿æœŸçš„ç”¨æˆ·å‚ä¸å†å²ä¸­æå–æ•´ä½“ç”¨æˆ·å…´è¶£çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡åˆ©ç”¨ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">PLMã€å¤šæ³¨æ„åŠ›å±‚å’Œæ³¨æ„åŠ›ç¨€ç–æœºåˆ¶ä»¥åŸºäºä¼šè¯çš„æ–¹å¼å¯¹ç”¨æˆ·å†å²è¿›è¡Œç¼–ç æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç”¨æˆ·ç«¯å’Œé¡¹ç›®ç«¯ç‰¹å¾å……åˆ†èåˆç”¨äºå‚ä¸åº¦</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">é¢„æµ‹ï¼ŒåŒæ—¶ä¿æŒä¸¤ç«¯çš„ç‹¬ç«‹è¡¨ç¤ºï¼Œè¿™å¯¹äºå®é™…æ¨¡å‹éƒ¨ç½²æ˜¯æœ‰æ•ˆçš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä»ç”¨æˆ·å‚ä¸å†å²ä¸­æå–å…¨å±€å…´è¶£æ¥å¢å¼ºç”¨æˆ·ç”»åƒã€‚åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿› (SoTA) </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ–¹æ³•ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"english_title\": \"SPAR: Personalized Content-Based Recommendation via Long Engagement Attention\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"english_summary\": \"Leveraging usersâ€™ long engagement histories is essential for personalized content </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">recommendations. The success of pretrained language models (PLMs) in NLP has led to their use in encoding user </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">histories and candidate items, framing content recommendations as textual semantic matching tasks. However, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">existing works still struggle with processing very long user historical text and insufficient user-item </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interaction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the challenges of holistic user interest extraction from the long user engagement history. It achieves so by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">leveraging PLM, poly-attention layers and attention sparsity mechanisms to encode userâ€™s history in a session-based</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">manner. The user and item side features are sufficiently fused for engagement prediction while maintaining </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">standalone representations for both sides, which is efficient for practical model deployment. Moreover, we enhance </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">user profiling by exploiting large language model (LLM) to extract global interests from user engagement history. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Extensive experiments on two benchmark datasets demonstrate that our framework outperforms existing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">state-of-the-art (SoTA) methods.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">' }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"PaLM2-VAdapterï¼šæ¸è¿›å¼å¯¹é½è¯­è¨€æ¨¡å‹æˆä¸ºå¼ºå¤§çš„è§†è§‰è¯­è¨€é€‚é…å™¨\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"æœ¬æ–‡è¡¨æ˜ï¼Œæ¸è¿›å¼å¯¹é½çš„è¯­è¨€æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ¡¥æ¥å†»ç»“çš„è§†è§‰ç¼–ç å™¨å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è™½ç„¶è§†è§‰ç¼–ç å™¨å’Œ LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">çš„åŸºæœ¬æ¶æ„å’Œé¢„è®­ç»ƒæ–¹æ³•å·²ç»å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†è§†è§‰è¯­è¨€é€‚é…å™¨çš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥åœ¨æœ€è¿‘çš„ç ”ç©¶ä¸­å·®å¼‚å¾ˆå¤§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹æœ€å…ˆè¿›</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">çš„æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨æ¶æ„è¿›è¡Œäº†å½»åº•æ¢ç´¢ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå…·æœ‰æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨çš„è§†è§‰è¯­è¨€å¯¹é½è¡¨ç°</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å‡ºç¼“æ…¢çš„æ”¶æ•›æ€§å’Œæœ‰é™çš„å¯æ‰©å±•æ€§ï¼Œç¼ºä¹ç›´æ¥çš„ç›‘ç£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PaLM2-VAdapterï¼Œé‡‡ç”¨æ¸è¿›å¼å¯¹é½çš„è¯­è¨€æ¨¡</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å‹ä½œä¸ºè§†è§‰è¯­è¨€é€‚é…å™¨ã€‚ä¸å…·æœ‰æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨çš„å¼ºå¤§åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡­ç»éªŒè¯æ˜å…·æœ‰æ›´å¿«çš„æ”¶æ•›æ€§ã€æ›´é«˜çš„æ€§èƒ½å’Œæ›´å¼ºçš„</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å¯æ‰©å±•æ€§ã€‚è·¨è¶Šå›¾åƒå’Œè§†é¢‘çš„å„ç§è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œå­—å¹•ä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å±•ç¤ºäº†æœ€å…ˆè¿›çš„è§†è§‰ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å‚æ•°å°‘äº†30~70%ï¼Œè¿™æ ‡å¿—ç€æ•ˆç‡çš„æ˜¾ç€æé«˜ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Adapter\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"This paper demonstrates that a progressively aligned language model can </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">effectively bridge frozen vision encoders and large language models (LLMs). While the fundamental architecture and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training methods of vision encoders and LLMs have been extensively studied, the architecture and training </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">strategy of vision-language adapters vary significantly across recent works. Our research undertakes a thorough </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">exploration of the state-of-the-art perceiver resampler architecture and builds a strong baseline. However, we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">observe that the vision-language alignment with perceiver resampler exhibits slow convergence and limited </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scalability with a lack of direct supervision. To address this issue, we propose PaLM2-VAdapter, employing a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">progressively aligned language model as the vision-language adapter. Compared to the strong baseline with perceiver</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">resampler, our method empirically shows faster convergence, higher performance and stronger scalability. Extensive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments across various Visual Question Answering (VQA) and captioning tasks on both images and videos </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">demonstrate that our model exhibits state-of-the-art visual understanding and multi-modal reasoning capabilities. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Notably, our method achieves these advancements with 30~70% fewer parameters than the state-of-the-art large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vision-language models, marking a significant efficiency improvement.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'{\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  \"Summary\": {\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_title\": \"RLVFï¼šä»è¯­è¨€åé¦ˆä¸­å­¦ä¹ ï¼Œé¿å…è¿‡åº¦æ¦‚æ‹¬\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"chinese_summary\": </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªè¡Œä¸šå’Œä¸ªä½“ä¸­çš„å¹¿æ³›é‡‡ç”¨ï¼Œé’ˆå¯¹ç‰¹å®šç”¨æˆ·æˆ–ç”¨ä¾‹ï¼ŒæŒ‰ç…§é«˜çº§äººç±»åé¦ˆå¯¹å…¶è¿›è¡Œè°ƒæ•´çš„èƒ½åŠ›å˜å¾—è¶Š</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ¥è¶Šé‡è¦ã€‚è™½ç„¶ LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ç”¨æˆ·é€šå¸¸å¸Œæœ›æ¨¡å‹å§‹ç»ˆéµå¾ªå¹¿æ³›çš„åŸåˆ™ï¼Œæ¯”å¦‚ç”Ÿæˆæµç•…çš„æ–‡æœ¬ï¼Œä½†ä¸ªåˆ«ç”¨æˆ·å’Œç”¨ä¾‹æœ‰æ›´ç»†å¾®çš„åå¥½ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯èƒ½è¦æ±‚ LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å†™å‡ºæ›´ç®€æ´çš„å·¥ä½œç”µå­é‚®ä»¶ï¼Œä½†è¦å†™å‡ºæ›´è¯¦ç»†çš„ä¸ªäººç”µå­é‚®ä»¶ï¼Œè¿™ä½¿å¾—åé¦ˆå…·æœ‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚æ ¹æ®è¿™äº›åå¥½è°ƒæ•´æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ï¼šéœ€è¦å¤§é‡èµ„æºæ‰èƒ½æ”¶é›†æ‰€æœ‰ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„åå¥½ï¼Œå¹¶ä¸”åœ¨ä¸€ä¸ªä¸Šä¸‹æ–‡ä¸­çš„æ¨¡å‹å¾®è°ƒä¼šå¯¹å…¶ä»–ä¸Šä¸‹æ–‡ä¸­çš„æ¨¡å‹è¡Œä¸ºäº§ç”Ÿä¸å¯é¢„æµ‹çš„</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å½±å“ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨è¯­è¨€åé¦ˆæ¥è°ƒæ•´æ¨¡å‹çš„é—®é¢˜ï¼Œè¿™ç§åé¦ˆå¯¹äºäººä»¬æ¥è¯´å¿«é€Ÿä¸”å®¹æ˜“æä¾›ï¼ˆè§å›¾ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1ï¼‰ã€‚åˆå¹¶åé¦ˆçš„å¸¸è§æ–¹æ³•ï¼Œä¾‹å¦‚ç›‘ç£ä¸Šä¸‹æ–‡è’¸é¦ (SCD) æˆ–å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨äººç±»åé¦ˆ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(RLHF)ï¼Œä½¿ç”¨ç¤ºä¾‹çº§ç›‘ç®¡é€šè¿‡ç›‘ç£å®Œæˆæˆ–åå¥½æ ‡ç­¾ã€‚æ­¤ç±»æ–¹æ³•éœ€è¦å¤§é‡ç”¨æˆ·æä¾›çš„ï¼ˆåå¥½ï¼‰æ•°æ®è¯­æ–™åº“ï¼Œè€Œè·å–è¿™äº›æ•°æ®è¯­æ–™åº“å¯</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">èƒ½æ—¢æ˜‚è´µåˆç¹çã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¸ä¼šé™åˆ¶åé¦ˆå¯èƒ½é€‚ç”¨çš„ä¸Šä¸‹æ–‡ä¹‹å¤–çš„æ¨¡å‹è¡Œä¸ºï¼Œå› æ­¤ LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">å¯èƒ½ä¼šä»¥æ„å¤–çš„æ–¹å¼è°ƒæ•´å…¶è¡Œä¸ºï¼Œä¾‹å¦‚åœ¨åå¥½ä»…é€‚ç”¨äºä¸ªäººç”µå­é‚®ä»¶æ—¶ç”Ÿæˆæ›´å†—é•¿çš„å·¥ä½œç”µå­é‚®ä»¶ã€‚è¯­è¨€åé¦ˆå¯¹äºäººç±»æ¥è¯´æ›´å®¹æ˜“</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ã€æ›´å¿«é€Ÿåœ°æä¾›ã€‚ä¸ºæ­¤ï¼Œå¦ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯å°†æ­¤ç±»è¯­è¨€åé¦ˆçº³å…¥æç¤ºä¸­ï¼Œå¯èƒ½é€šè¿‡è¿­ä»£è¿‡ç¨‹æ¥æŒç»­æ·»åŠ å…¶ä»–åé¦ˆç‚¹ã€‚ç„¶è€Œï¼Œæ­¤æ–¹</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ³•éœ€è¦åœ¨æ‰€æœ‰æœªæ¥æŸ¥è¯¢ä¸­é‡æ–°ä½¿ç”¨è¯¥æç¤ºã€‚éšç€æ›´å¤šåé¦ˆçš„ç§¯ç´¯ï¼ŒåŒ…å«è®¸å¤šä¸Šä¸‹æ–‡ç›¸å…³åé¦ˆçš„é•¿æç¤ºä¼šä½¿æ¨ç†å˜å¾—æ˜‚è´µï¼›æ­¤å¤–ï¼Œè¯†</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">åˆ«å“ªäº›åé¦ˆåº”å½“åœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­é€‚ç”¨å¯èƒ½å˜å¾—å¾ˆå›°éš¾ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è°ƒæ•´ </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLMï¼Œä»¥ä¾¿åœ¨æä¾›ä¸€ä¸ªæŒ‡å®šåé¦ˆçš„å¥å­æ—¶ï¼Œæ¨¡å‹èƒ½å¤Ÿè¾¨åˆ«åé¦ˆé€‚ç”¨çš„å“ªäº›æƒ…å†µï¼Œå¹¶åœ¨æœªæ¥è¾“å‡ºä¸­é€‚å½“åœ°çº³å…¥è¯¥åé¦ˆã€‚æˆ‘ä»¬æå‡ºäº†æƒ…å¢ƒ</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">åŒ–æ‰¹åˆ¤ä¸çº¦æŸæ€§åå¥½ä¼˜åŒ– </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(C3PO)ï¼Œå…¶ä¸­æˆ‘ä»¬é¦–å…ˆä¸ºè¶…å‡ºåé¦ˆèŒƒå›´å’Œæœªè¶…å‡ºåé¦ˆèŒƒå›´çš„æƒ…å¢ƒåˆæˆç”Ÿæˆå‡è®¾æ€§æç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹è¿™äº›æç¤ºè¿›è¡ŒåŸå§‹å®Œæˆé‡‡æ ·ï¼Œ</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ä¸åº”ç”¨åé¦ˆï¼Œä»¥åŠå¯¹æ‰€æ”¶é›†çš„åé¦ˆè¿›è¡Œä¿®è®¢ï¼Œä»¥ä¾¿ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆä¸€ä¸ªåˆæˆåå¥½æ•°æ®é›†ï¼ŒæŒ‡å®šåé¦ˆåº”å½“ï¼ˆå’Œä¸åº”å½“ï¼‰å¦‚ä½•åº”ç”¨ã€‚æ¥</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ç€ï¼Œæˆ‘ä»¬æ ¹æ®åˆæˆåå¥½æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘è¶…å‡ºåé¦ˆé€‚ç”¨èŒƒå›´çš„æç¤ºçš„åŸå§‹æ¨¡å‹çš„å·®å¼‚ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å°†è¯­è¨€åé¦ˆåº”ç”¨äºç›¸å…³åœºæ™¯ï¼ŒåŒæ—¶ä¿ç•™äº†å…¶ä»–ä¸Šä¸‹æ–‡çš„ç°æœ‰è¡Œä¸ºã€‚å¯¹äºäººç±»å’Œ GPT-4 ç”Ÿæˆçš„è¯­è¨€åé¦ˆï¼ŒC3PO</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">æœ‰æ•ˆåœ°éµå®ˆç»™å®šçš„åé¦ˆï¼Œä¸ä¸Šä¸‹æ–‡ä¸­åŸºå‡†ç›¸å½“ï¼ŒåŒæ—¶å°†è¿‡åº¦æ¦‚æ‹¬å‡å°‘äº† 30%ã€‚\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_title\": \"RLVF: Learning from Verbal Feedback without Overgeneralization\",\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'    \"english_summary\": \"The diversity of contexts in which large language models (LLMs) are deployed </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as â€œDonâ€™t</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">use emojis when drafting emails to my boss.â€ However, while writing high-level feedback is far simpler than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">small synthetic preference dataset specifying how the feedback should (and should not) be applied. It then </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">original model for prompts where the feedback does not apply. Our experimental results indicate that our approach </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">effectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">For both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to in-context baselines while reducing overgeneralization by 30%.\"\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'  }\\n'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'}\\n'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"æ— éœ€æç¤ºå³å¯è¿›è¡Œæ€æƒ³é“¾æ¨ç†\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"ä»¥å‰çš„è¯­è¨€æ¨¡å‹ï¼ˆLMLï¼‰æ¨ç†èƒ½åŠ›æå‡ç ”ç©¶ï¼Œä¸»è¦æ˜¯å…³æ³¨å¼ºåŒ–ç‰¹å®šçš„æç¤ºæŠ€å·§ï¼Œæ¯”å¦‚å°‘é‡æç¤ºæˆ–é›¶æç¤ºæ€æƒ³é“¾ï¼ˆCoTï¼‰æç¤ºã€‚è¿™äº›æ–¹\u001b[0m\n",
       "\u001b[32mæ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡äººå·¥æç¤ºå·¥ç¨‹ã€‚ç ”ç©¶ä»ä¸€ä¸ªæ–°é¢–çš„è§’åº¦å‡ºå‘ï¼Œæå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šLMLæ˜¯å¦å¯ä»¥åœ¨æ²¡æœ‰æç¤ºçš„æƒ…å†µä¸‹è¿›è¡Œ\u001b[0m\n",
       "\u001b[32mæœ‰æ•ˆçš„æ¨ç†ï¼Ÿç ”ç©¶å‘ç°ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå¯ä»¥é€šè¿‡ç®€å•åœ°æ”¹å˜è§£ç è¿‡ç¨‹æ¥ä»é¢„è®­ç»ƒçš„LMLä¸­æå–CoTæ¨ç†è·¯å¾„ã€‚ç ”ç©¶æ²¡æœ‰ä½¿ç”¨ä¼ ç»Ÿçš„\u001b[0m\n",
       "\u001b[32mè´ªå©ªè§£ç ï¼Œè€Œæ˜¯ç ”ç©¶äº†å‰kä¸ªæ›¿ä»£æ ‡è®°ï¼Œå‘ç°CoTè·¯å¾„ç»å¸¸å­˜åœ¨äºè¿™äº›åºåˆ—ä¸­ã€‚è¿™ç§æ–¹æ³•ä¸ä»…ç»•è¿‡äº†æç¤ºçš„æ··æ‚å› ç´ ï¼Œè¿˜å…è®¸ç ”ç©¶äºº\u001b[0m\n",
       "\u001b[32må‘˜è¯„ä¼°LMLçš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶äººå‘˜è¿˜è§‚å¯Ÿåˆ°ï¼Œè§£ç è·¯å¾„ä¸­å­˜åœ¨CoTä¸æ¨¡å‹å¯¹è§£ç ç­”æ¡ˆçš„è¾ƒé«˜ç½®ä¿¡åº¦ç›¸å…³ã€‚ç½®ä¿¡åº¦é‡å¯ä»¥æœ‰æ•ˆåŒºåˆ†C\u001b[0m\n",
       "\u001b[32moTè·¯å¾„å’ŒéCoTè·¯å¾„ã€‚å„ç§æ¨ç†åŸºå‡†çš„å¤§é‡å®è¯ç ”ç©¶è¡¨æ˜ï¼Œæ‰€æå‡ºçš„CoTè§£ç æ–¹æ³•æ˜æ˜¾ä¼˜äºæ ‡å‡†è´ªå©ªè§£ç æ–¹æ³•ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"Chain-of-Thought Reasoning Without Prompting\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Prior research on enhancing the reasoning capabilities of large language models \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m primarily focuses on specific prompting techniques such as few-shot or zero-shot chain-of-thought \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCoT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mprompting. These methods, while effective, often involve manually intensive prompt engineering. Our study takes a \u001b[0m\n",
       "\u001b[32mnovel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal that, intriguingly, \u001b[0m\n",
       "\u001b[32mCoT reasoning paths can be elicited from pre-trained LLMs by simply altering the decoding process. Rather than \u001b[0m\n",
       "\u001b[32mconventional greedy decoding, we investigate the top-k alternative tokens, uncovering that CoT paths are frequently\u001b[0m\n",
       "\u001b[32minherent in these sequences. This approach not only bypasses the confounders of prompting but also allows us to \u001b[0m\n",
       "\u001b[32massess the LLMsâ€™ intrinsic reasoning abilities. Moreover, we observe that the presence of a CoT in the decoding \u001b[0m\n",
       "\u001b[32mpath correlates with a higher confidence in the modelâ€™s decoded answer. This confidence metric effectively \u001b[0m\n",
       "\u001b[32mdifferentiates between CoT and non-CoT paths. Extensive empirical studies on various reasoning benchmarks show that\u001b[0m\n",
       "\u001b[32mthe proposed CoT-decoding substantially outperforms the standard greedy decoding.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"ç”Ÿæˆæ€§è¡¨å¾å¼æŒ‡å¯¼å¾®è°ƒ\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"åŸºäºæ–‡æœ¬çš„è¯­è¨€é—®é¢˜çš†å¯å½’ç»“ä¸ºç”Ÿæˆæˆ–åµŒå…¥ã€‚ç›®å‰ï¼Œæ¨¡å‹åªæ“…é•¿äºå…¶ä¸­ä¸€ç±»ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ç”Ÿæˆæ€§è¡¨å¾å¼æŒ‡å¯¼å¾®è°ƒï¼ˆGRITï¼‰ï¼Œè®­\u001b[0m\n",
       "\u001b[32mç»ƒå¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æŒ‡ä»¤æ¥åŒºåˆ†ç”Ÿæˆå’ŒåµŒå…¥ä»»åŠ¡ï¼Œä»¥å¤„ç†è¿™ä¸¤ç§ä»»åŠ¡ã€‚ä¸å…¶ä»–å¼€æ”¾æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬ç”Ÿæˆçš„ GRITLM 7B \u001b[0m\n",
       "\u001b[32måœ¨æµ·é‡æ–‡æœ¬åµŒå…¥åŸºå‡†ï¼ˆMTEBï¼‰ä¸Šåˆ›ä¸‹æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å…¶è§„æ¨¡èŒƒå›´å†…ï¼Œåœ¨å„ç§ç”Ÿæˆæ€§ä»»åŠ¡ä¸Šéƒ½ä¼˜äºæ‰€æœ‰æ¨¡å‹ã€‚é€šè¿‡è¿›ä¸€æ­¥æ‰©å±•\u001b[0m\n",
       "\u001b[32mï¼ŒGRITLM 8x7B ä¼˜äºæ‰€æœ‰æˆ‘ä»¬å°è¯•è¿‡çš„å¼€æ”¾ç”Ÿæˆè¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶ä»ç„¶æ˜¯æœ€ä½³åµŒå…¥æ¨¡å‹ä¹‹ä¸€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç° GRIT \u001b[0m\n",
       "\u001b[32måŒ¹é…ä»…é’ˆå¯¹ç”Ÿæˆæˆ–åµŒå…¥æ•°æ®è¿›è¡Œçš„è®­ç»ƒï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç»Ÿä¸€ä¸¤è€…ï¼Œè€Œä¸ä¼šæŸå¤±æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡ GRIT \u001b[0m\n",
       "\u001b[32mç»Ÿä¸€èƒ½å¤Ÿå°†é’ˆå¯¹é•¿æ–‡æ¡£çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„é€Ÿåº¦æé«˜å¤§äº 60%ï¼Œä¸å†éœ€è¦å•ç‹¬çš„æ£€ç´¢å’Œç”Ÿæˆæ¨¡å‹ã€‚æ¨¡å‹ã€ä»£ç ç­‰å‡å¯åœ¨ \u001b[0m\n",
       "\u001b[32mhttps://github.com/ContextualAI/gritlm å…è´¹è·å¾—ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"Generative Representational Instruction Tuning\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"All text-based language problems can be reduced to either generation or embedding.\u001b[0m\n",
       "\u001b[32mCurrent models only perform well at one or the other. We introduce generative representational instruction tuning \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mGRIT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m whereby a large language model is trained to handle both generative and embedding tasks by distinguishing \u001b[0m\n",
       "\u001b[32mbetween them through instructions. Compared to other open models, our resulting GRITLM 7B sets a new state of the \u001b[0m\n",
       "\u001b[32mart on the Massive Text Embedding Benchmark \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMTEB\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and outperforms all models up to its size on a range of \u001b[0m\n",
       "\u001b[32mgenerative tasks. By scaling up further, GRITLM 8x7B outperforms all open generative language models that we tried \u001b[0m\n",
       "\u001b[32mwhile still being among the best embedding models. Notably, we find that GRIT matches training on only generative \u001b[0m\n",
       "\u001b[32mor embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT \u001b[0m\n",
       "\u001b[32mspeeds up Retrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m by > 60% for long documents, by no longer requiring separate \u001b[0m\n",
       "\u001b[32mretrieval and generation models. Models, code, etc. are freely available at \u001b[0m\n",
       "\u001b[32mhttps://github.com/ContextualAI/gritlm.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"å¦‚ä½•è®­ç»ƒæ•°æ®é«˜æ•ˆçš„ LLM\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"å¤§è¯­è¨€æ¨¡å‹ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m çš„è®­ç»ƒéå¸¸æ˜‚è´µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”¨äºé¢„è®­ç»ƒ LLM \u001b[0m\n",
       "\u001b[32mçš„æ•°æ®é«˜æ•ˆæ–¹æ³•ï¼Œå³æ—¨åœ¨ä¼˜åŒ–æ¨¡å‹è´¨é‡å’Œè®­ç»ƒèµ„æº/æ•°æ®æ¶ˆè€—çš„å¸•ç´¯æ‰˜å‰æ²¿çš„æŠ€æœ¯ã€‚æˆ‘ä»¬è¯•å›¾äº†è§£ä¸åŸºäº \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mè®¡ç®—æˆæœ¬é«˜çš„æ•°æ®è´¨é‡ä¼°è®¡å’Œ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mii\u001b[0m\u001b[32m)\u001b[0m\u001b[32m ç‰¹å¾ç©ºé—´ä¸­è¦†ç›–èŒƒå›´å’Œå¤šæ ·æ€§æœ€å¤§åŒ–æªæ–½ç›¸å…³çš„æ•°æ®é€‰æ‹©ä¾‹ç¨‹ç›¸å…³çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„ç¬¬ä¸€é¡¹æŠ€æœ¯ \u001b[0m\n",
       "\u001b[32mASK-LLMï¼Œåˆ©ç”¨æŒ‡ä»¤è°ƒæ•´ LLM çš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›æ¥ç›´æ¥è¯„ä¼°è®­ç»ƒæ ·æœ¬çš„è´¨é‡ã€‚ä¸ºäº†æé«˜è¦†ç›–ç‡ï¼Œæˆ‘ä»¬æå‡ºäº† DENSITY \u001b[0m\n",
       "\u001b[32mé‡‡æ ·ï¼Œå®ƒå¯¹æ•°æ®åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ä»¥é€‰æ‹©å¤šæ ·åŒ–çš„æ ·æœ¬ã€‚åœ¨æˆ‘ä»¬çš„ 19 \u001b[0m\n",
       "\u001b[32mä¸ªé‡‡æ ·å™¨çš„æ¯”è¾ƒä¸­ï¼Œæ¶‰åŠæ•°ç™¾ä¸ªè¯„ä¼°ä»»åŠ¡å’Œé¢„è®­ç»ƒè¿è¡Œï¼Œæˆ‘ä»¬å‘ç° ASK-LLM å’Œ DENSITY \u001b[0m\n",
       "\u001b[32mæ˜¯å„è‡ªç±»åˆ«ä¸­çš„æœ€ä½³æ–¹æ³•ã€‚è¦†ç›–é‡‡æ ·å¯ä»¥æ¢å¤å®Œæ•´æ•°æ®çš„æ€§èƒ½ï¼Œè€Œä½¿ç”¨ ASK-LLM \u001b[0m\n",
       "\u001b[32mæ•°æ®è®­ç»ƒçš„æ¨¡å‹å§‹ç»ˆä¼˜äºå®Œæ•´æ•°æ®è®­ç»ƒâ€”â€”å³ä½¿æˆ‘ä»¬èˆå¼ƒäº† 90% çš„åŸå§‹æ•°æ®é›†ï¼ŒåŒæ—¶æ”¶æ•›é€Ÿåº¦æé«˜äº† 70%ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"How to Train Data-Efficient LLMs\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"The training of large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is expensive. In this paper, we study\u001b[0m\n",
       "\u001b[32mdata-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model\u001b[0m\n",
       "\u001b[32mquality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection \u001b[0m\n",
       "\u001b[32mroutines based on \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m expensive-to-compute data-quality estimates, and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mii\u001b[0m\u001b[32m)\u001b[0m\u001b[32m maximization of coverage and \u001b[0m\n",
       "\u001b[32mdiversity-based measures in the feature space. Our first technique, ASK-LLM, leverages the zero-shot reasoning \u001b[0m\n",
       "\u001b[32mcapabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we\u001b[0m\n",
       "\u001b[32mpropose DENSITY sampling, which models the data distribution to select a diverse sample. In our comparison of 19 \u001b[0m\n",
       "\u001b[32msamplers, involving hundreds of evaluation tasks and pre-training runs, we find that ASK-LLM and DENSITY are the \u001b[0m\n",
       "\u001b[32mbest methods in their respective categories. Coverage sampling can recover the performance of the full data, while \u001b[0m\n",
       "\u001b[32mmodels trained on ASK-LLM data consistently outperform full-data trainingâ€”even when we reject 90% of the original \u001b[0m\n",
       "\u001b[32mdataset, while converging up to 70% faster.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"æ‹¥æœ‰æé•¿èƒŒæ™¯è®°å¿†æ‘˜è¦çš„ç±»äººé˜…è¯»ä»£ç†\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mä¸ä»…å—æœ€å¤§èƒŒæ™¯é•¿åº¦é™åˆ¶ï¼Œè€Œä¸”æ— æ³•ç¨³å¥åœ°æ¶ˆè€—é•¿è¾“å…¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† ReadAgentï¼Œè¿™æ˜¯ä¸€ç§ LLM \u001b[0m\n",
       "\u001b[32mä»£ç†ç³»ç»Ÿï¼Œåœ¨æˆ‘ä»¬çš„å®éªŒä¸­å°†æœ‰æ•ˆèƒŒæ™¯é•¿åº¦å¢åŠ äº† 20 å€ã€‚å—äººç±»äº’åŠ¨é˜…è¯»é•¿æ–‡æ¡£æ–¹å¼çš„å¯å‘ï¼Œæˆ‘ä»¬å°† ReadAgent \u001b[0m\n",
       "\u001b[32må®ç°ä¸ºä¸€ä¸ªç®€å•çš„æç¤ºç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨ LLM çš„å…ˆè¿›è¯­è¨€èƒ½åŠ›æ¥ï¼š\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m å†³å®šå°†å“ªäº›å†…å®¹ä¸€èµ·å­˜å‚¨åœ¨ä¸€ä¸ªè®°å¿†ç‰‡æ®µä¸­ï¼Œ\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32må°†è¿™äº›è®°å¿†ç‰‡æ®µå‹ç¼©æˆç§°ä¸ºæ‘˜è¦è®°å¿†çš„ç®€çŸ­ç‰‡æ®µè®°å¿†ï¼Œä»¥åŠ \u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m é‡‡å–æªæ–½åœ¨åŸå§‹æ–‡æœ¬ä¸­æŸ¥æ‰¾æ®µè½ï¼Œå¦‚æœ ReadAgent \u001b[0m\n",
       "\u001b[32méœ€è¦æé†’è‡ªå·±ç›¸å…³çš„ç»†èŠ‚ä»¥å®Œæˆä»»åŠ¡ã€‚æˆ‘ä»¬ä½¿ç”¨æ£€ç´¢æ–¹æ³•ã€ä½¿ç”¨åŸå§‹çš„é•¿èƒŒæ™¯å’Œä½¿ç”¨æ‘˜è¦è®°å¿†ï¼Œé’ˆå¯¹åŸºå‡†è¯„ä¼°äº† \u001b[0m\n",
       "\u001b[32mReadAgentã€‚åœ¨ä¸‰ä¸ªé•¿æ–‡æ¡£é˜…è¯»ç†è§£ä»»åŠ¡ï¼šQUALITYã€NarrativeQA å’Œ QMSum ä¸Šæ‰§è¡Œäº†è¿™äº›è¯„ä¼°ã€‚ReadAgent \u001b[0m\n",
       "\u001b[32måœ¨æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ä¸Šéƒ½ä¼˜äºåŸºå‡†ï¼ŒåŒæ—¶å°†æœ‰æ•ˆçš„èƒŒæ™¯çª—å£æ‰©å¤§äº† 3-20 å€ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"A Human-Inspired Reading Agent with Gist\\\\nMemory of Very Long Contexts\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Current Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are not only limited to some maximum context \u001b[0m\n",
       "\u001b[32mlength, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, \u001b[0m\n",
       "\u001b[32man LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans \u001b[0m\n",
       "\u001b[32minteractively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced \u001b[0m\n",
       "\u001b[32mlanguage capabilities of LLMs to \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m decide what content to store together in a memory episode, \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m compress those \u001b[0m\n",
       "\u001b[32mmemory episodes into short episodic memories called gist memories, and \u001b[0m\u001b[32m(\u001b[0m\u001b[32m3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m take actions to look up passages in the \u001b[0m\n",
       "\u001b[32moriginal text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent \u001b[0m\n",
       "\u001b[32magainst baselines using retrieval methods, using the original long contexts, and using the gist memories. These \u001b[0m\n",
       "\u001b[32mevaluations are performed on three long-document reading comprehension tasks: QUALITY, NarrativeQA, and QMSum. \u001b[0m\n",
       "\u001b[32mReadAgent outperforms the baselines on all three tasks while extending the effective context window by 3 â€” \u001b[0m\n",
       "\u001b[32m20x.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"BitDeltaï¼šä½ çš„å¾®è°ƒå¯èƒ½åªéœ€è¦ 1 æ¯”ç‰¹\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"å¤§å‹è¯­è¨€æ¨¡å‹ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mé€šå¸¸åˆ†ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒï¼šåœ¨å¤§è§„æ¨¡äº’è”ç½‘æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥åŠé’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚é‰´äºé¢„è®­ç»ƒçš„è®¡ç®—éœ€æ±‚è¾ƒé«˜ï¼Œç›´è§‚åœ°è®¤\u001b[0m\n",
       "\u001b[32mä¸ºå¾®è°ƒä¼šç»™æ¨¡å‹æ·»åŠ çš„ä¿¡æ¯æ›´å°‘ï¼Œå› æ­¤æ›´æ˜“äºå‹ç¼©ã€‚æˆ‘ä»¬é€šè¿‡å°†å¾®è°ƒæ¨¡å‹çš„æƒé‡åˆ†è§£ä¸ºå…¶é¢„è®­ç»ƒéƒ¨åˆ†å’Œä¸€ä¸ªé™„åŠ çš„ delta \u001b[0m\n",
       "\u001b[32mæ¥æ¢ç´¢è¿™ä¸€å‡è®¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼ŒBitDeltaï¼Œè¯¥æ–¹æ³•æˆåŠŸåœ°å°†è¿™ä¸ª delta é‡åŒ–ä¸º 1 \u001b[0m\n",
       "\u001b[32mæ¯”ç‰¹ï¼Œè€Œä¸ä¼šæŸå®³æ€§èƒ½ã€‚è¿™ä¸€æœ‰è¶£çš„å‘ç°ä¸ä»…å‡¸æ˜¾äº†å¾®è°ƒè¿‡ç¨‹ä¸­æ·»åŠ çš„ä¿¡æ¯æ½œåœ¨å†—ä½™ï¼Œè€Œä¸”å¯¹å¾®è°ƒæ¨¡å‹çš„å¤šç§Ÿæˆ·æœåŠ¡å’Œå¤šç§Ÿæˆ·å­˜å‚¨\u001b[0m\n",
       "\u001b[32må…·æœ‰é‡å¤§æ„ä¹‰ã€‚é€šè¿‡å¯ç”¨ä½¿ç”¨å•ä¸ªé«˜ç²¾åº¦åŸºç¡€æ¨¡å‹ä»¥åŠå¤šä¸ª 1 æ¯”ç‰¹çš„ deltaï¼ŒBitDelta å°† GPU å†…å­˜éœ€æ±‚å¤§å¹…å‡å°‘äº† 10 \u001b[0m\n",
       "\u001b[32må€ä»¥ä¸Šï¼Œè¿™ä¹Ÿå¯ä»¥è½¬åŒ–ä¸ºå¤šç§Ÿæˆ·è®¾ç½®ä¸­å¢å¼ºçš„ç”Ÿæˆå»¶è¿Ÿã€‚æˆ‘ä»¬é€šè¿‡è·¨è¶Š Llama-2 å’Œ Mistral æ¨¡å‹ç³»åˆ—ä»¥åŠé«˜è¾¾ 70B \u001b[0m\n",
       "\u001b[32må‚æ•°çš„æ¨¡å‹è¿›è¡Œå®éªŒå¯¹ BitDelta è¿›è¡Œäº†éªŒè¯ï¼Œå±•ç¤ºäº†åœ¨æ‰€æœ‰æµ‹è¯•è®¾ç½®ä¸‹æœ€å°çš„æ€§èƒ½ä¸‹é™ã€‚ä»£ç å¯åœ¨ \u001b[0m\n",
       "\u001b[32mhttps://github.com/bitdeltalä¸Šè·å¾—ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"BitDelta: Your Fine-Tune May Only Be Worth One Bit\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are typically trained in two phases: pre-training on \u001b[0m\n",
       "\u001b[32mlarge internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of \u001b[0m\n",
       "\u001b[32mpre-training, itâ€™s intuitive to assume that fine-tuning adds less new information to the model, and is thus more \u001b[0m\n",
       "\u001b[32mcompressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained \u001b[0m\n",
       "\u001b[32mcomponents and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta\u001b[0m\n",
       "\u001b[32mdown to 1 bit without compromising performance. This interesting finding not only highlights the potential \u001b[0m\n",
       "\u001b[32mredundancy of information added during fine-tuning, but also has significant implications for the multi-tenant \u001b[0m\n",
       "\u001b[32mserving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model \u001b[0m\n",
       "\u001b[32maccompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which\u001b[0m\n",
       "\u001b[32mcan also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through \u001b[0m\n",
       "\u001b[32mexperiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal \u001b[0m\n",
       "\u001b[32mperformance degradation over all tested settings. Code is available at https://github.com/bitdeltal.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"OpenMathInstruct-1ï¼šä¸€ä¸ªå…·æœ‰180ä¸‡æ•°å­¦æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆæˆçš„ç”Ÿæˆæ•°æ®é›†åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è·å–ç›®æ ‡æŠ€èƒ½æ–¹é¢ã€‚å½“å‰å¤§è§„æ¨¡çš„\u001b[0m\n",
       "\u001b[32mæ•°å­¦æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œå¦‚MetaMathQAï¼ˆYuç­‰äººï¼Œ2024ï¼‰å’ŒMAmmoTHï¼ˆYueç­‰äººï¼Œ2024ï¼‰æ˜¯ä½¿ç”¨å…·æœ‰å•†ä¸šé™åˆ¶æ€§è®¸å¯è¯çš„é—­æºLLMçš„è¾“å‡º\u001b[0m\n",
       "\u001b[32mæ„å»ºçš„ã€‚é™åˆ¶åœ¨è¿™äº›æ•°æ®ç”Ÿæˆç®¡é“ä¸­ä½¿ç”¨å¼€æºLLMçš„ä¸€ä¸ªå…³é”®åŸå› æ˜¯æœ€å¥½çš„é—­æºLLMï¼ˆå¦‚GPT-4ï¼‰å’Œæœ€å¥½çš„å¼€æºLLMä¹‹é—´çš„æ•°å­¦æŠ€èƒ½å·®\u001b[0m\n",
       "\u001b[32mè·å¾ˆå¤§ã€‚åŸºäºå¼€æºLLMçš„æœ€æ–°è¿›å±•ã€æˆ‘ä»¬æå‡ºçš„æç¤ºæ–°é¢–æ€§ä»¥åŠä¸€äº›è›®åŠ›æ‰©å±•ï¼Œæˆ‘ä»¬æ„å»ºäº†OpenMathInstruct-1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«180\u001b[0m\n",
       "\u001b[32mä¸‡ä¸ªé—®é¢˜-è§£å†³æ–¹æ¡ˆå¯¹çš„æ•°å­¦æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ˜¯ä½¿ç”¨æœ€è¿‘å‘å¸ƒçš„ã€è·å¾—è®¸å¯çš„Mixtralæ¨¡å‹ï¼Œé€šè¿‡ç»¼åˆGSM8Kå’ŒMATHï¼ˆä¸¤ä¸ª\u001b[0m\n",
       "\u001b[32mæµè¡Œçš„æ•°å­¦æ¨ç†åŸºå‡†ï¼‰çš„ä»£ç è§£é‡Šå™¨è§£å†³æ–¹æ¡ˆæ„å»ºçš„ã€‚æˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹OpenMath-CodeLlama-70Bï¼Œåœ¨OpenMathInstruct-1çš„ä¸€ä¸ªå­\u001b[0m\n",
       "\u001b[32mé›†ä¸Šè®­ç»ƒï¼Œåœ¨GSM8Kä¸Šè·å¾—äº†84.6%çš„åˆ†æ•°ï¼Œåœ¨MATHä¸Šè·å¾—äº†50.7%çš„åˆ†æ•°ï¼Œè¿™ä¸æœ€å¥½çš„gptè’¸é¦æ¨¡å‹æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬åœ¨å•†ä¸šä¸Šå®½æ¾çš„\u001b[0m\n",
       "\u001b[32mè®¸å¯è¯ä¸‹å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’ŒOpenMathInstruct-1æ•°æ®é›†ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"The huge development and inference costs associated with general-purpose large \u001b[0m\n",
       "\u001b[32mlanguage models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have led to the rise of smaller, task-specific LLMs. Recent work has proposed creating these\u001b[0m\n",
       "\u001b[32mdomain/task-specific LLMs by generating high-quality synthetic data using powerful closed-source models such as \u001b[0m\n",
       "\u001b[32mGPT-3.5/4 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOpenAI et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and training smaller models on the generated distillation data \u001b[0m\u001b[32m(\u001b[0m\u001b[32mEldan and Li, 2023;\u001b[0m\n",
       "\u001b[32mGunasekar et al., 2023; Li et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. For mathematical reasoning, our task of interest, all the current \u001b[0m\n",
       "\u001b[32mstate-of-the-art open-source models are gpt-distilled \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWang et al., 2024; Yue et al., 2024; Gou et al., 2024; Liao \u001b[0m\n",
       "\u001b[32met al., 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. However, model development recipes relying on proprietary models like GPT-4 can have serious \u001b[0m\n",
       "\u001b[32mlimitations: \u001b[0m\u001b[32m(\u001b[0m\u001b[32ma\u001b[0m\u001b[32m)\u001b[0m\u001b[32m legal restraints on how the finetuned models can be used,\\\\\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mb\u001b[0m\u001b[32m)\u001b[0m\u001b[32m generating data with \u001b[0m\n",
       "\u001b[32mclosed-source models is typically costlier than state-of-the-art open-source models, and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m these recipes lack \u001b[0m\n",
       "\u001b[32mreproducibility as closed-source model behaviors can vary significantly over time \u001b[0m\u001b[32m(\u001b[0m\u001b[32mChen et al., 2023a\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. For \u001b[0m\n",
       "\u001b[32mdeveloping mathematical reasoning models, why are open-source models not used in place of closed-source models? To \u001b[0m\n",
       "\u001b[32manswer this, we compare GPT-4 with Mixtral 8x7B model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mJiang et al., 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, currently one of the best open-source \u001b[0m\n",
       "\u001b[32mLLMs at mathematical reasoning, by generating code-interpreter\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è‡ªåšå¼ˆå¾®è°ƒæ‰©æ•£æ¨¡å‹\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"æˆ‘ä»¬ä»‹ç»äº† SPIN-Diffusionï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ‰©æ•£æ¨¡å‹çš„è‡ªåšå¼ˆå¾®è°ƒç®—æ³•ã€‚è¯¥ç»“æœæ˜¯ä» Stable\u001b[0m\n",
       "\u001b[32mDiffusion v1.5 å¯¹ Pick-a-Pic æ•°æ®é›†çš„è·èƒœå›¾ç‰‡è¿›è¡Œå¾®è°ƒå¾—åˆ°çš„ã€‚ç”¨äºç”Ÿæˆä»¥ä¸Šå›¾åƒçš„æç¤ºæ˜¯ä» Pick-a-Pic \u001b[0m\n",
       "\u001b[32mæµ‹è¯•é›†ä¸­é€‰å–çš„ã€‚ç”Ÿæˆçš„å›¾åƒåœ¨æ•´ä½“è§†è§‰å¸å¼•åŠ›å’Œä¸æç¤ºçš„ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚SPIN-Diffusion \u001b[0m\n",
       "\u001b[32mçš„ç‰¹ç‚¹æ˜¯å®ƒä¸ä¾èµ–äºæˆå¯¹çš„äººç±»åå¥½æ•°æ®ï¼Œä¸ºåœ¨ä»…æä¾›æ¯ä¸ªæ–‡æœ¬æç¤ºä¸€å¼ å›¾ç‰‡çš„è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæä¾›äº†æœ‰ç”¨çš„å·¥å…·ã€‚\",\\n'\u001b[0m\n",
       ",\n",
       "        \u001b[32m'    \"english_title\": \"Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"We introduce SPIN-Diffusion, a self-play fine-tuning algorithm for diffusion \u001b[0m\n",
       "\u001b[32mmodels. The results are fine-tuned from Stable Diffusion v1.5 on the winner images of the Pick-a-Pic dataset. The \u001b[0m\n",
       "\u001b[32mprompts used for generating the above images are chosen from the Pick-a-Pic test set. The generated images \u001b[0m\n",
       "\u001b[32mdemonstrate superior performance in terms of overall visual attractiveness and coherence with the prompts. \u001b[0m\n",
       "\u001b[32mSPIN-Diffusion is featured by its independence from paired human preference data, offering a useful tool for \u001b[0m\n",
       "\u001b[32mfine-tuning on custom datasets with only single image per text prompt provided.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"å¯å­¦ä¹ æ ¸å‡½æ•°çš„çº¿æ€§å˜æ¢å™¨æ˜¯æ›´å¥½çš„ä¸Šä¸‹æ–‡æ¨¡å‹\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å¿«é€Ÿå‘å±•çš„é¢†åŸŸå†…ï¼Œæ¨è¿›è¯­è¨€æ¨¡å‹ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mçš„äºšäºŒæ¬¡æ¶æ„æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„å·¥ä½œã€‚åŒ…æ‹¬çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨å†…çš„å½“å‰åˆ›æ–°æœ€åˆå› å…¶åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­è¶…è¶Šäº† Transformer \u001b[0m\n",
       "\u001b[32mæ€§èƒ½è€Œå¹¿å—èµèª‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¿…è¦çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ–¹é¢æš´éœ²å‡ºç¼ºé™·ï¼Œè¿™æ˜¯ Transformer ä¼ ç»Ÿä¸Šè¡¨ç°å‡ºè‰²çš„é¢†åŸŸã€‚Based \u001b[0m\n",
       "\u001b[32mæ¨¡å‹ä½œä¸ºä¸€ç§æ··åˆè§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œå®ƒå°†çº¿æ€§å˜æ¢å™¨ä¸å—æŒ‡æ•°å‡½æ•°çš„æ³°å‹’å±•å¼€æ‰€å¯å‘çš„æ ¸å‡½æ•°ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡å·ç§¯ç½‘ç»œè¿›è¡Œäº†æ‰©\u001b[0m\n",
       "\u001b[32må±•ã€‚å®ƒå¤åˆ¶äº† Transformer åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç†Ÿç»ƒåº¦ï¼Œæˆä¸ºè¯¥é¢†åŸŸçš„æœ‰åŠ›ç«äº‰è€…ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹ Based \u001b[0m\n",
       "\u001b[32mæ ¸å‡½æ•°æå‡ºäº†ä¸€ç§ç‹¬ç‰¹è€Œä¼˜é›…çš„ä¿®æ”¹ï¼Œé€šè¿‡å¤šæŸ¥è¯¢å…³è”å¬å›ä»»åŠ¡å’Œåœ¨ Pile \u001b[0m\n",
       "\u001b[32mæ•°æ®é›†ä¸Šå±•ç¤ºçš„æ•´ä½“è¯­è¨€å»ºæ¨¡è¿‡ç¨‹è¯„ä¼°ï¼Œæ”¾å¤§äº†å…¶ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"Linear Transformers with Learnable Kernel Functions are Better In-Context \u001b[0m\n",
       "\u001b[32mModels\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Advancing the frontier of subquadratic architectures for Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is \u001b[0m\n",
       "\u001b[32mcrucial in the rapidly evolving field of natural language processing. Current innovations, including State Space \u001b[0m\n",
       "\u001b[32mModels, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these\u001b[0m\n",
       "\u001b[32mmodels have revealed deficiencies in essential In-Context Learning capabilities â€” a domain where the Transformer \u001b[0m\n",
       "\u001b[32mtraditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel \u001b[0m\n",
       "\u001b[32minspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the \u001b[0m\n",
       "\u001b[32mTransformerâ€™s in-context adeptness, it became a strong contender in the field. In our work, we present a singular, \u001b[0m\n",
       "\u001b[32melegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the \u001b[0m\n",
       "\u001b[32mMulti-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile \u001b[0m\n",
       "\u001b[32mdataset.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"åœ¨ä¸€åƒå…†å­—èŠ‚çš„å¹²è‰å †ä¸­å¯»æ‰¾é’ˆå¤´ï¼šå¾ªç¯è®°å¿†å‘ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹é”™å¤±çš„ä¸œè¥¿\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"æœ¬æ–‡è®¨è®ºäº†ä½¿ç”¨ç”Ÿæˆå¼ Transformer \u001b[0m\n",
       "\u001b[32mæ¨¡å‹å¤„ç†é•¿æ–‡æ¡£çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è¯„ä¼°ä¸åŒçš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº† \u001b[0m\n",
       "\u001b[32mBABILongï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨æå–å’Œå¤„ç†å†—é•¿æ–‡æœ¬ä¸­çš„åˆ†å¸ƒå¼äº‹å®æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬ GPT-4 å’Œ RAG \u001b[0m\n",
       "\u001b[32mçš„åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜å¸¸è§çš„æ–¹æ³•ä»…å¯¹é«˜è¾¾ 10^4 ä¸ªå…ƒç´ çš„åºåˆ—æœ‰æ•ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”¨å¾ªç¯è®°å¿†å¢å¼ºå¯¹ GPT-2 \u001b[0m\n",
       "\u001b[32mçš„å¾®è°ƒä½¿å…¶èƒ½å¤Ÿå¤„ç†æ¶‰åŠå¤šè¾¾ 10^7 \u001b[0m\n",
       "\u001b[32mä¸ªå…ƒç´ çš„ä»»åŠ¡ã€‚è¿™ä¸€æˆå°±æ˜¯ä¸€é¡¹é‡å¤§é£è·ƒï¼Œå› ä¸ºè¿™æ˜¯è¿„ä»Šä¸ºæ­¢ä»»ä½•å¼€æ”¾ç¥ç»ç½‘ç»œæ¨¡å‹å¤„ç†çš„æœ€é•¿è¾“å…¥ï¼Œå±•ç¤ºäº†å¤„ç†é•¿åºåˆ—èƒ½åŠ›çš„æ˜¾\u001b[0m\n",
       "\u001b[32mè‘—æé«˜ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"This paper tackles the challenge of processing lengthy documents using generative \u001b[0m\n",
       "\u001b[32mtransformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess \u001b[0m\n",
       "\u001b[32mmodel capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which \u001b[0m\n",
       "\u001b[32mincludes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to 10^4 \u001b[0m\n",
       "\u001b[32melements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving \u001b[0m\n",
       "\u001b[32mup to 10^7 elements. This achievement marks a substantial leap, as it is by far the longest input processed by any \u001b[0m\n",
       "\u001b[32mopen neural network model to date, demonstrating a significant improvement in the processing capabilities for long \u001b[0m\n",
       "\u001b[32msequences.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"DataDreamerï¼šåˆæˆæ•°æ®ç”Ÿæˆå’Œå¯å¤åˆ¶ LLM å·¥ä½œæµå·¥å…·\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"å¤§å‹è¯­è¨€æ¨¡å‹ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m å·²æˆä¸º NLP \u001b[0m\n",
       "\u001b[32mç ”ç©¶äººå‘˜åœ¨å„ç§ä»»åŠ¡ä¸­çš„ä¸€ä¸ªä¸»è¦å·¥å…·ã€‚å¦‚ä»Šï¼Œå¾ˆå¤šç ”ç©¶äººå‘˜åœ¨åˆæˆæ•°æ®ç”Ÿæˆã€ä»»åŠ¡è¯„ä¼°ã€æ¨¡å‹å¾®è°ƒã€è’¸é¦å’Œå…¶ä»–æ¨¡å‹å¾ªç¯ç ”ç©¶å·¥\u001b[0m\n",
       "\u001b[32mä½œæµä¸­ä½¿ç”¨ \u001b[0m\n",
       "\u001b[32mLLMã€‚ä½†æ˜¯ï¼Œåœ¨ä½¿ç”¨è¿™äº›æ¨¡å‹æ—¶ä¼šäº§ç”Ÿä¸€äº›æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜æºäºå…¶è§„æ¨¡ã€å°é—­æºä»£ç çš„æœ¬è´¨ä»¥åŠè¿™äº›æ–°å…´å·¥ä½œæµç¼ºä¹æ ‡å‡†åŒ–å·¥å…·ã€‚è¿™\u001b[0m\n",
       "\u001b[32mäº›æ¨¡å‹çš„å¿«é€Ÿå‘å±•å’Œè¿™äº›ç‹¬æœ‰çš„æŒ‘æˆ˜å¯¹å¼€æ”¾ç§‘å­¦å’Œä½¿ç”¨è¿™äº›æ¨¡å‹çš„å·¥ä½œçš„å¯å¤åˆ¶æ€§äº§ç”Ÿäº†ç›´æ¥çš„ä¸åˆ©å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† \u001b[0m\n",
       "\u001b[32mDataDreamerï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æº Python åº“ï¼Œå…è®¸ç ”ç©¶äººå‘˜ç¼–å†™ç®€å•çš„ä»£ç æ¥å®ç°å¼ºå¤§çš„ LLM å·¥ä½œæµã€‚DataDreamer \u001b[0m\n",
       "\u001b[32mè¿˜å¸®åŠ©ç ”ç©¶äººå‘˜éµå®ˆæˆ‘ä»¬æè®®çš„æœ€ä½³å®è·µï¼Œä»¥ä¿ƒè¿›å¼€æ”¾ç§‘å­¦å’Œå¯å¤åˆ¶æ€§ã€‚è¯¥åº“å’Œæ–‡æ¡£å¯ä»¥åœ¨ä»¥ä¸‹ä½ç½®è·å¾—ï¼šhttps://github.com/da\u001b[0m\n",
       "\u001b[32mtadreamer-dev/DataDreamerã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM \u001b[0m\n",
       "\u001b[32mWorkflows\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have become a dominant and important tool for NLP \u001b[0m\n",
       "\u001b[32mresearchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task \u001b[0m\n",
       "\u001b[32mevaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise \u001b[0m\n",
       "\u001b[32mwhen using these models that stem from their scale, their closed source nature, and the lack of standardized \u001b[0m\n",
       "\u001b[32mtooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique \u001b[0m\n",
       "\u001b[32mchallenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In \u001b[0m\n",
       "\u001b[32mthis paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to\u001b[0m\n",
       "\u001b[32mimplement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to \u001b[0m\n",
       "\u001b[32mencourage open science and reproducibility. The library and documentation are available at: \u001b[0m\n",
       "\u001b[32mhttps://github.com/datadreamer-dev/DataDreamer.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"GaussianObjectï¼šåªç”¨å››å¼ å›¾åƒè·å¾—å¸¦æœ‰é«˜æ–¯æ³¼æº…çš„é«˜è´¨é‡3Då¯¹è±¡\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"é‡å»ºå’Œæ¸²æŸ“é«˜åº¦ç¨€ç–è§†å›¾ä¸­çš„3Då¯¹è±¡å¯¹æ¨å¹¿3Dè§†è§‰æŠ€æœ¯çš„åº”ç”¨å’Œæ”¹å–„ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç¨€ç–è§†å›¾çš„å›¾åƒåªåŒ…å«éå¸¸æœ‰é™çš„\u001b[0m\n",
       "\u001b[32m3Dä¿¡æ¯ï¼Œè¿™å¸¦æ¥äº†ä¸¤ä¸ªé‡å¤§æŒ‘æˆ˜ï¼š1ï¼‰éš¾ä»¥å»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå› ä¸ºåŒ¹é…çš„å›¾åƒå¤ªå°‘ï¼›2ï¼‰ç”±äºè§†å›¾è¦†ç›–ä¸è¶³ï¼Œå¯¹è±¡ä¿¡æ¯éƒ¨åˆ†çœç•¥æˆ–\u001b[0m\n",
       "\u001b[32mé«˜åº¦å‹ç¼©ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GaussianObjectï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨é«˜æ–¯æ³¼æº…æ¥è¡¨ç¤ºå’Œæ¸²æŸ“3Då¯¹è±¡çš„æ¡†æ¶ï¼Œå®ƒä»…ä½¿ç”¨4ä¸ªè¾“å…¥\u001b[0m\n",
       "\u001b[32må›¾åƒå°±èƒ½å®ç°é«˜æ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬é¦–å…ˆä»‹ç»äº†è§†è§‰åŒ…ç»œå’Œæµ®åŠ¨æ¶ˆé™¤æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å°†ç»“æ„å…ˆå…¥ä¸ºä¸»åœ°æ³¨å…¥åˆ°åˆå§‹ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œä»¥å¸®åŠ©\u001b[0m\n",
       "\u001b[32må»ºç«‹å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œä»è€Œäº§ç”Ÿä¸€ä¸ªç²—ç•¥çš„3Dé«˜æ–¯è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬åŸºäºæ‰©æ•£æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªé«˜æ–¯ä¿®å¤æ¨¡å‹æ¥è¡¥å……è¢«çœç•¥çš„å¯¹è±¡ä¿¡æ¯\u001b[0m\n",
       "\u001b[32mï¼Œå…¶ä¸­é«˜æ–¯ä½“è¿›ä¸€æ­¥å¾—åˆ°ç»†åŒ–ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªç”Ÿæˆç­–ç•¥æ¥è·å¾—ç”¨äºè®­ç»ƒä¿®å¤æ¨¡å‹çš„å›¾åƒå¯¹ã€‚æˆ‘ä»¬çš„GaussianObjectåœ¨å‡ ä¸ªå…·æœ‰\u001b[0m\n",
       "\u001b[32mæŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬MipNeRF360ã€OmniObject3Då’ŒOpenll-luminationï¼Œä»…ä½¿ç”¨4ä¸ªè§†å›¾å°±è·å¾—äº†å¼ºå¤§çš„é‡å»ºç»“æœï¼Œ\u001b[0m\n",
       "\u001b[32må¹¶ä¸”æ˜æ˜¾ä¼˜äºä»¥å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢https://gaussianobject.github.io/ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with \u001b[0m\n",
       "\u001b[32mGaussian Splatting\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Reconstructing and rendering 3D objects from 2D images has been a longstanding and\u001b[0m\n",
       "\u001b[32mimportant topic, which plays critical roles in a vast range of real-life applications. One key factor that impedes \u001b[0m\n",
       "\u001b[32musers, especially ones without expert knowledge, from widely using these techniques is that usually dozens of \u001b[0m\n",
       "\u001b[32mmulti-view images need to be captured, which is cumbersome and sometimes impractical. Efficiently reconstructing \u001b[0m\n",
       "\u001b[32mhigh-quality 3D objects from highly sparse captured images is of great value for expediting downstream applications\u001b[0m\n",
       "\u001b[32msuch as 3D asset creation for game/movie production and AR/VR products.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"LLM Comparatorï¼šå¯è§†åŒ–åˆ†æï¼Œè¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¶æ’è¯„ä¼°\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"è¯¥ PDF æ–‡æ¡£ä»‹ç»äº† LLM Comparatorï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¯è§†åŒ–åˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mè‡ªåŠ¨å¹¶æ’è¯„ä¼°ç»“æœçš„äº¤äº’å¼å·¥å…·ã€‚è¯¥å·¥å…·æ”¯æŒæ¨¡å‹å¼€å‘è€…å’Œç ”ç©¶äººå‘˜æ¯”è¾ƒä¸¤ä¸ª LLM \u001b[0m\n",
       "\u001b[32mçš„å“åº”è´¨é‡ï¼Œè¯†åˆ«å‡ºä½•æ—¶ä»¥åŠä¸ºä½•ä¸€ä¸ªæ¨¡å‹çš„æ€§èƒ½ä¼˜äºæˆ–åŠ£äºå¦ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶æ¢ç´¢ä¸¤ä¸ªæ¨¡å‹å“åº”ä¹‹é—´çš„å·®å¼‚ã€‚LLM Comparator \u001b[0m\n",
       "\u001b[32mæä¾›äº†ä¸€ä¸ªäº¤äº’å¼è¡¨æ ¼ï¼Œç”¨äºæ£€æŸ¥å•ä¸ªæç¤ºåŠå…¶å“åº”ï¼Œä»¥åŠä¸€ä¸ªå¯è§†åŒ–æ‘˜è¦ï¼Œæ”¯æŒå¸®åŠ©ç†è§£æ¨¡å‹å·®å¼‚çš„åˆ†æå·¥ä½œæµã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language \u001b[0m\n",
       "\u001b[32mModels\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"This PDF document presents LLM Comparator, an interactive tool for visual \u001b[0m\n",
       "\u001b[32manalytics of automatic side-by-side evaluation results of large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. It enables model developers\u001b[0m\n",
       "\u001b[32mand researchers to compare the quality of responses from two LLMs, identify when and why one model performs better \u001b[0m\n",
       "\u001b[32mor worse than the other, and explore the differences in their responses. LLM Comparator provides an interactive \u001b[0m\n",
       "\u001b[32mtable for inspecting individual prompts and their responses, and a visualization summary that supports analytical \u001b[0m\n",
       "\u001b[32mworkflows for understanding model differences.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"LAVEï¼šé’ˆå¯¹è§†é¢‘ç¼–è¾‘çš„ LLM åŠ©åŠ›ä»£ç†ä¸è‡ªç„¶è¯­è¨€å¢å¼º\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \"LAVE æ˜¯ä¸€æ¬¾è§†é¢‘ç¼–è¾‘å·¥å…·ï¼Œå®ƒæä¾›äº† LLM \u001b[0m\n",
       "\u001b[32måŠ©åŠ›ä»£ç†ä¸è‡ªç„¶è¯­è¨€å¢å¼ºåŠŸèƒ½ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æ›´è½»æ¾åœ°ç¼–è¾‘è§†é¢‘ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æ–‡å­—ä¸ LAVE \u001b[0m\n",
       "\u001b[32mçš„è§†é¢‘ç¼–è¾‘ä»£ç†äº’åŠ¨ï¼Œåœ¨æ•´ä¸ªç¼–è¾‘è¿‡ç¨‹ä¸­è·å¾—å®æ—¶å¸®åŠ©ã€‚æ­¤å¤–ï¼ŒLAVE \u001b[0m\n",
       "\u001b[32mè¿˜æä¾›ä¸€ç³»åˆ—è¯­è¨€å¢å¼ºåŠŸèƒ½ï¼Œä¾‹å¦‚è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ ‡é¢˜ã€æ˜¾ç¤ºè§†é¢‘æ‘˜è¦å¸®åŠ©ç”¨æˆ·å¿«é€Ÿäº†è§£è§†é¢‘å†…å®¹ã€åœ¨ç¼©ç•¥å›¾åº“ä¸­é€šè¿‡ç‚¹å‡»è§†é¢‘å³å¯\u001b[0m\n",
       "\u001b[32må°†å…¶æ·»åŠ åˆ°ç¼–è¾‘æ—¶é—´è½´ç­‰ï¼Œè¿™äº›åŠŸèƒ½å¯ä»¥æå¤§åœ°ç®€åŒ–å’ŒåŠ é€Ÿè§†é¢‘ç¼–è¾‘æµç¨‹ï¼Œé™ä½æ–°æ‰‹å…¥é—¨é—¨æ§›ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"The Increasing demand for video content creation has been hindered by the \u001b[0m\n",
       "\u001b[32mtechnical expertise and effort typically required for video editing. This paper presents an integration of large \u001b[0m\n",
       "\u001b[32mlanguage models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m into the video editing workflow to lower these barriers, specifically focusing on assistance\u001b[0m\n",
       "\u001b[32mand language augmentation. Our proposed system, LAVE, offers an agent that interacts with users using natural \u001b[0m\n",
       "\u001b[32mlanguage, providing assistance with various editing tasks. It also incorporates language augmentation into the \u001b[0m\n",
       "\u001b[32mediting workflow, with features such as automated video summarization, video title generation, and quick video \u001b[0m\n",
       "\u001b[32mselection from a gallery. We believe that LAVE simplifies the video editing process, making it more accessible to a\u001b[0m\n",
       "\u001b[32mwider range of users.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"åŸºäºå‡½æ•°è°ƒç”¨çš„é›¶æ ·æœ¬å¯¹è¯çŠ¶æ€è¿½è¸ªå™¨ä¸­çš„å¤§è¯­è¨€æ¨¡å‹\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å› å…¶åœ¨ä¸€èˆ¬è¯­å¢ƒä¸‹çš„é«˜çº§ç†è§£å’Œç”Ÿæˆèƒ½åŠ›è€Œåœ¨ä¼šè¯ç³»ç»Ÿä¸­æ—¥ç›Šæ™®åŠã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä»»åŠ¡å¯¼å‘å¯¹è¯ï¼ˆTODï¼‰ä¸­çš„æœ‰\u001b[0m\n",
       "\u001b[32mæ•ˆæ€§ä»ç„¶ä¸å°½å¦‚äººæ„ï¼Œè¿™ä¸ä»…éœ€è¦ç”Ÿæˆå“åº”ï¼Œè¿˜éœ€è¦åœ¨ç‰¹å®šä»»åŠ¡å’Œé¢†åŸŸå†…è¿›è¡Œæœ‰æ•ˆçš„å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ\u001b[0m\n",
       "\u001b[32må‡ºäº†ä¸€ç§é€šè¿‡å‡½æ•°è°ƒç”¨ä½¿ç”¨ LLM è§£å†³ DST çš„æ–°æ–¹æ³• FNCTODã€‚è¯¥æ–¹æ³•æ”¹è¿›äº†é›¶æ ·æœ¬ \u001b[0m\n",
       "\u001b[32mDSTï¼Œå…è®¸é€‚åº”ä¸åŒçš„é¢†åŸŸï¼Œè€Œæ— éœ€å¤§é‡æ•°æ®æ”¶é›†æˆ–æ¨¡å‹è°ƒæ•´ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä¸­ç­‰è§„æ¨¡çš„å¼€æºå’Œä¸“æœ‰ LLM \u001b[0m\n",
       "\u001b[32méƒ½å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼šåœ¨ä¸Šä¸‹æ–‡ä¸­æç¤ºä¸‹ï¼Œå®ƒä½¿å„ç§ 7B æˆ– 13B å‚æ•°æ¨¡å‹èƒ½å¤Ÿè¶…è¶Š ChatGPT \u001b[0m\n",
       "\u001b[32må®ç°çš„å…ˆå‰æœ€å…ˆè¿›ï¼ˆSOTAï¼‰æŠ€æœ¯ï¼Œå¹¶å°† ChatGPT çš„æ€§èƒ½æé«˜äº† 5.6% çš„å¹³å‡ JGAã€‚GPT-3.5 å’Œ GPT-4 çš„å•ä¸ªæ¨¡å‹ç»“æœåˆ†åˆ«æé«˜äº† \u001b[0m\n",
       "\u001b[32m4.8% å’Œ 14%ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œé€šè¿‡é’ˆå¯¹å°è§„æ¨¡çš„ä¸åŒä»»åŠ¡å¯¼å‘å¯¹è¯è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºä¸­ç­‰è§„æ¨¡çš„æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ 13B å‚æ•° \u001b[0m\n",
       "\u001b[32mLLaMA2-Chat æ¨¡å‹ï¼‰é…å¤‡å‡½æ•°è°ƒç”¨èƒ½åŠ›å’Œä¸ ChatGPT ç›¸åª²ç¾çš„ DST \u001b[0m\n",
       "\u001b[32mæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå…¶èŠå¤©èƒ½åŠ›ã€‚æˆ‘ä»¬å°†å¼€æºå®éªŒä»£ç å’Œæ¨¡å‹ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"Large Language Models as Zero-shot Dialogue State Tracker through Function \u001b[0m\n",
       "\u001b[32mCalling\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"Large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are increasingly prevalent in conversational systems \u001b[0m\n",
       "\u001b[32mdue to their advanced understanding and generative capabilities in general contexts. However, their effectiveness \u001b[0m\n",
       "\u001b[32min task-oriented dialogues \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTOD\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which requires not only response generation but also effective dialogue state \u001b[0m\n",
       "\u001b[32mtracking \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDST\u001b[0m\u001b[32m)\u001b[0m\u001b[32m within specific tasks and domains, remains less satisfying. In this work, we propose a novel \u001b[0m\n",
       "\u001b[32mapproach FNCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing \u001b[0m\n",
       "\u001b[32madaptation to diverse domains without extensive data collection or model tuning. Our experimental results \u001b[0m\n",
       "\u001b[32mdemonstrate that our approach achieves exceptional performance with both modestly sized open-source and also \u001b[0m\n",
       "\u001b[32mproprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous \u001b[0m\n",
       "\u001b[32mstate-of-the-art \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSOTA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m achieved by ChatGPT, and improves ChatGPTâ€™s performance beating the SOTA by 5.6% Avg. JGA. \u001b[0m\n",
       "\u001b[32mIndividual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by \u001b[0m\n",
       "\u001b[32mfine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, \u001b[0m\n",
       "\u001b[32mspecifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable \u001b[0m\n",
       "\u001b[32mto ChatGPT while maintaining their chat capabilities. We will open-source experimental code and model.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m' \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"chinese_title\": \"æ„å»ºå»‰ä»·ç¼©æ”¾ï¼šä¸€ç§ç”¨äºé«˜åˆ†è¾¨ç‡é€‚åº”çš„è‡ªçº§è”æ‰©æ•£æ¨¡å‹\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­å·²è¢«è¯æ˜éå¸¸æœ‰æ•ˆï¼›ç„¶è€Œï¼Œç”±äºå•å°ºåº¦è®­ç»ƒæ•°æ®ï¼Œå®ƒä»¬åœ¨ç”Ÿæˆä¸åŒå¤§å°çš„å›¾åƒæ—¶ä»ç„¶é¢ä¸´æ„å›¾æŒ‘æˆ˜ã€‚\u001b[0m\n",
       "\u001b[32mé’ˆå¯¹é«˜åˆ†è¾¨ç‡è°ƒæ•´å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—å’Œä¼˜åŒ–èµ„æºï¼Œä½†ä»æ— æ³•å®ç°ä¸ä½åˆ†è¾¨ç‡æ¨¡å‹ç›¸å½“çš„ç”Ÿæˆèƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€\u001b[0m\n",
       "\u001b[32mç§æ–°é¢–çš„è‡ªçº§è”æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨ä»ç»è¿‡è‰¯å¥½è®­ç»ƒçš„ä½åˆ†è¾¨ç‡æ¨¡å‹ä¸­è·å¾—çš„ä¸°å¯ŒçŸ¥è¯†ï¼Œå¿«é€Ÿé€‚åº”é«˜åˆ†è¾¨ç‡å›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œé‡‡\u001b[0m\n",
       "\u001b[32mç”¨æ— è°ƒä¼˜æˆ–å»‰ä»·ä¸Šé‡‡æ ·å™¨è°ƒä¼˜èŒƒä¾‹ã€‚è‡ªçº§è”æ‰©æ•£æ¨¡å‹é›†æˆäº†åºåˆ—å¤šå°ºåº¦ä¸Šé‡‡æ ·å™¨æ¨¡å—ï¼Œèƒ½æœ‰æ•ˆåœ°é€‚åº”æ›´é«˜çš„åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¿ç•™äº†åŸ\u001b[0m\n",
       "\u001b[32må§‹æ„å›¾å’Œç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ¢è½´å¼•å¯¼å™ªå£°é‡æ–°è°ƒåº¦ç­–ç•¥ï¼Œä»¥åŠ é€Ÿæ¨ç†è¿‡ç¨‹å¹¶æ”¹å–„å±€éƒ¨ç»“æ„ç»†èŠ‚ã€‚ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼Œ\u001b[0m\n",
       "\u001b[32mæˆ‘ä»¬çš„æ–¹æ³•å°†è®­ç»ƒé€Ÿåº¦æé«˜äº† 5 å€ï¼Œå¹¶ä¸”ä»…éœ€è¦é¢å¤–çš„ 0.002M è°ƒä¼˜å‚æ•°ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é€šè¿‡ä»…å¾®è°ƒ 10k \u001b[0m\n",
       "\u001b[32mæ­¥ï¼Œåœ¨å‡ ä¹æ²¡æœ‰é¢å¤–æ¨ç†æ—¶é—´çš„æƒ…å†µä¸‹ï¼Œå¿«é€Ÿé€‚åº”æ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒå’Œè§†é¢‘åˆæˆã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨ \u001b[0m\n",
       "\u001b[32mhttps://github.com/GuoLanging/Self-Cascade/ å‘å¸ƒã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"english_title\": \"Make a Cheap Scaling: A Self-Cascade Diffusion Model for Higher-Resolution \u001b[0m\n",
       "\u001b[32mAdaptation\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"english_summary\": \"Diffusion models have proven to be highly effective in image and video generation; \u001b[0m\n",
       "\u001b[32mhowever, they still face composition challenges when generating images of varying sizes due to single-scale \u001b[0m\n",
       "\u001b[32mtraining data. Adapting large pre-trained diffusion models for higher resolution demands substantial computational \u001b[0m\n",
       "\u001b[32mand optimization resources, yet achieving a generation capability comparable to low-resolution models remains \u001b[0m\n",
       "\u001b[32melusive. This paper proposes a novel self-cascade diffusion model that leverages the rich knowledge gained from a \u001b[0m\n",
       "\u001b[32mwell-trained low-resolution model for rapid adaptation to higher-resolution image and video generation, employing \u001b[0m\n",
       "\u001b[32meither tuning-free or cheap upsampler tuning paradigms. Integrating a sequence of multi-scale upsampler modules, \u001b[0m\n",
       "\u001b[32mthe self-cascade diffusion model can efficiently adapt to a higher resolution, preserving the original composition \u001b[0m\n",
       "\u001b[32mand generation capabilities. We further propose a pivot-guided noise re-schedule strategy to speed up the inference\u001b[0m\n",
       "\u001b[32mprocess and improve local structural details. Compared to full fine-tuning, our approach achieves a 5x training \u001b[0m\n",
       "\u001b[32mspeed-up and requires only an additional 0.002M tuning parameters. Extensive experiments demonstrate that our \u001b[0m\n",
       "\u001b[32mapproach can quickly adapt to higher resolution image and video synthesis by fine-tuning for just 10k steps, with \u001b[0m\n",
       "\u001b[32mvirtually no additional inference time. Our code will be released at \u001b[0m\n",
       "\u001b[32mhttps://github.com/GuoLanging/Self-Cascade/.\"\\n'\u001b[0m,\n",
       "        \u001b[32m' \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"é€šç”¨æ“ä½œç•Œé¢ï¼šåœ¨ä¸ä½¿ç”¨é‡å¤–æœºå™¨äººçš„æƒ…å†µä¸‹ï¼Œåœ¨é‡å¤–å¯¹æœºå™¨è¿›è¡Œæ•™å­¦\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"æˆ‘ä»¬æå‡ºäº†é€šç”¨æ“ä½œç•Œé¢ï¼ˆUMIï¼‰â€”â€”ä¸€ç§æ•°æ®æ”¶é›†å’Œç­–ç•¥å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸ç›´æ¥å°†é‡å¤–äººç±»æ¼”ç¤ºä¸­çš„æŠ€èƒ½è½¬ç§»åˆ°å¯éƒ¨ç½²æœºå™¨ç­–\u001b[0m\n",
       "\u001b[32mç•¥ä¸­ã€‚UMI \u001b[0m\n",
       "\u001b[32mé‡‡ç”¨æ‰‹æŒæŠ“æ‰‹ç»“åˆè°¨æ…çš„ç•Œé¢è®¾è®¡ï¼Œå¯å®ç°ä¾¿æºã€ä½æˆæœ¬ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ•°æ®æ”¶é›†ï¼Œä»¥è¿›è¡Œæå…·æŒ‘æˆ˜æ€§çš„åŒæ‰‹åŠ¨å’ŒåŠ¨æ€æ“ä½œæ¼”ç¤ºã€‚ä¸º\u001b[0m\n",
       "\u001b[32mä¿ƒè¿›å¯éƒ¨ç½²ç­–ç•¥å­¦ä¹ ï¼ŒUMI \u001b[0m\n",
       "\u001b[32måŒ…å«ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„ç­–ç•¥ç•Œé¢ï¼Œè¯¥ç•Œé¢å…·å¤‡æ¨ç†æ—¶å»¶è¿ŸåŒ¹é…å’Œç›¸å¯¹è½¨è¿¹åŠ¨ä½œè¡¨ç¤ºã€‚æ‰€å­¦ç­–ç•¥ä¸ç¡¬ä»¶æ— å…³ï¼Œä¸”å¯åœ¨å¤šç§æœºå™¨äººå¹³å°ä¸Š\u001b[0m\n",
       "\u001b[32méƒ¨ç½²ã€‚é…å¤‡è¿™äº›åŠŸèƒ½ï¼ŒUMI \u001b[0m\n",
       "\u001b[32mæ¡†æ¶è§£é”äº†æ–°çš„æœºå™¨äººæ“ä½œèƒ½åŠ›ï¼Œä»…é€šè¿‡é’ˆå¯¹æ¯ä¸ªä»»åŠ¡æ›´æ”¹è®­ç»ƒæ•°æ®ï¼Œå³å¯å®ç°é›¶æ¬¡æ¦‚æ‹¬çš„åŠ¨æ€ã€åŒæ‰‹åŠ¨ã€ç²¾ç¡®å®šä½å’Œé•¿æ—¶é—´è¡Œä¸º\u001b[0m\n",
       "\u001b[32mã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„çœŸå®å®éªŒå±•ç¤ºäº† UMI çš„å¤šåŠŸèƒ½æ€§å’Œæœ‰æ•ˆæ€§ï¼Œåœ¨ UMI \u001b[0m\n",
       "\u001b[32mä¸Šå­¦ä¹ çš„ç­–ç•¥ï¼Œåœ¨é’ˆå¯¹å„ç§äººç±»æ¼”ç¤ºè¿›è¡Œè®­ç»ƒåï¼Œå¯é›¶æ¬¡æ¦‚æ‹¬åˆ°æ–°ç¯å¢ƒå’Œç‰©ä½“ã€‚UMI çš„ç¡¬ä»¶å’Œè½¯ä»¶ç³»ç»Ÿå¯åœ¨ \u001b[0m\n",
       "\u001b[32mhttps://umi-gripper.github.io è·å–å¼€æºã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild \u001b[0m\n",
       "\u001b[32mRobots\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"We present Universal Manipulation Interface \u001b[0m\u001b[32m(\u001b[0m\u001b[32mUMI\u001b[0m\u001b[32m)\u001b[0m\u001b[32mâ€”a data collection and policy \u001b[0m\n",
       "\u001b[32mlearning framework that allows direct skill transfer from in-the-wild human demonstrations to deployable robot \u001b[0m\n",
       "\u001b[32mpolicies. UMI employs hand-held grippers coupled with careful interface design to enable portable, low-cost, and \u001b[0m\n",
       "\u001b[32minformation-rich data collection for challenging bimanual and dynamic manipulation demonstrations. To facilitate \u001b[0m\n",
       "\u001b[32mdeployable policy learning, UMI incorporates a carefully designed policy interface with inference-time latency \u001b[0m\n",
       "\u001b[32mmatching and a relative-trajectory action representation. The resulting learned policies are hardware-agnostic and \u001b[0m\n",
       "\u001b[32mdeployable across multiple robot platforms. Equipped with these features, UMI framework unlocks new robot \u001b[0m\n",
       "\u001b[32mmanipulation capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and long-horizon behaviors,\u001b[0m\n",
       "\u001b[32mby only changing the training data for each task. We demonstrate UMIâ€™s versatility and efficacy with comprehensive \u001b[0m\n",
       "\u001b[32mreal-world experiments, where policies learned via UMI zero-shot generalize to novel environments and objects when \u001b[0m\n",
       "\u001b[32mtrained on diverse human demonstrations. UMIâ€™s hardware and software system is open-sourced at \u001b[0m\n",
       "\u001b[32mhttps://umi-gripper.github.io.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m' \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"chinese_title\": \"SPARï¼šé€šè¿‡é•¿æœŸå‚ä¸æ³¨æ„åŠ›å®ç°ä¸ªæ€§åŒ–çš„åŸºäºå†…å®¹çš„æ¨è\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"chinese_summary\": \"åˆ©ç”¨ç”¨æˆ·çš„é•¿æœŸå‚ä¸å†å²å¯¹äºä¸ªæ€§åŒ–çš„å†…å®¹æ¨èè‡³å…³é‡è¦ã€‚é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m åœ¨ NLP \u001b[0m\n",
       "\u001b[32mä¸­çš„æˆåŠŸä½¿å…¶è¢«ç”¨äºå¯¹ç”¨æˆ·å†å²å’Œå€™é€‰é¡¹ç›®è¿›è¡Œç¼–ç ï¼Œå¹¶å°†å†…å®¹æ¨èæ„å»ºä¸ºæ–‡æœ¬è¯­ä¹‰åŒ¹é…ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œåœ¨å¤„ç†è¶…é•¿çš„ç”¨æˆ·\u001b[0m\n",
       "\u001b[32må†å²æ–‡æœ¬å’Œä¸è¶³çš„ç”¨æˆ·ä¸é¡¹ç›®çš„äº¤äº’æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå†…å®¹çš„æ¨èæ¡†æ¶ \u001b[0m\n",
       "\u001b[32mSPARï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°è§£å†³äº†ä»é•¿æœŸçš„ç”¨æˆ·å‚ä¸å†å²ä¸­æå–æ•´ä½“ç”¨æˆ·å…´è¶£çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡åˆ©ç”¨ \u001b[0m\n",
       "\u001b[32mPLMã€å¤šæ³¨æ„åŠ›å±‚å’Œæ³¨æ„åŠ›ç¨€ç–æœºåˆ¶ä»¥åŸºäºä¼šè¯çš„æ–¹å¼å¯¹ç”¨æˆ·å†å²è¿›è¡Œç¼–ç æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç”¨æˆ·ç«¯å’Œé¡¹ç›®ç«¯ç‰¹å¾å……åˆ†èåˆç”¨äºå‚ä¸åº¦\u001b[0m\n",
       "\u001b[32mé¢„æµ‹ï¼ŒåŒæ—¶ä¿æŒä¸¤ç«¯çš„ç‹¬ç«‹è¡¨ç¤ºï¼Œè¿™å¯¹äºå®é™…æ¨¡å‹éƒ¨ç½²æ˜¯æœ‰æ•ˆçš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mä»ç”¨æˆ·å‚ä¸å†å²ä¸­æå–å…¨å±€å…´è¶£æ¥å¢å¼ºç”¨æˆ·ç”»åƒã€‚åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿› \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSoTA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mæ–¹æ³•ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"english_title\": \"SPAR: Personalized Content-Based Recommendation via Long Engagement Attention\",\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"english_summary\": \"Leveraging usersâ€™ long engagement histories is essential for personalized content \u001b[0m\n",
       "\u001b[32mrecommendations. The success of pretrained language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in NLP has led to their use in encoding user \u001b[0m\n",
       "\u001b[32mhistories and candidate items, framing content recommendations as textual semantic matching tasks. However, \u001b[0m\n",
       "\u001b[32mexisting works still struggle with processing very long user historical text and insufficient user-item \u001b[0m\n",
       "\u001b[32minteraction. In this paper, we introduce a content-based recommendation framework, SPAR, which effectively tackles \u001b[0m\n",
       "\u001b[32mthe challenges of holistic user interest extraction from the long user engagement history. It achieves so by \u001b[0m\n",
       "\u001b[32mleveraging PLM, poly-attention layers and attention sparsity mechanisms to encode userâ€™s history in a session-based\u001b[0m\n",
       "\u001b[32mmanner. The user and item side features are sufficiently fused for engagement prediction while maintaining \u001b[0m\n",
       "\u001b[32mstandalone representations for both sides, which is efficient for practical model deployment. Moreover, we enhance \u001b[0m\n",
       "\u001b[32muser profiling by exploiting large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to extract global interests from user engagement history. \u001b[0m\n",
       "\u001b[32mExtensive experiments on two benchmark datasets demonstrate that our framework outperforms existing \u001b[0m\n",
       "\u001b[32mstate-of-the-art \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSoTA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m methods.\"\\n'\u001b[0m,\n",
       "        \u001b[32m' \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"PaLM2-VAdapterï¼šæ¸è¿›å¼å¯¹é½è¯­è¨€æ¨¡å‹æˆä¸ºå¼ºå¤§çš„è§†è§‰è¯­è¨€é€‚é…å™¨\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"æœ¬æ–‡è¡¨æ˜ï¼Œæ¸è¿›å¼å¯¹é½çš„è¯­è¨€æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ¡¥æ¥å†»ç»“çš„è§†è§‰ç¼–ç å™¨å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è™½ç„¶è§†è§‰ç¼–ç å™¨å’Œ LLM \u001b[0m\n",
       "\u001b[32mçš„åŸºæœ¬æ¶æ„å’Œé¢„è®­ç»ƒæ–¹æ³•å·²ç»å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†è§†è§‰è¯­è¨€é€‚é…å™¨çš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥åœ¨æœ€è¿‘çš„ç ”ç©¶ä¸­å·®å¼‚å¾ˆå¤§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹æœ€å…ˆè¿›\u001b[0m\n",
       "\u001b[32mçš„æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨æ¶æ„è¿›è¡Œäº†å½»åº•æ¢ç´¢ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå…·æœ‰æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨çš„è§†è§‰è¯­è¨€å¯¹é½è¡¨ç°\u001b[0m\n",
       "\u001b[32må‡ºç¼“æ…¢çš„æ”¶æ•›æ€§å’Œæœ‰é™çš„å¯æ‰©å±•æ€§ï¼Œç¼ºä¹ç›´æ¥çš„ç›‘ç£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PaLM2-VAdapterï¼Œé‡‡ç”¨æ¸è¿›å¼å¯¹é½çš„è¯­è¨€æ¨¡\u001b[0m\n",
       "\u001b[32må‹ä½œä¸ºè§†è§‰è¯­è¨€é€‚é…å™¨ã€‚ä¸å…·æœ‰æ„ŸçŸ¥å™¨é‡é‡‡æ ·å™¨çš„å¼ºå¤§åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡­ç»éªŒè¯æ˜å…·æœ‰æ›´å¿«çš„æ”¶æ•›æ€§ã€æ›´é«˜çš„æ€§èƒ½å’Œæ›´å¼ºçš„\u001b[0m\n",
       "\u001b[32må¯æ‰©å±•æ€§ã€‚è·¨è¶Šå›¾åƒå’Œè§†é¢‘çš„å„ç§è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œå­—å¹•ä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å±•ç¤ºäº†æœ€å…ˆè¿›çš„è§†è§‰ç†è§£å’Œå¤šæ¨¡æ€æ¨ç†\u001b[0m\n",
       "\u001b[32mèƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å‚æ•°å°‘äº†30~70%ï¼Œè¿™æ ‡å¿—ç€æ•ˆç‡çš„æ˜¾ç€æé«˜ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language \u001b[0m\n",
       "\u001b[32mAdapter\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"This paper demonstrates that a progressively aligned language model can \u001b[0m\n",
       "\u001b[32meffectively bridge frozen vision encoders and large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. While the fundamental architecture and \u001b[0m\n",
       "\u001b[32mpre-training methods of vision encoders and LLMs have been extensively studied, the architecture and training \u001b[0m\n",
       "\u001b[32mstrategy of vision-language adapters vary significantly across recent works. Our research undertakes a thorough \u001b[0m\n",
       "\u001b[32mexploration of the state-of-the-art perceiver resampler architecture and builds a strong baseline. However, we \u001b[0m\n",
       "\u001b[32mobserve that the vision-language alignment with perceiver resampler exhibits slow convergence and limited \u001b[0m\n",
       "\u001b[32mscalability with a lack of direct supervision. To address this issue, we propose PaLM2-VAdapter, employing a \u001b[0m\n",
       "\u001b[32mprogressively aligned language model as the vision-language adapter. Compared to the strong baseline with perceiver\u001b[0m\n",
       "\u001b[32mresampler, our method empirically shows faster convergence, higher performance and stronger scalability. Extensive \u001b[0m\n",
       "\u001b[32mexperiments across various Visual Question Answering \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and captioning tasks on both images and videos \u001b[0m\n",
       "\u001b[32mdemonstrate that our model exhibits state-of-the-art visual understanding and multi-modal reasoning capabilities. \u001b[0m\n",
       "\u001b[32mNotably, our method achieves these advancements with 30~70% fewer parameters than the state-of-the-art large \u001b[0m\n",
       "\u001b[32mvision-language models, marking a significant efficiency improvement.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'  \"Summary\": \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_title\": \"RLVFï¼šä»è¯­è¨€åé¦ˆä¸­å­¦ä¹ ï¼Œé¿å…è¿‡åº¦æ¦‚æ‹¬\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"chinese_summary\": \u001b[0m\n",
       "\u001b[32m\"éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªè¡Œä¸šå’Œä¸ªä½“ä¸­çš„å¹¿æ³›é‡‡ç”¨ï¼Œé’ˆå¯¹ç‰¹å®šç”¨æˆ·æˆ–ç”¨ä¾‹ï¼ŒæŒ‰ç…§é«˜çº§äººç±»åé¦ˆå¯¹å…¶è¿›è¡Œè°ƒæ•´çš„èƒ½åŠ›å˜å¾—è¶Š\u001b[0m\n",
       "\u001b[32mæ¥è¶Šé‡è¦ã€‚è™½ç„¶ LLM \u001b[0m\n",
       "\u001b[32mç”¨æˆ·é€šå¸¸å¸Œæœ›æ¨¡å‹å§‹ç»ˆéµå¾ªå¹¿æ³›çš„åŸåˆ™ï¼Œæ¯”å¦‚ç”Ÿæˆæµç•…çš„æ–‡æœ¬ï¼Œä½†ä¸ªåˆ«ç”¨æˆ·å’Œç”¨ä¾‹æœ‰æ›´ç»†å¾®çš„åå¥½ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯èƒ½è¦æ±‚ LLM \u001b[0m\n",
       "\u001b[32må†™å‡ºæ›´ç®€æ´çš„å·¥ä½œç”µå­é‚®ä»¶ï¼Œä½†è¦å†™å‡ºæ›´è¯¦ç»†çš„ä¸ªäººç”µå­é‚®ä»¶ï¼Œè¿™ä½¿å¾—åé¦ˆå…·æœ‰ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚æ ¹æ®è¿™äº›åå¥½è°ƒæ•´æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§\u001b[0m\n",
       "\u001b[32mï¼šéœ€è¦å¤§é‡èµ„æºæ‰èƒ½æ”¶é›†æ‰€æœ‰ä¸åŒä¸Šä¸‹æ–‡ä¸­çš„åå¥½ï¼Œå¹¶ä¸”åœ¨ä¸€ä¸ªä¸Šä¸‹æ–‡ä¸­çš„æ¨¡å‹å¾®è°ƒä¼šå¯¹å…¶ä»–ä¸Šä¸‹æ–‡ä¸­çš„æ¨¡å‹è¡Œä¸ºäº§ç”Ÿä¸å¯é¢„æµ‹çš„\u001b[0m\n",
       "\u001b[32må½±å“ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨è¯­è¨€åé¦ˆæ¥è°ƒæ•´æ¨¡å‹çš„é—®é¢˜ï¼Œè¿™ç§åé¦ˆå¯¹äºäººä»¬æ¥è¯´å¿«é€Ÿä¸”å®¹æ˜“æä¾›ï¼ˆè§å›¾ \u001b[0m\n",
       "\u001b[32m1ï¼‰ã€‚åˆå¹¶åé¦ˆçš„å¸¸è§æ–¹æ³•ï¼Œä¾‹å¦‚ç›‘ç£ä¸Šä¸‹æ–‡è’¸é¦ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSCD\u001b[0m\u001b[32m)\u001b[0m\u001b[32m æˆ–å¼ºåŒ–å­¦ä¹ ï¼Œåˆ©ç”¨äººç±»åé¦ˆ \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mRLHF\u001b[0m\u001b[32m)\u001b[0m\u001b[32mï¼Œä½¿ç”¨ç¤ºä¾‹çº§ç›‘ç®¡é€šè¿‡ç›‘ç£å®Œæˆæˆ–åå¥½æ ‡ç­¾ã€‚æ­¤ç±»æ–¹æ³•éœ€è¦å¤§é‡ç”¨æˆ·æä¾›çš„ï¼ˆåå¥½ï¼‰æ•°æ®è¯­æ–™åº“ï¼Œè€Œè·å–è¿™äº›æ•°æ®è¯­æ–™åº“å¯\u001b[0m\n",
       "\u001b[32mèƒ½æ—¢æ˜‚è´µåˆç¹çã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¸ä¼šé™åˆ¶åé¦ˆå¯èƒ½é€‚ç”¨çš„ä¸Šä¸‹æ–‡ä¹‹å¤–çš„æ¨¡å‹è¡Œä¸ºï¼Œå› æ­¤ LLM \u001b[0m\n",
       "\u001b[32må¯èƒ½ä¼šä»¥æ„å¤–çš„æ–¹å¼è°ƒæ•´å…¶è¡Œä¸ºï¼Œä¾‹å¦‚åœ¨åå¥½ä»…é€‚ç”¨äºä¸ªäººç”µå­é‚®ä»¶æ—¶ç”Ÿæˆæ›´å†—é•¿çš„å·¥ä½œç”µå­é‚®ä»¶ã€‚è¯­è¨€åé¦ˆå¯¹äºäººç±»æ¥è¯´æ›´å®¹æ˜“\u001b[0m\n",
       "\u001b[32mã€æ›´å¿«é€Ÿåœ°æä¾›ã€‚ä¸ºæ­¤ï¼Œå¦ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯å°†æ­¤ç±»è¯­è¨€åé¦ˆçº³å…¥æç¤ºä¸­ï¼Œå¯èƒ½é€šè¿‡è¿­ä»£è¿‡ç¨‹æ¥æŒç»­æ·»åŠ å…¶ä»–åé¦ˆç‚¹ã€‚ç„¶è€Œï¼Œæ­¤æ–¹\u001b[0m\n",
       "\u001b[32mæ³•éœ€è¦åœ¨æ‰€æœ‰æœªæ¥æŸ¥è¯¢ä¸­é‡æ–°ä½¿ç”¨è¯¥æç¤ºã€‚éšç€æ›´å¤šåé¦ˆçš„ç§¯ç´¯ï¼ŒåŒ…å«è®¸å¤šä¸Šä¸‹æ–‡ç›¸å…³åé¦ˆçš„é•¿æç¤ºä¼šä½¿æ¨ç†å˜å¾—æ˜‚è´µï¼›æ­¤å¤–ï¼Œè¯†\u001b[0m\n",
       "\u001b[32måˆ«å“ªäº›åé¦ˆåº”å½“åœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­é€‚ç”¨å¯èƒ½å˜å¾—å¾ˆå›°éš¾ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è°ƒæ•´ \u001b[0m\n",
       "\u001b[32mLLMï¼Œä»¥ä¾¿åœ¨æä¾›ä¸€ä¸ªæŒ‡å®šåé¦ˆçš„å¥å­æ—¶ï¼Œæ¨¡å‹èƒ½å¤Ÿè¾¨åˆ«åé¦ˆé€‚ç”¨çš„å“ªäº›æƒ…å†µï¼Œå¹¶åœ¨æœªæ¥è¾“å‡ºä¸­é€‚å½“åœ°çº³å…¥è¯¥åé¦ˆã€‚æˆ‘ä»¬æå‡ºäº†æƒ…å¢ƒ\u001b[0m\n",
       "\u001b[32måŒ–æ‰¹åˆ¤ä¸çº¦æŸæ€§åå¥½ä¼˜åŒ– \u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mC3PO\u001b[0m\u001b[32m)\u001b[0m\u001b[32mï¼Œå…¶ä¸­æˆ‘ä»¬é¦–å…ˆä¸ºè¶…å‡ºåé¦ˆèŒƒå›´å’Œæœªè¶…å‡ºåé¦ˆèŒƒå›´çš„æƒ…å¢ƒåˆæˆç”Ÿæˆå‡è®¾æ€§æç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹è¿™äº›æç¤ºè¿›è¡ŒåŸå§‹å®Œæˆé‡‡æ ·ï¼Œ\u001b[0m\n",
       "\u001b[32mä¸åº”ç”¨åé¦ˆï¼Œä»¥åŠå¯¹æ‰€æ”¶é›†çš„åé¦ˆè¿›è¡Œä¿®è®¢ï¼Œä»¥ä¾¿ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆä¸€ä¸ªåˆæˆåå¥½æ•°æ®é›†ï¼ŒæŒ‡å®šåé¦ˆåº”å½“ï¼ˆå’Œä¸åº”å½“ï¼‰å¦‚ä½•åº”ç”¨ã€‚æ¥\u001b[0m\n",
       "\u001b[32mç€ï¼Œæˆ‘ä»¬æ ¹æ®åˆæˆåå¥½æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘è¶…å‡ºåé¦ˆé€‚ç”¨èŒƒå›´çš„æç¤ºçš„åŸå§‹æ¨¡å‹çš„å·®å¼‚ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨\u001b[0m\n",
       "\u001b[32mæ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å°†è¯­è¨€åé¦ˆåº”ç”¨äºç›¸å…³åœºæ™¯ï¼ŒåŒæ—¶ä¿ç•™äº†å…¶ä»–ä¸Šä¸‹æ–‡çš„ç°æœ‰è¡Œä¸ºã€‚å¯¹äºäººç±»å’Œ GPT-4 ç”Ÿæˆçš„è¯­è¨€åé¦ˆï¼ŒC3PO\u001b[0m\n",
       "\u001b[32mæœ‰æ•ˆåœ°éµå®ˆç»™å®šçš„åé¦ˆï¼Œä¸ä¸Šä¸‹æ–‡ä¸­åŸºå‡†ç›¸å½“ï¼ŒåŒæ—¶å°†è¿‡åº¦æ¦‚æ‹¬å‡å°‘äº† 30%ã€‚\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_title\": \"RLVF: Learning from Verbal Feedback without Overgeneralization\",\\n'\u001b[0m,\n",
       "        \u001b[32m'    \"english_summary\": \"The diversity of contexts in which large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are deployed \u001b[0m\n",
       "\u001b[32mrequires the ability to modify or customize default model behaviors to incorporate nuanced requirements and \u001b[0m\n",
       "\u001b[32mpreferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as â€œDonâ€™t\u001b[0m\n",
       "\u001b[32muse emojis when drafting emails to my boss.â€ However, while writing high-level feedback is far simpler than \u001b[0m\n",
       "\u001b[32mcollecting annotations for reinforcement learning from human feedback \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRLHF\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, we find that simply prompting a model\u001b[0m\n",
       "\u001b[32mwith such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the \u001b[0m\n",
       "\u001b[32mproblem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized \u001b[0m\n",
       "\u001b[32mCritiques with Constrained Preference Optimization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mC3PO\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. C3PO uses a piece of high-level feedback to generate a \u001b[0m\n",
       "\u001b[32msmall synthetic preference dataset specifying how the feedback should \u001b[0m\u001b[32m(\u001b[0m\u001b[32mand should not\u001b[0m\u001b[32m)\u001b[0m\u001b[32m be applied. It then \u001b[0m\n",
       "\u001b[32mfine-tunes the model in accordance with the synthetic preference data while minimizing the divergence from the \u001b[0m\n",
       "\u001b[32moriginal model for prompts where the feedback does not apply. Our experimental results indicate that our approach \u001b[0m\n",
       "\u001b[32meffectively applies verbal feedback to relevant scenarios while preserving existing behaviors for other contexts. \u001b[0m\n",
       "\u001b[32mFor both human- and GPT-4-generated high-level feedback, C3PO effectively adheres to the given feedback comparably \u001b[0m\n",
       "\u001b[32mto in-context baselines while reducing overgeneralization by 30%.\"\\n'\u001b[0m,\n",
       "        \u001b[32m'  \u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m,\n",
       "        \u001b[32m'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "summary_list = []\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"./summary_test.txt\"\n",
    "\n",
    "# Read the file and store each 8 lines as a list element\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for i in range(0, len(lines), 8):\n",
    "        summary = lines[i:i+8]\n",
    "        summary_list.append(summary)\n",
    "\n",
    "# Print the list of summaries\n",
    "print(summary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere (Unsued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cohere guardrails-ai pydantic -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import guardrails as gd\n",
    "from guardrails.validators import ValidRange, ValidChoices\n",
    "from pydantic import BaseModel, Field\n",
    "from rich import print\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summary(BaseModel):\n",
    "    chinese_title: str = Field(..., description=\"åœ¨æ‘˜è¦å¼€å¤´å‘ˆç°æ–‡æ¡£çš„ä¸­æ–‡æ ‡é¢˜ã€‚è¿™å°†ä½œä¸ºæ‘˜è¦çš„é¦–è¦å†…å®¹ï¼Œç«‹åˆ»å‘è¯»è€…æ˜ç¡®æ–‡æ¡£çš„æ ¸å¿ƒä¸»é¢˜ã€‚\", min_length=1)\n",
    "    chinese_summary: str = Field(..., description=\"\"\"é’ˆå¯¹æ¯ä¸ªæä¾›çš„PDFæ–‡ä»¶ï¼Œåˆ›å»ºä¸€ä¸ªç²¾ç¡®ä¸”ç®€æ´çš„æ‘˜è¦ï¼Œæ•æ‰æ–‡ä»¶çš„ç²¾é«“ï¼Œä¸“æ³¨äºä¸»è¦è¯é¢˜æˆ–é—®é¢˜ã€å…³é”®å‘ç°æˆ–è®ºç‚¹ã€æ–¹æ³•è®ºï¼ˆå¦‚æœé€‚ç”¨ï¼‰ä»¥åŠç»“è®ºå’Œå½±å“ã€‚æ‘˜è¦åº”è¯¥æ˜¯å¼•äººå…¥èƒœçš„ï¼Œç”¨æ¸…æ™°ç®€å•çš„è¯­è¨€ä¹¦å†™ï¼Œç¡®ä¿æ™®é€šå¤§å­¦æ•™è‚²æ°´å¹³çš„è¯»è€…èƒ½å¤Ÿè½»æ¾ç†è§£ã€‚\n",
    "\n",
    "å°†ä»¥ä¸‹å…ƒç´ æ•´åˆè¿›ä¸€ä¸ªè¿è´¯çš„æ®µè½ä¸­ï¼Œä¸ä½¿ç”¨é¡¹ç›®ç¬¦å·ï¼š\n",
    "\n",
    "å¼•è¨€: ä»¥æ–‡ä»¶çš„ä¸­å¿ƒä¸»é¢˜æˆ–æ¢è¯¢çš„ç®€çŸ­ä»‹ç»å¼€å§‹ã€‚\n",
    "ä¸»è¦å‘ç°æˆ–è®ºç‚¹: çªå‡ºæ–‡ä»¶çš„ä¸»è¦å‘ç°æˆ–è®ºç‚¹ï¼Œä¿æŒæ¸…æ™°å’Œå¸å¼•åŠ›ã€‚\n",
    "æ–¹æ³•è®ºæˆ–æ–¹æ³•: å¦‚æœç›¸å…³ï¼Œç®€è¦æè¿°æ–¹æ³•è®ºæˆ–æ–¹æ³•ï¼Œç®€åŒ–ä»»ä½•æŠ€æœ¯æœ¯è¯­ã€‚\n",
    "ç»“è®ºå’Œå½±å“: ä»¥æ–‡ä»¶çš„æ›´å¹¿æ³›å½±å“æˆ–å…¶æ½œåœ¨å½±å“çš„æ€»ç»“ç»“æŸï¼Œå¼ºè°ƒå…¶é‡è¦æ€§ã€‚\n",
    "ä½ çš„ç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªç®€æ´è€Œå…¨é¢çš„æ¦‚è§ˆï¼Œä½¿è¯»è€…èƒ½å¤Ÿåœ¨ä¸éœ€è¦é˜…è¯»å…¨æ–‡çš„æƒ…å†µä¸‹æŠŠæ¡æ–‡ä»¶çš„æœ¬è´¨ã€‚æ¯ä¸ªæ‘˜è¦å¿…é¡»æ˜¯ç‹¬ç‰¹çš„ï¼Œå‡†ç¡®åæ˜ PDFçš„ç‰¹å®šå†…å®¹å’Œè¯­è°ƒã€‚\"\"\", min_length=1)\n",
    "    english_title: str = Field(..., description=\"Present the document's in English. This should be the first element in the summary to immediately inform the reader of the document's subject.\", min_length=1)\n",
    "    english_summary: str = Field(..., description=\"\"\"For each PDF document provided, create an articulate and concise summary that captures the essence of the document, focusing on the main topic or question, key findings or arguments, methodology (if applicable), and the conclusions and implications. The summary should be engaging, written in clear, simple language to ensure it's accessible to a general audience with a college-level education.\n",
    "\n",
    "Incorporate the following elements into a single, coherent paragraph without using bullet points:\n",
    "\n",
    "Introduction: Begin with a brief introduction to the document's central theme or inquiry.\n",
    "Main Findings or Arguments: Highlight the document's primary discoveries or arguments, maintaining clarity and engagement.\n",
    "Methodology or Approach: Briefly describe the methodology or approach, if relevant, simplifying any technical terms.\n",
    "Conclusions and Implications: Finish with a summary of the document's broader implications or its potential impact, underlining its significance.\n",
    "Your goal is to provide a succinct yet thorough overview that allows readers to grasp the document's essence without needing to delve into the full text. Each summary must be unique, reflecting the specific content and tone of the PDF accurately.\"\"\", min_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = pdflink_list[0]\n",
    "\n",
    "PROMPT = f\"\"\"For each PDF provided, produce a unified, concise summary that encapsulates the document's essential aspects, including its main topic or research question, key findings or arguments, methodology or approach (if relevant), and conclusions and implications. Craft this summary to be clear, engaging, and easily understandable to a general audience with a college-level education. Aim to convey the core message and significance of the document succinctly, yet thoroughly, allowing readers to grasp its essence without reading the document in its entirety.\n",
    "Each summary should seamlessly integrate the following elements into a single, cohesive paragraph:\n",
    "* Introduction: Start with a succinct introduction to the document's primary focus or inquiry, using straightforward and accessible language.\n",
    "* Main Findings or Arguments: Weave in an overview of the document's key discoveries or arguments, ensuring the narrative remains engaging and lucid.\n",
    "* Methodology or Approach: Where applicable, briefly elucidate on the methodology or approach employed in the document, simplifying any complex terminologies for the lay reader.\n",
    "* Conclusions and Implications: Conclude by summarizing the document's broader implications or its potential impact, highlighting why it matters.\n",
    "This summary needs to be crafted in both English and Chinese, maintaining a balance between clarity and depth to cater to a bilingual audience. Please refrain from using bullet points, ensuring the summary remains in one continuous, flowing paragraph that accurately mirrors the specific content and tone of its corresponding PDF.\n",
    "The ultimate goal is to deliver succinct yet comprehensive overviews, enabling readers to understand the essence of each document fully. Each summary must be unique, faithfully reflecting the specific nuances and themes of its respective PDF.\n",
    "\n",
    "{{Summary}}\n",
    "\n",
    "PDF contents:{pdf_text}\n",
    "\n",
    "@complete_json_suffix_v2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">For each PDF provided, produce a unified, concise summary that encapsulates the document's essential aspects, \n",
       "including its main topic or research question, key findings or arguments, methodology or approach <span style=\"font-weight: bold\">(</span>if relevant<span style=\"font-weight: bold\">)</span>, \n",
       "and conclusions and implications. Craft this summary to be clear, engaging, and easily understandable to a general \n",
       "audience with a college-level education. Aim to convey the core message and significance of the document \n",
       "succinctly, yet thoroughly, allowing readers to grasp its essence without reading the document in its entirety.\n",
       "Each summary should seamlessly integrate the following elements into a single, cohesive paragraph:\n",
       "* Introduction: Start with a succinct introduction to the document's primary focus or inquiry, using \n",
       "straightforward and accessible language.\n",
       "* Main Findings or Arguments: Weave in an overview of the document's key discoveries or arguments, ensuring the \n",
       "narrative remains engaging and lucid.\n",
       "* Methodology or Approach: Where applicable, briefly elucidate on the methodology or approach employed in the \n",
       "document, simplifying any complex terminologies for the lay reader.\n",
       "* Conclusions and Implications: Conclude by summarizing the document's broader implications or its potential \n",
       "impact, highlighting why it matters.\n",
       "This summary needs to be crafted in both English and Chinese, maintaining a balance between clarity and depth to \n",
       "cater to a bilingual audience. Please refrain from using bullet points, ensuring the summary remains in one \n",
       "continuous, flowing paragraph that accurately mirrors the specific content and tone of its corresponding PDF.\n",
       "The ultimate goal is to deliver succinct yet comprehensive overviews, enabling readers to understand the essence of\n",
       "each document fully. Each summary must be unique, faithfully reflecting the specific nuances and themes of its \n",
       "respective PDF.\n",
       "\n",
       "<span style=\"font-weight: bold\">{</span>Summary<span style=\"font-weight: bold\">}</span>\n",
       "\n",
       "PDF contents:<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/2402.10644.pdf</span>\n",
       "\n",
       "@complete_json_suffix_v2\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "For each PDF provided, produce a unified, concise summary that encapsulates the document's essential aspects, \n",
       "including its main topic or research question, key findings or arguments, methodology or approach \u001b[1m(\u001b[0mif relevant\u001b[1m)\u001b[0m, \n",
       "and conclusions and implications. Craft this summary to be clear, engaging, and easily understandable to a general \n",
       "audience with a college-level education. Aim to convey the core message and significance of the document \n",
       "succinctly, yet thoroughly, allowing readers to grasp its essence without reading the document in its entirety.\n",
       "Each summary should seamlessly integrate the following elements into a single, cohesive paragraph:\n",
       "* Introduction: Start with a succinct introduction to the document's primary focus or inquiry, using \n",
       "straightforward and accessible language.\n",
       "* Main Findings or Arguments: Weave in an overview of the document's key discoveries or arguments, ensuring the \n",
       "narrative remains engaging and lucid.\n",
       "* Methodology or Approach: Where applicable, briefly elucidate on the methodology or approach employed in the \n",
       "document, simplifying any complex terminologies for the lay reader.\n",
       "* Conclusions and Implications: Conclude by summarizing the document's broader implications or its potential \n",
       "impact, highlighting why it matters.\n",
       "This summary needs to be crafted in both English and Chinese, maintaining a balance between clarity and depth to \n",
       "cater to a bilingual audience. Please refrain from using bullet points, ensuring the summary remains in one \n",
       "continuous, flowing paragraph that accurately mirrors the specific content and tone of its corresponding PDF.\n",
       "The ultimate goal is to deliver succinct yet comprehensive overviews, enabling readers to understand the essence of\n",
       "each document fully. Each summary must be unique, faithfully reflecting the specific nuances and themes of its \n",
       "respective PDF.\n",
       "\n",
       "\u001b[1m{\u001b[0mSummary\u001b[1m}\u001b[0m\n",
       "\n",
       "PDF contents:\u001b[4;94mhttps://arxiv.org/pdf/2402.10644.pdf\u001b[0m\n",
       "\n",
       "@complete_json_suffix_v2\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "guard = gd.Guard.from_pydantic(Summary, prompt=PROMPT)\n",
    "print(guard.base_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6ziwZlCAQi45cixuyDoGKm6A17VsJYXR8Ipe1Elq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'user_id' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m co \u001b[38;5;241m=\u001b[39m cohere\u001b[38;5;241m.\u001b[39mClient(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6ziwZlCAQi45cixuyDoGKm6A17VsJYXR8Ipe1Elq\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m raw_llm_output, validated_output \u001b[38;5;241m=\u001b[39m guard(\n\u001b[1;32m      3\u001b[0m     co\u001b[38;5;241m.\u001b[39mgenerate,\n\u001b[1;32m      4\u001b[0m     prompt_params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: Summary},\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommand\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      7\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "co = cohere.Client(api_key='6ziwZlCAQi45cixuyDoGKm6A17VsJYXR8Ipe1Elq')\n",
    "raw_llm_output, validated_output = guard(\n",
    "    co.generate,\n",
    "    prompt_params={\"Summary\": Summary},\n",
    "    model='command',\n",
    "    max_tokens=1024,\n",
    "    temperature=0.6\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
